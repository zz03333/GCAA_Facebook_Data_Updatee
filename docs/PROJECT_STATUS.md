# Project Status & Structure Guide

**Last Updated**: 2026-01-15
**Your Question**: "Where are we now? How does the system work? What files are not needed?"

---

## ğŸ“ WHERE WE ARE NOW

### Current Status
âœ… **PRODUCTION READY** - All systems are operational and organized

The system is a **fully automated Facebook analytics pipeline** that:
1. **Collects** data from Facebook Graph API
2. **Analyzes** posts and engagement metrics
3. **Exports** insights to Google Sheets (20+ tabs)
4. **Visualizes** data in a React dashboard

### What Changed Recently
- âœ… Organized 21 Python files into 5 clean directories
- âœ… Added timestamp columns to all Google Sheets tabs
- âœ… Created Tab Documentation sheet explaining all 20+ tabs
- âœ… System runs daily via Cloud Run + Cloud Scheduler

---

## ğŸ—‚ï¸ PROJECT STRUCTURE

```
API_Parser/
â”œâ”€â”€ main.py                    â­ Flask API server (Cloud Run entry point)
â”œâ”€â”€ run_pipeline.py            â­ Pipeline orchestrator (data collection)
â”œâ”€â”€ Dockerfile                 ğŸš€ Cloud Run deployment config
â”œâ”€â”€ requirements.txt           ğŸ“¦ Python dependencies
â”œâ”€â”€ engagement_data.db         ğŸ’¾ SQLite database (analytics data)
â”‚
â”œâ”€â”€ utils/                     ğŸ”§ Core utilities (3 files)
â”‚   â”œâ”€â”€ config.py             â†’ Facebook/Google API configs
â”‚   â”œâ”€â”€ db_utils.py           â†’ Database helper functions
â”‚   â””â”€â”€ setup_database.py     â†’ Database schema setup
â”‚
â”œâ”€â”€ collectors/                ğŸ“¥ Data collection scripts (4 files)
â”‚   â”œâ”€â”€ collector_page.py     â†’ Page-level metrics (daily stats)
â”‚   â”œâ”€â”€ collector_ads.py      â†’ Ad campaign data
â”‚   â”œâ”€â”€ backfill_insights.py  â†’ Historical data backfill
â”‚   â””â”€â”€ fetch_2025_data.py    â†’ Date-specific fetching
â”‚
â”œâ”€â”€ analytics/                 ğŸ“Š Data processing (6 files)
â”‚   â”œâ”€â”€ analytics_processor.py â†’ Topic classification & KPI calculation
â”‚   â”œâ”€â”€ analytics_reports.py   â†’ Report generation
â”‚   â”œâ”€â”€ analytics_schema.py    â†’ Database schema definitions
â”‚   â”œâ”€â”€ analytics_trends.py    â†’ Trend analysis
â”‚   â”œâ”€â”€ ad_predictor.py        â†’ Ad performance prediction
â”‚   â””â”€â”€ query_analytics.py     â†’ Flexible query API
â”‚
â”œâ”€â”€ exporters/                 ğŸ“¤ Data export (3 files)
â”‚   â”œâ”€â”€ export_to_sheets.py   â­ Google Sheets export (20+ tabs)
â”‚   â”œâ”€â”€ export_to_docs.py      â†’ Google Docs export (optional)
â”‚   â””â”€â”€ firestore_sync.py      â†’ Firebase sync (optional)
â”‚
â”œâ”€â”€ tests/                     ğŸ§ª Test scripts (5 files)
â”‚   â””â”€â”€ (diagnostic & testing scripts)
â”‚
â”œâ”€â”€ notebooks/                 ğŸ““ Jupyter notebooks (5 files)
â”‚   â””â”€â”€ (data exploration & analysis)
â”‚
â””â”€â”€ fb-dashboard/              ğŸŒ React dashboard (BACKUP)
    â”œâ”€â”€ src/                   â†’ React components
    â”œâ”€â”€ dist/data/             â†’ Static JSON data files
    â”œâ”€â”€ sync/data_sync.py      â†’ Syncs Google Sheets â†’ JSON
    â””â”€â”€ (Firebase hosting configs)
```

---

## ğŸ”„ HOW RAW DATA IS UPDATED

### Automated Daily Flow (Cloud Run)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Cloud Scheduler triggers Cloud Run         â”‚
â”‚         POST request to main.py at 8:00 AM daily    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: main.py runs the pipeline                  â”‚
â”‚         run_pipeline.run_full_pipeline()            â”‚
â”‚                                                      â”‚
â”‚  â”œâ”€ Connects to Facebook Graph API                  â”‚
â”‚  â”œâ”€ Fetches posts (last 30 days)                    â”‚
â”‚  â”œâ”€ Fetches post insights (14 metrics per post)     â”‚
â”‚  â””â”€ Saves to engagement_data.db (SQLite)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Analytics processing                       â”‚
â”‚         analytics_processor.py runs                 â”‚
â”‚                                                      â”‚
â”‚  â”œâ”€ Classifies posts by topic (Climate/Energy/etc) â”‚
â”‚  â”œâ”€ Calculates KPIs (Engagement Rate, etc.)        â”‚
â”‚  â””â”€ Updates benchmarks & trends                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Export to Google Sheets                    â”‚
â”‚         exporters/export_to_sheets.py runs          â”‚
â”‚                                                      â”‚
â”‚  â””â”€ Creates/updates 20+ tabs in Google Sheets      â”‚
â”‚     with latest data + timestamp column             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Data Sources

| Data Source | What it provides | Update frequency |
|-------------|------------------|------------------|
| **Facebook Graph API** | Post metadata, insights, reactions | Daily (automated) |
| **SQLite Database** | Processed analytics, KPIs, trends | Daily (automated) |
| **Google Sheets** | Final reports & visualizations | Daily (automated) |

### Raw Data Tables (3 tabs in Google Sheets)

1. **raw_posts** - Post metadata
   - Columns: post_id, message, created_time, permalink_url, etc.
   - Source: Facebook `/{page-id}/posts` API

2. **raw_post_insights** - Engagement metrics
   - Columns: post_id, clicks, impressions, reactions, video views, etc.
   - Source: Facebook `/{post-id}/insights` API

3. **raw_page_daily** - Page-level daily stats
   - Columns: date, page_fans, post_impressions, etc.
   - Source: Facebook `/{page-id}/insights` API

---

## â• HOW TO ADD NEW TABS TO GOOGLE SHEETS

### Step-by-Step Process

#### 1. Create Export Function in `exporters/export_to_sheets.py`

```python
def export_your_new_tab(client, conn):
    """Export your custom analysis"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        # Get or create worksheet
        try:
            worksheet = spreadsheet.worksheet('your_tab_name')
        except gspread.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(
                title='your_tab_name',
                rows=500,
                cols=10
            )

        # Query data from database
        cursor = conn.cursor()
        cursor.execute("""
            SELECT column1, column2, column3
            FROM your_table
            ORDER BY some_column
        """)
        rows = cursor.fetchall()

        # Prepare data for Google Sheets
        values = [
            ['Column 1', 'Column 2', 'Column 3']  # Header row
        ]

        for row in rows:
            values.append([
                row[0],  # column1
                row[1],  # column2
                row[2]   # column3
            ])

        # Export to sheet (with automatic timestamp column)
        update_with_timestamp(worksheet, 'A1', values)

        print(f"  âœ“ å·²å°å‡º your_tab_name ({len(rows)} records)")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡º your_tab_name å¤±æ•—: {e}")
        return False
```

#### 2. Register Function in `main()` function (same file)

Find the `main()` function around line 2108 and add your export:

```python
def main():
    # ... existing code ...

    # Add your export here (example location)
    print("\nğŸ“Š Custom Analysis:")
    if export_your_new_tab(client, conn):
        success_count += 1

    # Update total_count
    total_count = 22  # Increment by 1

    # ... rest of code ...
```

#### 3. Update Tab Documentation

Edit the `export_tab_documentation()` function around line 2015:

```python
def export_tab_documentation(client):
    # ... existing code ...

    docs = [
        ['Tab Name', 'Category', 'Purpose', 'Update Frequency', 'Key Columns'],

        # ... existing tabs ...

        # Add your new tab
        [
            'your_tab_name',
            'Analytics',  # or 'Raw Data', 'Reports', etc.
            'Description of what this tab shows',
            'Daily',
            'column1, column2, column3'
        ],
    ]
    # ... rest of code ...
```

#### 4. Test Locally

```bash
cd /Users/jinsoon/Desktop/GCAA/03_ç¤¾ç¾¤å®£å‚³/API_Parser
python exporters/export_to_sheets.py
```

#### 5. Deploy to Cloud Run

```bash
# Rebuild Docker image
docker build -t gcr.io/[PROJECT_ID]/facebook-analytics .

# Deploy to Cloud Run
gcloud run deploy facebook-analytics \
  --image gcr.io/[PROJECT_ID]/facebook-analytics \
  --region asia-east1
```

---

## ğŸ—‘ï¸ FILES NOT NEEDED TO RUN THE SYSTEM

### âš ï¸ Can be archived/deleted (but keep fb-dashboard as backup)

#### Documentation Files (Safe to archive)
```
CLEANUP_SUMMARY.md
DEPLOYMENT_GUIDE.md
FLEXIBLE_QUERY_COMPLETE.md
IMPLEMENTATION_COMPLETE.md
LOOKER_STUDIO_GUIDE.md
PHASE_C_COMPLETE.md
Plan for Page-Level Insights Collector.md
QUERY_GUIDE.md
README_ANALYTICS.md
REORGANIZATION_SUMMARY.md
claude code prompt.md
åˆä½µæ–‡ç« ç¸½é›†.md
```

#### Old Test Data (Safe to delete)
```
engagement_data.db.backup  (old backup, can delete if current DB works)
```

#### Dashboard-related (Keep fb-dashboard as backup, others optional)
```
dashboard.html  (standalone HTML, replaced by fb-dashboard)
fb-dashboard/   (KEEP THIS - it's your old React dashboard backup)
```

#### Deployment Scripts (Optional - only needed for manual deployment)
```
deploy.sh
setup-scheduler.sh
daily_run.sh
```

### âœ… CRITICAL FILES - DO NOT DELETE

These files are essential for the system to run:

#### Core System
- `main.py` - Flask API server
- `run_pipeline.py` - Pipeline orchestrator
- `Dockerfile` - Cloud Run deployment
- `requirements.txt` - Python dependencies
- `engagement_data.db` - SQLite database

#### Directories (all files in these are needed)
- `utils/` - Configuration & DB utilities
- `collectors/` - Data collection scripts
- `analytics/` - Data processing
- `exporters/` - Google Sheets export
- `tests/` - Testing & diagnostics
- `notebooks/` - Jupyter analysis (optional but useful)

#### Keep for Reference
- `FINAL_SUMMARY.md` - Latest project summary
- `SYSTEM_ARCHITECTURE.md` - System architecture doc
- `PROJECT_STATUS.md` - This file!
- `plan.md` - Original development plan
- `CLAUDE.md` - Claude Code instructions

---

## ğŸ¯ QUICK REFERENCE

### Run Full Pipeline Locally
```bash
python run_pipeline.py
```

### Export to Google Sheets Only
```bash
python exporters/export_to_sheets.py
```

### Check What Tabs Are Exported
Look at `exporters/export_to_sheets.py` main() function (line 2108)
Currently exports **21 tabs** total:

**Raw Data (3 tabs)**
- raw_posts
- raw_post_insights
- raw_page_daily

**Analytics - Best Times (3 tabs)**
- best_posting_times_general
- best_posting_times_by_topic
- best_posting_times_by_action

**Analytics - Performance (3 tabs)**
- format_type_performance
- issue_topic_performance
- format_issue_cross

**Analytics - Posts (3 tabs)**
- top_posts
- quadrant_analysis
- deep_dive_metrics

**Analytics - Trends (2 tabs)**
- weekly_trends
- hourly_performance

**Ad Analytics (5 tabs)**
- ad_recommendations
- trending_posts
- organic_vs_paid
- ad_campaigns
- ad_roi_analysis

**Data Export (2 tabs)**
- ad_recommendations_data
- organic_vs_paid_data

**Reports (2 tabs)**
- yearly_posting_analysis
- pipeline_logs

**Documentation (1 tab)**
- ğŸ“– Tab Documentation

### View Google Sheets
Open: "Facebook Insights Metrics_Data Warehouse"
Check the `data_updated_at` column (last column) to see when data was last refreshed

---

## ğŸ”§ MAINTENANCE

### Daily (Automated)
- Cloud Scheduler triggers pipeline at 8:00 AM
- Collects latest Facebook data
- Updates all Google Sheets tabs
- Logs run to `pipeline_logs` tab

### Weekly (Manual check)
- Review Google Sheets for data freshness
- Check Cloud Run logs for errors

### Every 60 Days (Manual)
- Renew Facebook Access Token
- Update `FACEBOOK_ACCESS_TOKEN_BASE64` in Cloud Run

---

## ğŸ“ NEED HELP?

**Common Tasks:**
- Adding new tabs â†’ See "How to Add New Tabs" section above
- Checking data freshness â†’ Open Google Sheets, check `data_updated_at` column
- Running pipeline manually â†’ `python run_pipeline.py`
- Viewing system architecture â†’ Open [SYSTEM_ARCHITECTURE.md](SYSTEM_ARCHITECTURE.md)

**Files to Reference:**
- Full project details â†’ [FINAL_SUMMARY.md](FINAL_SUMMARY.md)
- System architecture â†’ [SYSTEM_ARCHITECTURE.md](SYSTEM_ARCHITECTURE.md)
- Original goals â†’ [claude code prompt.md](claude code prompt.md)
- Development history â†’ [plan.md](plan.md)
