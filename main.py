"""
Facebook Post Insights è‡ªå‹•åŒ–æ•¸æ“šæŠ“å–å·¥å…·
å¾ Facebook Graph API ç²å–ç²‰çµ²å°ˆé çš„è²¼æ–‡å±¤ç´šæ•¸æ“šï¼Œä¸¦å½™æ•´è‡³ Google Sheetsã€‚
"""

import requests
import pandas as pd
import gspread
from google.oauth2 import service_account
import json
import os
import base64
from datetime import datetime, timedelta
import time
from typing import List, Dict, Any, Optional
from flask import Flask, jsonify, request

# ==================== è¨­å®šå€ ====================

# Facebook API è¨­å®š
def get_facebook_token():
    """å–å¾— Facebook Access Tokenï¼ˆæ”¯æ´ base64 ç·¨ç¢¼ï¼‰"""
    token = os.environ.get('FACEBOOK_ACCESS_TOKEN')
    token_base64 = os.environ.get('FACEBOOK_ACCESS_TOKEN_BASE64')

    if token_base64:
        return base64.b64decode(token_base64).decode('utf-8')
    elif token:
        return token
    else:
        return 'EAAPbnmTSpmoBQaz8PQJU8jTcLhsmMaB8cq9pYrvsHeAuzncRMgbRpWauF9vtQLNalTJd4X4idcvdTrFliXZCzxrzZCa9CvXRMCWYf8H4NFy4Fq8aay0G20ZCUxXEneQ0WGfOZBWDGfqJMC64WxwzLHAY3g3REJdmqNZCUPSC4ac6VX8Lu8SzXktQ3dERX4r1C'

FACEBOOK_CONFIG = {
    'app_id': '1085898272974442',
    'page_id': '103640919705348',
    'access_token': get_facebook_token(),
    'api_version': 'v23.0'
}

# Google Sheets è¨­å®š
GOOGLE_SHEETS_CONFIG = {
    'spreadsheet_name': 'Faceboook Insights Metrics_Data Warehouse',
    'worksheet_name': 'raw_data'
}

# è²¼æ–‡å±¤ç´š Insights æŒ‡æ¨™
# æ›´æ–°æ—¥æœŸ: 2026-01-15 - æ–°å¢äº’å‹•èˆ‡è§¸åŠæŒ‡æ¨™
POST_INSIGHTS_METRICS = [
    # è²¼æ–‡äº’å‹•æŒ‡æ¨™
    'post_clicks',
    'post_engaged_users',           # æ–°å¢: èˆ‡è²¼æ–‡äº’å‹•çš„ä¸é‡è¤‡ç”¨æˆ¶æ•¸
    'post_negative_feedback',       # æ–°å¢: è² é¢å›é¥‹æ¬¡æ•¸
    # è²¼æ–‡è§¸åŠæŒ‡æ¨™
    'post_impressions_unique',
    'post_impressions_fan_unique',  # æ–°å¢: ç²‰çµ²è§¸åŠäººæ•¸
    'post_impressions_viral_unique', # æ–°å¢: ç—…æ¯’å¼è§¸åŠäººæ•¸
    # 'post_impressions',  # âœ— å·²æ£„ç”¨
    # 'post_impressions_organic',  # âœ— å·²æ£„ç”¨
    # 'post_impressions_paid',  # âœ— å·²æ£„ç”¨
    # è²¼æ–‡å¿ƒæƒ…åæ‡‰æŒ‡æ¨™
    'post_reactions_like_total',
    'post_reactions_love_total',
    'post_reactions_wow_total',
    'post_reactions_haha_total',
    'post_reactions_sorry_total',
    'post_reactions_anger_total',
    # å½±ç‰‡ç›¸é—œæŒ‡æ¨™
    'post_video_views',
    'post_video_views_organic',
    'post_video_views_paid',
]

# ==================== æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ====================

def test_facebook_api_connection(config: Dict[str, str]) -> bool:
    """æ¸¬è©¦ Facebook API é€£æ¥æ˜¯å¦æ­£å¸¸"""
    try:
        url = f"https://graph.facebook.com/{config['api_version']}/{config['page_id']}"
        params = {
            'access_token': config['access_token'],
            'fields': 'id,name,fan_count'
        }

        response = requests.get(url, params=params)
        response.raise_for_status()

        data = response.json()
        print(f"âœ“ API é€£æ¥æˆåŠŸ")
        print(f"  Page ID: {data.get('id')}")
        print(f"  Page Name: {data.get('name')}")
        print(f"  ç²‰çµ²æ•¸: {data.get('fan_count', 'N/A')}")
        return True

    except requests.exceptions.RequestException as e:
        print(f"âœ— API é€£æ¥å¤±æ•—: {e}")
        return False
    except Exception as e:
        print(f"âœ— æœªé æœŸçš„éŒ¯èª¤: {e}")
        return False


def fetch_page_posts(config: Dict[str, str], since: str, until: str, limit: int = 100) -> Optional[List[Dict]]:
    """å¾ Facebook API ç²å–é é¢è²¼æ–‡åˆ—è¡¨"""
    import calendar
    try:
        url = f"https://graph.facebook.com/{config['api_version']}/{config['page_id']}/posts"

        # è½‰æ›æ—¥æœŸç‚º Unix timestamp (UTC)
        since_dt = datetime.strptime(since, '%Y-%m-%d')
        until_dt = datetime.strptime(until, '%Y-%m-%d')
        since_ts = calendar.timegm(since_dt.timetuple())
        until_ts = calendar.timegm(until_dt.timetuple()) + 86400

        params = {
            'access_token': config['access_token'],
            'fields': 'id,message,created_time,permalink_url',
            'since': since_ts,
            'until': until_ts,
            'limit': limit
        }

        all_posts = []
        print(f"æ­£åœ¨ç²å–è²¼æ–‡åˆ—è¡¨...")
        print(f"æ—¥æœŸç¯„åœ: {since} åˆ° {until}")

        while url:
            response = requests.get(url, params=params)
            response.raise_for_status()

            data = response.json()
            posts = data.get('data', [])

            # ç‚ºæ¯å‰‡è²¼æ–‡ç²å–é¡å¤–è³‡è¨Š
            for post in posts:
                post_id = post['id']

                # ç²å–åæ‡‰ç¸½æ•¸
                reactions_url = f"https://graph.facebook.com/{config['api_version']}/{post_id}/reactions"
                reactions_params = {'access_token': config['access_token'], 'summary': 'total_count', 'limit': 0}
                reactions_response = requests.get(reactions_url, params=reactions_params)
                if reactions_response.status_code == 200:
                    reactions_data = reactions_response.json()
                    post['reactions'] = {'summary': {'total_count': reactions_data.get('summary', {}).get('total_count', 0)}}

                # ç²å–ç•™è¨€ç¸½æ•¸
                comments_url = f"https://graph.facebook.com/{config['api_version']}/{post_id}/comments"
                comments_params = {'access_token': config['access_token'], 'summary': 'total_count', 'limit': 0}
                comments_response = requests.get(comments_url, params=comments_params)
                if comments_response.status_code == 200:
                    comments_data = comments_response.json()
                    post['comments'] = {'summary': {'total_count': comments_data.get('summary', {}).get('total_count', 0)}}

                # ç²å–åˆ†äº«æ•¸
                shares_url = f"https://graph.facebook.com/{config['api_version']}/{post_id}"
                shares_params = {'access_token': config['access_token'], 'fields': 'shares'}
                shares_response = requests.get(shares_url, params=shares_params)
                if shares_response.status_code == 200:
                    shares_data = shares_response.json()
                    post['shares'] = shares_data.get('shares', {})

            all_posts.extend(posts)

            # æª¢æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é 
            paging = data.get('paging', {})
            url = paging.get('next')
            params = {}

            print(f"  å·²ç²å– {len(all_posts)} å‰‡è²¼æ–‡...")

        print(f"âœ“ å…±ç²å– {len(all_posts)} å‰‡è²¼æ–‡")
        return all_posts

    except requests.exceptions.RequestException as e:
        print(f"âœ— API è«‹æ±‚å¤±æ•—: {e}")
        return None
    except Exception as e:
        print(f"âœ— æœªé æœŸçš„éŒ¯èª¤: {e}")
        return None


def fetch_post_insights(config: Dict[str, str], post_id: str, metrics: List[str]) -> Optional[Dict]:
    """å¾ Facebook API ç²å–å–®ä¸€è²¼æ–‡çš„æ´å¯Ÿæ•¸æ“š"""
    try:
        url = f"https://graph.facebook.com/{config['api_version']}/{post_id}/insights"
        params = {
            'access_token': config['access_token'],
            'metric': ','.join(metrics)
        }

        response = requests.get(url, params=params)
        response.raise_for_status()

        data = response.json()
        insights_data = data.get('data', [])

        # å°‡ insights æ•¸æ“šè½‰æ›ç‚ºå­—å…¸æ ¼å¼
        insights_dict = {}
        for metric_data in insights_data:
            metric_name = metric_data.get('name')
            values = metric_data.get('values', [])
            if values:
                value = values[0].get('value', 0)
                insights_dict[metric_name] = value

        return insights_dict

    except requests.exceptions.RequestException:
        return {}
    except Exception as e:
        print(f"ç²å–è²¼æ–‡ {post_id} çš„ insights æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {}


def process_posts_data(posts: List[Dict], page_id: str, page_name: str, fetch_date: str) -> pd.DataFrame:
    """è™•ç†è²¼æ–‡æ•¸æ“šï¼Œè½‰æ›ç‚ºçµæ§‹åŒ–çš„ DataFrame"""
    processed_records = []

    for post in posts:
        record = {
            'fetch_date': fetch_date,
            'post_id': post.get('id', ''),
            'page_id': page_id,
            'page_name': page_name,
            'message': post.get('message', '')[:500],
            'created_time': post.get('created_time', ''),
            'permalink_url': post.get('permalink_url', ''),
        }

        # è™•ç†åæ‡‰æ•¸
        reactions_data = post.get('reactions', {})
        if isinstance(reactions_data, dict):
            record['reactions_count'] = reactions_data.get('summary', {}).get('total_count', 0)
        else:
            record['reactions_count'] = 0

        # è™•ç†ç•™è¨€æ•¸
        comments_data = post.get('comments', {})
        if isinstance(comments_data, dict):
            record['comments_count'] = comments_data.get('summary', {}).get('total_count', 0)
        else:
            record['comments_count'] = 0

        # è™•ç†åˆ†äº«æ•¸
        shares_data = post.get('shares', {})
        if isinstance(shares_data, dict):
            record['shares_count'] = shares_data.get('count', 0)
        else:
            record['shares_count'] = 0

        # è™•ç† insights æ•¸æ“š
        insights = post.get('insights', {})
        for metric_name, value in insights.items():
            record[metric_name] = value

        processed_records.append(record)

    return pd.DataFrame(processed_records)


def setup_google_sheets_client() -> Optional[gspread.Client]:
    """è¨­å®š Google Sheets å®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„æœå‹™å¸³æˆ¶é‡‘é‘°ï¼‰"""
    try:
        # å¾ç’°å¢ƒè®Šæ•¸è®€å–æœå‹™å¸³æˆ¶ JSONï¼ˆæ”¯æ´ base64 ç·¨ç¢¼ï¼‰
        credentials_json = os.environ.get('GCP_SA_CREDENTIALS')
        credentials_base64 = os.environ.get('GCP_SA_CREDENTIALS_BASE64')

        if credentials_base64:
            # å¦‚æœæ˜¯ base64 ç·¨ç¢¼ï¼Œå…ˆè§£ç¢¼
            credentials_json = base64.b64decode(credentials_base64).decode('utf-8')
        elif not credentials_json:
            print("âœ— æ‰¾ä¸åˆ° GCP_SA_CREDENTIALS æˆ– GCP_SA_CREDENTIALS_BASE64 ç’°å¢ƒè®Šæ•¸")
            return None

        # å°‡ JSON å­—ä¸²è½‰æ›ç‚ºå­—å…¸
        credentials_dict = json.loads(credentials_json)

        # è¨­å®šæ¬Šé™ç¯„åœ
        scope = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]

        # å»ºç«‹æœå‹™å¸³æˆ¶æ†‘è­‰
        credentials = service_account.Credentials.from_service_account_info(
            credentials_dict, scopes=scope)

        # å»ºç«‹ gspread å®¢æˆ¶ç«¯
        client = gspread.authorize(credentials)

        print("âœ“ Google Sheets å®¢æˆ¶ç«¯è¨­å®šæˆåŠŸ")
        return client

    except Exception as e:
        print(f"âœ— Google Sheets å®¢æˆ¶ç«¯è¨­å®šå¤±æ•—: {e}")
        return None


def write_to_google_sheets(client: gspread.Client, config: Dict[str, str], df: pd.DataFrame) -> bool:
    """å°‡ DataFrame å¯«å…¥ Google Sheets"""
    try:
        spreadsheet = client.open(config['spreadsheet_name'])
        worksheet = spreadsheet.worksheet(config['worksheet_name'])

        # å®šç¾©æ¬„ä½é †åº
        expected_columns = [
            'fetch_date', 'post_id', 'page_id', 'page_name', 'message', 'created_time', 'permalink_url',
            'reactions_count', 'comments_count', 'shares_count',
            'post_clicks', 'post_engaged_users', 'post_negative_feedback',
            'post_impressions_unique', 'post_impressions_fan_unique', 'post_impressions_viral_unique',
            'post_impressions', 'post_impressions_organic', 'post_impressions_paid',
            'post_reactions_like_total', 'post_reactions_love_total', 'post_reactions_wow_total',
            'post_reactions_haha_total', 'post_reactions_sorry_total', 'post_reactions_anger_total',
            'post_video_views', 'post_video_views_organic', 'post_video_views_paid'
        ]

        # ç¢ºä¿ DataFrame åŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½
        for col in expected_columns:
            if col not in df.columns:
                if col in ['fetch_date', 'post_id', 'page_id', 'page_name', 'message', 'created_time', 'permalink_url']:
                    df[col] = ''
                else:
                    df[col] = 0

        df = df[expected_columns]

        # æª¢æŸ¥å·¥ä½œè¡¨
        existing_data = worksheet.get_all_values()

        if not existing_data or not existing_data[0]:
            worksheet.append_row(expected_columns)
            print("âœ“ å·²å¯«å…¥æ¨™é¡Œåˆ—")
            existing_data = [expected_columns]
        else:
            existing_headers = existing_data[0]
            if existing_headers != expected_columns:
                worksheet.delete_rows(1)
                worksheet.insert_row(expected_columns, 1)
                print("âœ“ å·²æ›´æ–°æ¨™é¡Œåˆ—")
                existing_data = worksheet.get_all_values()

        # æª¢æŸ¥é‡è¤‡è¨˜éŒ„
        existing_records = set()
        if len(existing_data) > 1:
            headers = existing_data[0]
            try:
                post_id_idx = headers.index('post_id')
                fetch_date_idx = headers.index('fetch_date')

                for row in existing_data[1:]:
                    if len(row) > max(post_id_idx, fetch_date_idx):
                        key = f"{row[post_id_idx]}_{row[fetch_date_idx]}"
                        existing_records.add(key)

                print(f"âœ“ å·²è¼‰å…¥ {len(existing_records)} ç­†ç¾æœ‰è¨˜éŒ„")
            except ValueError as e:
                print(f"âš ï¸  ç„¡æ³•æ‰¾åˆ°å¿…è¦æ¬„ä½ç´¢å¼•: {e}")

        # éæ¿¾æ–°è¨˜éŒ„
        new_rows = []
        duplicate_count = 0

        for idx, row in df.iterrows():
            key = f"{row['post_id']}_{row['fetch_date']}"
            if key not in existing_records:
                new_rows.append(row.tolist())
            else:
                duplicate_count += 1

        print(f"\næ•¸æ“šå»é‡çµæœ:")
        print(f"  ç¸½è¨˜éŒ„æ•¸: {len(df)}")
        print(f"  æ–°è¨˜éŒ„: {len(new_rows)}")
        print(f"  é‡è¤‡è¨˜éŒ„ï¼ˆå·²è·³éï¼‰: {duplicate_count}")

        # æ‰¹æ¬¡å¯«å…¥
        if new_rows:
            worksheet.append_rows(new_rows)
            print(f"\nâœ“ å·²å¯«å…¥ {len(new_rows)} ç­†æ–°æ•¸æ“šåˆ° Google Sheets")
            return True
        else:
            print("\nâš ï¸  ç„¡æ–°æ•¸æ“šéœ€è¦å¯«å…¥")
            return True

    except Exception as e:
        print(f"âœ— å¯«å…¥ Google Sheets å¤±æ•—: {e}")
        return False


def main_posts_collection(since_date: str = None, until_date: str = None) -> bool:
    """ä¸»è¦çš„è²¼æ–‡æ•¸æ“šè’é›†æµç¨‹"""

    # è¨­å®šæ—¥æœŸç¯„åœ - æŠ“å–æ‰€æœ‰æ­·å²è³‡æ–™
    if until_date is None:
        until_date = datetime.now().strftime('%Y-%m-%d')
    if since_date is None:
        since_date = '2024-01-01'

    fetch_date = datetime.now().strftime('%Y-%m-%d')

    print(f"=== é–‹å§‹è²¼æ–‡æ•¸æ“šè’é›† ===")
    print(f"æ—¥æœŸç¯„åœ: {since_date} åˆ° {until_date}")
    print(f"åŸ·è¡Œæ—¥æœŸ: {fetch_date}")

    # æ¸¬è©¦ API é€£æ¥
    if not test_facebook_api_connection(FACEBOOK_CONFIG):
        return False

    # è¨­å®š Google Sheets
    sheets_client = setup_google_sheets_client()
    if not sheets_client:
        return False

    # ç²å–é é¢è³‡è¨Š
    try:
        url = f"https://graph.facebook.com/{FACEBOOK_CONFIG['api_version']}/{FACEBOOK_CONFIG['page_id']}"
        params = {'access_token': FACEBOOK_CONFIG['access_token'], 'fields': 'id,name'}
        response = requests.get(url, params=params)
        response.raise_for_status()
        page_data = response.json()
        page_name = page_data.get('name', '')
        print(f"\nç²‰çµ²å°ˆé : {page_name}")
    except Exception as e:
        print(f"âœ— ç„¡æ³•ç²å–é é¢è³‡è¨Š: {e}")
        return False

    # ç²å–è²¼æ–‡åˆ—è¡¨
    print(f"\næ­¥é©Ÿ 1: ç²å–è²¼æ–‡åˆ—è¡¨")
    posts = fetch_page_posts(FACEBOOK_CONFIG, since_date, until_date)

    if posts is None or not posts:
        print("âš ï¸ ç„¡è²¼æ–‡æˆ–ç²å–å¤±æ•—")
        return False

    print(f"âœ“ æˆåŠŸç²å– {len(posts)} å‰‡è²¼æ–‡")

    # ç²å– insights
    print(f"\næ­¥é©Ÿ 2: ç²å–è²¼æ–‡ Insights æ•¸æ“š")
    success_count = 0

    for i, post in enumerate(posts, 1):
        post_id = post.get('id')
        print(f"  è™•ç† {i}/{len(posts)}: {post_id}", end='')

        insights = fetch_post_insights(FACEBOOK_CONFIG, post_id, POST_INSIGHTS_METRICS)

        if insights:
            post['insights'] = insights
            success_count += 1
            print(f" - âœ“")
        else:
            post['insights'] = {}
            print(f" - âŠ˜")

        time.sleep(0.2)

    print(f"\nâœ“ Insights æ•¸æ“šç²å–å®Œæˆ (æˆåŠŸ: {success_count})")

    # è™•ç†æ•¸æ“š
    print(f"\næ­¥é©Ÿ 3: è™•ç†æ•¸æ“š")
    df = process_posts_data(posts, FACEBOOK_CONFIG['page_id'], page_name, fetch_date)
    print(f"âœ“ å·²è™•ç† {len(df)} å‰‡è²¼æ–‡æ•¸æ“š")

    # å¯«å…¥ Google Sheets
    print(f"\næ­¥é©Ÿ 4: å¯«å…¥ Google Sheets")
    success = write_to_google_sheets(sheets_client, GOOGLE_SHEETS_CONFIG, df)

    if success:
        print(f"\nğŸ‰ è²¼æ–‡æ•¸æ“šè’é›†å®Œæˆ!")
        return True
    else:
        print(f"\nâŒ å¯«å…¥å¤±æ•—")
        return False


# ==================== Flask App for Cloud Run ====================

app = Flask(__name__)

@app.route('/', methods=['GET', 'POST'])
def run_collection():
    """Cloud Scheduler æœƒå‘¼å«é€™å€‹ç«¯é»ä¾†è§¸ç™¼å®Œæ•´æ•¸æ“šæ”¶é›†æµç¨‹"""
    try:
        import run_pipeline
        from exporters import export_to_sheets as exporter
        from exporters import firestore_sync

        results = {
            'pipeline': False,
            'export': False,
            'firestore': False
        }

        # Step 1: åŸ·è¡Œ run_pipelineï¼ˆæŠ“å– Facebook è³‡æ–™ + åˆ†æï¼‰
        print("=" * 60)
        print("é–‹å§‹åŸ·è¡Œå®Œæ•´æ•¸æ“šæ”¶é›†æµç¨‹...")
        print("=" * 60)

        try:
            run_pipeline.run_full_pipeline()
            results['pipeline'] = True
            print("âœ“ run_pipeline å®Œæˆ")
        except Exception as e:
            print(f"âœ— run_pipeline å¤±æ•—: {e}")

        # Step 2: åŸ·è¡Œ export_to_sheetsï¼ˆåŒ¯å‡ºåˆ° Google Sheetsï¼‰
        try:
            success = exporter.main()
            results['export'] = success
            print("âœ“ export_to_sheets å®Œæˆ" if success else "âœ— export_to_sheets å¤±æ•—")
        except Exception as e:
            print(f"âœ— export_to_sheets å¤±æ•—: {e}")

        # Step 3: åŸ·è¡Œ firestore_syncï¼ˆåŒæ­¥åˆ° Firestore for real-time dashboardï¼‰
        try:
            success = firestore_sync.sync_all()
            results['firestore'] = success
            print("âœ“ firestore_sync å®Œæˆ" if success else "âœ— firestore_sync å¤±æ•—")
        except Exception as e:
            print(f"âœ— firestore_sync å¤±æ•—: {e}")
            # Firestore sync failure is not critical, don't fail the whole pipeline
            results['firestore'] = False

        # åˆ¤æ–·çµæœ
        if results['pipeline'] and results['export']:
            return jsonify({
                'status': 'success',
                'message': 'å®Œæ•´æ•¸æ“šæ”¶é›†æµç¨‹å·²å®Œæˆ',
                'results': results,
                'timestamp': datetime.now().isoformat()
            }), 200
        else:
            return jsonify({
                'status': 'partial',
                'message': 'éƒ¨åˆ†æµç¨‹å¤±æ•—',
                'results': results,
                'timestamp': datetime.now().isoformat()
            }), 500

    except Exception as e:
        import traceback
        return jsonify({
            'status': 'error',
            'message': str(e),
            'traceback': traceback.format_exc(),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return jsonify({'status': 'healthy'}), 200


@app.route('/analytics', methods=['POST'])
def run_analytics():
    """åŸ·è¡Œæ•¸æ“šåˆ†æè™•ç†ç«¯é»"""
    try:
        from analytics import analytics_processor

        conn = analytics_processor.get_connection()

        # åŸ·è¡Œåˆ†ææµç¨‹
        classified_count = analytics_processor.process_all_posts_classification(conn)
        kpi_count = analytics_processor.calculate_post_kpis(conn)
        analytics_processor.update_benchmarks(conn)

        conn.close()

        return jsonify({
            'status': 'success',
            'message': 'åˆ†æè™•ç†å®Œæˆ',
            'classified_count': classified_count,
            'kpi_count': kpi_count,
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/reports/weekly', methods=['GET'])
def get_weekly_report():
    """å–å¾—é€±å ±ç«¯é»"""
    try:
        from analytics import analytics_reports

        conn = analytics_reports.get_connection()
        report_text = analytics_reports.generate_weekly_report(conn)
        conn.close()

        return jsonify({
            'status': 'success',
            'report': report_text,
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/query', methods=['GET'])
def query_custom():
    """
    è‡ªè¨‚æŸ¥è©¢ç«¯é»

    åƒæ•¸:
        start_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
        granularity: ç²’åº¦ (daily/weekly/monthly, é è¨­: weekly)
        type: æŸ¥è©¢é¡å‹ (trends/topics/time_slots/top_posts/comparison)
        topic: ä¸»é¡Œç¯©é¸ (å¯é¸)
        time_slot: æ™‚æ®µç¯©é¸ (å¯é¸)
        limit: å›å‚³ç­†æ•¸ (é è¨­: 10)

    ç¯„ä¾‹:
        /query?start_date=2025-11-01&end_date=2025-11-30&granularity=weekly&type=trends
        /query?start_date=2025-11-01&end_date=2025-11-30&type=topics
        /query?start_date=2025-11-01&end_date=2025-11-30&type=top_posts&limit=20
    """
    try:
        from analytics import query_analytics

        # è§£æåƒæ•¸
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        granularity = request.args.get('granularity', 'weekly')
        query_type = request.args.get('type', 'trends')
        topic = request.args.get('topic')
        time_slot = request.args.get('time_slot')
        limit = int(request.args.get('limit', 10))

        # é©—è­‰åƒæ•¸
        if not start_date or not end_date:
            # é è¨­æœ€è¿‘ 30 å¤©
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

        conn = query_analytics.get_connection()

        # æ ¹æ“šæŸ¥è©¢é¡å‹åŸ·è¡Œä¸åŒæŸ¥è©¢
        if query_type == 'trends':
            data = query_analytics.query_by_date_range(conn, start_date, end_date, granularity)
        elif query_type == 'topics':
            data = query_analytics.query_topic_performance(conn, start_date, end_date, topic)
        elif query_type == 'time_slots':
            data = query_analytics.query_time_slot_performance(conn, start_date, end_date)
        elif query_type == 'top_posts':
            data = query_analytics.query_top_posts(conn, start_date, end_date, limit, topic, time_slot)
        else:
            conn.close()
            return jsonify({
                'status': 'error',
                'message': f'ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹: {query_type}'
            }), 400

        conn.close()

        return jsonify({
            'status': 'success',
            'query_type': query_type,
            'start_date': start_date,
            'end_date': end_date,
            'granularity': granularity if query_type == 'trends' else None,
            'data': data,
            'count': len(data),
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        import traceback
        return jsonify({
            'status': 'error',
            'message': str(e),
            'traceback': traceback.format_exc(),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/reports/custom', methods=['GET'])
def get_custom_report():
    """
    å–å¾—è‡ªè¨‚å ±è¡¨ç«¯é»

    åƒæ•¸:
        start_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
        granularity: ç²’åº¦ (daily/weekly/monthly, é è¨­: weekly)

    ç¯„ä¾‹:
        /reports/custom?start_date=2025-11-01&end_date=2025-11-30&granularity=weekly
    """
    try:
        from analytics import query_analytics

        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        granularity = request.args.get('granularity', 'weekly')

        # é è¨­æœ€è¿‘ 30 å¤©
        if not start_date or not end_date:
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

        conn = query_analytics.get_connection()
        report_text = query_analytics.generate_custom_report(conn, start_date, end_date, granularity)
        conn.close()

        return jsonify({
            'status': 'success',
            'start_date': start_date,
            'end_date': end_date,
            'granularity': granularity,
            'report': report_text,
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/export-sheets', methods=['POST'])
def export_to_sheets():
    """å°å‡ºå ±è¡¨è‡³ Google Sheets ç«¯é»"""
    try:
        import export_to_sheets as exporter

        success = exporter.main()

        if success:
            return jsonify({
                'status': 'success',
                'message': 'å ±è¡¨å·²æˆåŠŸå°å‡ºè‡³ Google Sheets',
                'timestamp': datetime.now().isoformat()
            }), 200
        else:
            return jsonify({
                'status': 'error',
                'message': 'éƒ¨åˆ†å ±è¡¨å°å‡ºå¤±æ•—',
                'timestamp': datetime.now().isoformat()
            }), 500

    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
