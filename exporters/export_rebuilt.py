"""
Facebook Analytics Export - Rebuilt Version
Exports comprehensive analytics to Google Sheets with clear, actionable insights.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import gspread
from google.oauth2 import service_account
import json
import os
import base64
import sqlite3
from datetime import datetime, timedelta

# Configuration
SPREADSHEET_NAME = 'Facebook Insights Metrics_Data Warehouse'
DB_PATH = 'data/engagement_data.db'


def get_connection():
    """Get SQLite database connection"""
    project_root = Path(__file__).parent.parent
    db_path = project_root / 'data' / 'engagement_data.db'
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def setup_google_sheets_client():
    """Set up Google Sheets client"""
    try:
        credentials_json = os.environ.get('GCP_SA_CREDENTIALS')
        credentials_base64 = os.environ.get('GCP_SA_CREDENTIALS_BASE64')

        if credentials_base64:
            credentials_json = base64.b64decode(credentials_base64).decode('utf-8')
        elif not credentials_json:
            print("âš ï¸  æ‰¾ä¸åˆ° Google Sheets æ†‘è­‰ç’°å¢ƒè®Šæ•¸")
            return None

        credentials_dict = json.loads(credentials_json)
        scope = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]
        credentials = service_account.Credentials.from_service_account_info(
            credentials_dict, scopes=scope)
        client = gspread.authorize(credentials)
        print("âœ“ Google Sheets å®¢æˆ¶ç«¯è¨­å®šæˆåŠŸ")
        return client

    except Exception as e:
        print(f"âœ— Google Sheets å®¢æˆ¶ç«¯è¨­å®šå¤±æ•—: {e}")
        return None


def delete_all_worksheets(client, keep_sheets=None):
    """Delete all worksheets except specified ones"""
    keep_sheets = keep_sheets or []
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)
        worksheets = spreadsheet.worksheets()
        
        for ws in worksheets:
            if ws.title not in keep_sheets:
                try:
                    spreadsheet.del_worksheet(ws)
                    print(f"  ğŸ—‘ï¸ åˆªé™¤: {ws.title}")
                except Exception as e:
                    print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {ws.title}: {e}")
        
        return True
    except Exception as e:
        print(f"âœ— åˆªé™¤å·¥ä½œè¡¨å¤±æ•—: {e}")
        return False


def create_worksheet(client, name, rows=1000, cols=20):
    """Create or get worksheet"""
    spreadsheet = client.open(SPREADSHEET_NAME)
    try:
        ws = spreadsheet.worksheet(name)
        ws.clear()
    except gspread.exceptions.WorksheetNotFound:
        ws = spreadsheet.add_worksheet(title=name, rows=rows, cols=cols)
    return ws


def format_header(ws, col_range):
    """Format header row with styling"""
    ws.format(col_range, {
        "backgroundColor": {"red": 0.2, "green": 0.4, "blue": 0.7},
        "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}},
        "horizontalAlignment": "CENTER"
    })


def export_raw_posts(client, conn):
    """Export raw posts data"""
    ws = create_worksheet(client, 'ğŸ“‹ Raw Posts')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT 
            p.post_id,
            datetime(substr(p.created_time, 1, 19)) as published_at,
            substr(p.message, 1, 200) as content_preview,
            p.permalink_url,
            pc.format_type,
            pc.issue_topic,
            pc.media_type,
            i.post_impressions_unique as reach,
            i.likes_count,
            i.comments_count,
            i.shares_count,
            i.post_clicks,
            pp.engagement_rate,
            pp.performance_tier
        FROM posts p
        LEFT JOIN posts_classification pc ON p.post_id = pc.post_id
        LEFT JOIN post_insights_snapshots i ON p.post_id = i.post_id
        LEFT JOIN posts_performance pp ON p.post_id = pp.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = p.post_id)
        ORDER BY p.created_time DESC
    """)
    
    rows = cursor.fetchall()
    
    headers = ['Post ID', 'ç™¼å¸ƒæ™‚é–“', 'å…§å®¹é è¦½', 'é€£çµ', 'è¡Œå‹•é¡å‹', 'è­°é¡Œ', 'åª’é«”é¡å‹',
               'è§¸åŠ', 'è®šæ•¸', 'ç•™è¨€', 'åˆ†äº«', 'é»æ“Š', 'äº’å‹•ç‡%', 'è¡¨ç¾ç­‰ç´š']
    data = [headers]
    
    for row in rows:
        data.append(list(row))
    
    ws.update('A1', data)
    format_header(ws, 'A1:N1')
    
    print(f"  âœ“ Raw Posts: {len(rows)} ç­†")
    return len(rows)


def export_raw_insights(client, conn):
    """Export raw insights snapshots"""
    ws = create_worksheet(client, 'ğŸ“Š Raw Insights')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT 
            p.post_id,
            datetime(substr(p.created_time, 1, 19)) as published_at,
            i.fetch_date,
            i.post_impressions_unique as reach,
            i.likes_count,
            i.comments_count,
            i.shares_count,
            i.post_clicks,
            i.post_video_views,
            i.post_reactions_like_total as like_reactions,
            i.post_reactions_love_total as love,
            i.post_reactions_wow_total as wow,
            i.post_reactions_haha_total as haha,
            i.post_reactions_sorry_total as sad,
            i.post_reactions_anger_total as angry,
            p.permalink_url
        FROM post_insights_snapshots i
        JOIN posts p ON i.post_id = p.post_id
        WHERE i.fetch_date >= '2025-12-01'
        ORDER BY p.created_time DESC, i.fetch_date DESC
    """)
    
    rows = cursor.fetchall()
    
    headers = ['Post ID', 'ç™¼å¸ƒæ™‚é–“', 'æŠ“å–æ—¥æœŸ', 'è§¸åŠ', 'è®š', 'ç•™è¨€', 'åˆ†äº«', 'é»æ“Š',
               'å½±ç‰‡è§€çœ‹', 'ğŸ‘', 'â¤ï¸', 'ğŸ˜®', 'ğŸ˜‚', 'ğŸ˜¢', 'ğŸ˜¡', 'é€£çµ']
    data = [headers]
    
    for row in rows:
        data.append(list(row))
    
    ws.update('A1', data)
    format_header(ws, 'A1:P1')
    
    print(f"  âœ“ Raw Insights: {len(rows)} ç­†")
    return len(rows)


def export_performance_summary(client, conn):
    """Export overall performance summary"""
    ws = create_worksheet(client, 'ğŸ¯ Performance Summary')
    cursor = conn.cursor()
    
    # Overall stats
    cursor.execute("""
        SELECT 
            COUNT(*) as total_posts,
            SUM(i.post_impressions_unique) as total_reach,
            SUM(i.likes_count + i.comments_count + i.shares_count) as total_engagement,
            AVG(pp.engagement_rate) as avg_er,
            SUM(i.post_clicks) as total_clicks,
            SUM(i.shares_count) as total_shares
        FROM posts p
        JOIN post_insights_snapshots i ON p.post_id = i.post_id
        JOIN posts_performance pp ON p.post_id = pp.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = p.post_id)
    """)
    overall = cursor.fetchone()
    
    # Performance tier breakdown
    cursor.execute("""
        SELECT 
            pp.performance_tier,
            COUNT(*) as count,
            AVG(pp.engagement_rate) as avg_er,
            AVG(i.post_impressions_unique) as avg_reach
        FROM posts_performance pp
        JOIN post_insights_snapshots i ON pp.post_id = i.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = pp.post_id)
        GROUP BY pp.performance_tier
        ORDER BY avg_er DESC
    """)
    tiers = cursor.fetchall()
    
    data = [
        ['ğŸ“Š æ•´é«”è¡¨ç¾æ‘˜è¦', '', '', ''],
        ['', '', '', ''],
        ['æŒ‡æ¨™', 'æ•¸å€¼', '', ''],
        ['ç¸½è²¼æ–‡æ•¸', overall['total_posts'], '', ''],
        ['ç¸½è§¸åŠäººæ•¸', f"{overall['total_reach']:,}", '', ''],
        ['ç¸½äº’å‹•æ•¸', f"{overall['total_engagement']:,}", '', ''],
        ['å¹³å‡äº’å‹•ç‡', f"{overall['avg_er']:.2f}%", '', ''],
        ['ç¸½é»æ“Šæ•¸', f"{overall['total_clicks']:,}", '', ''],
        ['ç¸½åˆ†äº«æ•¸', f"{overall['total_shares']:,}", '', ''],
        ['', '', '', ''],
        ['ğŸ“ˆ è¡¨ç¾ç­‰ç´šåˆ†å¸ƒ', '', '', ''],
        ['ç­‰ç´š', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡%', 'å¹³å‡è§¸åŠ'],
    ]
    
    tier_names = {'viral': 'ğŸ”¥ ç†±é–€', 'high': 'â­ å„ªè³ª', 'average': 'ğŸ“Œ ä¸€èˆ¬', 'low': 'ğŸ“‰ å¾…æ”¹é€²'}
    for t in tiers:
        data.append([
            tier_names.get(t['performance_tier'], t['performance_tier']),
            t['count'],
            f"{t['avg_er']:.2f}",
            int(t['avg_reach'])
        ])
    
    ws.update('A1', data)
    ws.format('A1:D1', {
        "backgroundColor": {"red": 0.9, "green": 0.5, "blue": 0.2},
        "textFormat": {"bold": True, "fontSize": 14}
    })
    
    print("  âœ“ Performance Summary")
    return 1


def export_best_times(client, conn):
    """Export best posting times analysis"""
    ws = create_worksheet(client, 'â° Best Times')
    cursor = conn.cursor()
    
    # By hour
    cursor.execute("""
        SELECT 
            pc.hour_of_day as hour,
            COUNT(*) as posts,
            AVG(pp.engagement_rate) as avg_er,
            AVG(i.post_impressions_unique) as avg_reach
        FROM posts_classification pc
        JOIN posts_performance pp ON pc.post_id = pp.post_id
        JOIN post_insights_snapshots i ON pc.post_id = i.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = pc.post_id)
        GROUP BY pc.hour_of_day
        ORDER BY avg_er DESC
    """)
    hourly = cursor.fetchall()
    
    # By day of week
    cursor.execute("""
        SELECT 
            pc.day_of_week,
            COUNT(*) as posts,
            AVG(pp.engagement_rate) as avg_er
        FROM posts_classification pc
        JOIN posts_performance pp ON pc.post_id = pp.post_id
        WHERE pc.day_of_week IS NOT NULL
        GROUP BY pc.day_of_week
        ORDER BY avg_er DESC
    """)
    daily = cursor.fetchall()
    
    day_names = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']
    
    data = [
        ['â° æœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æ', '', '', '', ''],
        ['', '', '', '', ''],
        ['ğŸ• ä¾å°æ™‚ (äº’å‹•ç‡æ’åº)', '', '', '', ''],
        ['æ™‚é–“', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡%', 'å¹³å‡è§¸åŠ', 'å»ºè­°'],
    ]
    
    for i, h in enumerate(hourly):
        hour = h['hour']
        time_str = f"{hour:02d}:00"
        suggestion = 'â­ æ¨è–¦' if i < 3 else ''
        data.append([time_str, h['posts'], f"{h['avg_er']:.2f}", int(h['avg_reach']), suggestion])
    
    data.extend([
        ['', '', '', '', ''],
        ['ğŸ“… ä¾æ˜ŸæœŸ (äº’å‹•ç‡æ’åº)', '', '', '', ''],
        ['æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡%', '', ''],
    ])
    
    for d in daily:
        day_idx = d['day_of_week']
        day_name = day_names[day_idx] if 0 <= day_idx < 7 else str(day_idx)
        data.append([day_name, d['posts'], f"{d['avg_er']:.2f}", '', ''])
    
    ws.update('A1', data)
    format_header(ws, 'A1:E1')
    
    print("  âœ“ Best Times")
    return 1


def export_content_analysis(client, conn):
    """Export content type analysis"""
    ws = create_worksheet(client, 'ğŸ“ Content Analysis')
    cursor = conn.cursor()
    
    # By action type
    cursor.execute("""
        SELECT 
            COALESCE(pc.format_type, 'unclassified') as action_type,
            COUNT(*) as posts,
            AVG(pp.engagement_rate) as avg_er,
            SUM(i.shares_count) as total_shares,
            AVG(i.post_impressions_unique) as avg_reach
        FROM posts_classification pc
        JOIN posts_performance pp ON pc.post_id = pp.post_id
        JOIN post_insights_snapshots i ON pc.post_id = i.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = pc.post_id)
        GROUP BY pc.format_type
        ORDER BY avg_er DESC
    """)
    actions = cursor.fetchall()
    
    # By topic
    cursor.execute("""
        SELECT 
            COALESCE(pc.issue_topic, 'other') as topic,
            COUNT(*) as posts,
            AVG(pp.engagement_rate) as avg_er,
            SUM(i.shares_count) as total_shares
        FROM posts_classification pc
        JOIN posts_performance pp ON pc.post_id = pp.post_id
        JOIN post_insights_snapshots i ON pc.post_id = i.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = pc.post_id)
        GROUP BY pc.issue_topic
        ORDER BY avg_er DESC
    """)
    topics = cursor.fetchall()
    
    action_names = {
        'event': 'ğŸ“… æ´»å‹•', 'press': 'ğŸ“° è¨˜è€…æœƒ', 'statement': 'ğŸ“œ è²æ˜ç¨¿',
        'opinion': 'ğŸ’­ è§€é»', 'op_ed': 'âœï¸ æŠ•æ›¸', 'report': 'ğŸ“Š å ±å‘Š',
        'booth': 'ğŸª æ“ºæ”¤', 'edu': 'ğŸ“š ç§‘æ™®', 'action': 'ğŸ“¢ è¡Œå‹•è™Ÿå¬',
        'unclassified': 'â“ æœªåˆ†é¡'
    }
    
    topic_names = {
        'nuclear': 'â˜¢ï¸ æ ¸èƒ½', 'climate': 'ğŸŒ æ°£å€™', 'net_zero': 'ğŸ¯ æ·¨é›¶',
        'industry': 'ğŸ­ ç”¢æ¥­', 'renewable': 'ğŸŒ± å†ç”Ÿèƒ½æº', 'other': 'ğŸ“Œ å…¶ä»–'
    }
    
    data = [
        ['ğŸ“ å…§å®¹é¡å‹åˆ†æ', '', '', '', ''],
        ['', '', '', '', ''],
        ['ğŸ¬ ä¾è¡Œå‹•é¡å‹', '', '', '', ''],
        ['é¡å‹', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡%', 'ç¸½åˆ†äº«', 'å¹³å‡è§¸åŠ'],
    ]
    
    for a in actions:
        name = action_names.get(a['action_type'], a['action_type'])
        data.append([name, a['posts'], f"{a['avg_er']:.2f}", a['total_shares'], int(a['avg_reach'])])
    
    data.extend([
        ['', '', '', '', ''],
        ['ğŸ·ï¸ ä¾è­°é¡Œ', '', '', '', ''],
        ['è­°é¡Œ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡%', 'ç¸½åˆ†äº«', ''],
    ])
    
    for t in topics:
        name = topic_names.get(t['topic'], t['topic'])
        data.append([name, t['posts'], f"{t['avg_er']:.2f}", t['total_shares'], ''])
    
    ws.update('A1', data)
    format_header(ws, 'A1:E1')
    
    print("  âœ“ Content Analysis")
    return 1


def export_top_posts(client, conn):
    """Export top performing posts"""
    ws = create_worksheet(client, 'ğŸ† Top Posts')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT 
            substr(p.message, 1, 100) as preview,
            datetime(substr(p.created_time, 1, 19)) as published,
            pc.format_type,
            pc.issue_topic,
            i.post_impressions_unique as reach,
            pp.engagement_rate,
            i.likes_count + i.comments_count + i.shares_count as total_engagement,
            i.shares_count,
            p.permalink_url
        FROM posts p
        JOIN posts_classification pc ON p.post_id = pc.post_id
        JOIN posts_performance pp ON p.post_id = pp.post_id
        JOIN post_insights_snapshots i ON p.post_id = i.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = p.post_id)
        ORDER BY pp.engagement_rate DESC
        LIMIT 50
    """)
    
    rows = cursor.fetchall()
    
    headers = ['å…§å®¹é è¦½', 'ç™¼å¸ƒæ™‚é–“', 'è¡Œå‹•', 'è­°é¡Œ', 'è§¸åŠ', 'äº’å‹•ç‡%', 'ç¸½äº’å‹•', 'åˆ†äº«', 'é€£çµ']
    data = [headers]
    
    for row in rows:
        data.append(list(row))
    
    ws.update('A1', data)
    format_header(ws, 'A1:I1')
    
    print(f"  âœ“ Top Posts: {len(rows)} ç­†")
    return len(rows)


def export_monthly_trends(client, conn):
    """Export monthly trends"""
    ws = create_worksheet(client, 'ğŸ“ˆ Monthly Trends')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT 
            strftime('%Y-%m', p.created_time) as month,
            COUNT(*) as posts,
            SUM(i.post_impressions_unique) as total_reach,
            AVG(pp.engagement_rate) as avg_er,
            SUM(i.shares_count) as total_shares
        FROM posts p
        JOIN post_insights_snapshots i ON p.post_id = i.post_id
        JOIN posts_performance pp ON p.post_id = pp.post_id
        WHERE i.fetch_date = (SELECT MAX(fetch_date) FROM post_insights_snapshots WHERE post_id = p.post_id)
        GROUP BY strftime('%Y-%m', p.created_time)
        ORDER BY month DESC
    """)
    
    rows = cursor.fetchall()
    
    headers = ['æœˆä»½', 'è²¼æ–‡æ•¸', 'ç¸½è§¸åŠ', 'å¹³å‡äº’å‹•ç‡%', 'ç¸½åˆ†äº«']
    data = [headers]
    
    for row in rows:
        data.append([row['month'], row['posts'], row['total_reach'], f"{row['avg_er']:.2f}", row['total_shares']])
    
    ws.update('A1', data)
    format_header(ws, 'A1:E1')
    
    print(f"  âœ“ Monthly Trends: {len(rows)} å€‹æœˆ")
    return len(rows)


def main():
    """Main export function"""
    print("\n" + "="*60)
    print("ğŸ”„ Facebook Analytics Export - Rebuilt Version")
    print("="*60)
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Setup
    client = setup_google_sheets_client()
    if not client:
        return False
    
    conn = get_connection()
    
    # Delete old worksheets
    print("\nğŸ—‘ï¸ åˆªé™¤èˆŠå·¥ä½œè¡¨...")
    delete_all_worksheets(client, keep_sheets=[])
    
    # Create a blank sheet first (spreadsheet needs at least one)
    spreadsheet = client.open(SPREADSHEET_NAME)
    try:
        spreadsheet.add_worksheet(title='_temp', rows=1, cols=1)
    except:
        pass
    
    # Export new analytics
    print("\nğŸ“Š åŒ¯å‡ºæ–°åˆ†æå ±è¡¨...")
    
    try:
        export_performance_summary(client, conn)
        export_best_times(client, conn)
        export_content_analysis(client, conn)
        export_top_posts(client, conn)
        export_monthly_trends(client, conn)
        export_raw_posts(client, conn)
        export_raw_insights(client, conn)
        
        # Delete temp sheet
        try:
            temp = spreadsheet.worksheet('_temp')
            spreadsheet.del_worksheet(temp)
        except:
            pass
        
        print("\n" + "="*60)
        print("âœ… åŒ¯å‡ºå®Œæˆ!")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ åŒ¯å‡ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        conn.close()


if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
