"""
Facebook ç¤¾ç¾¤æ•¸æ“šåˆ†ææ¡†æ¶ - Google Sheets å°å‡ºå·¥å…·
å°‡åˆ†æå ±è¡¨å°å‡ºåˆ° Google Sheets ä»¥ä¾¿è¦–è¦ºåŒ–èˆ‡åˆ†äº«
"""

import sys
from pathlib import Path
# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

import gspread
from google.oauth2 import service_account
import json
import os
import base64
import re
from datetime import datetime, timedelta
from analytics import analytics_reports, analytics_trends, ad_predictor



def add_timestamp_column(data):
    """Add 'data_updated_at' timestamp column to all rows"""
    if not data:
        return data
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # Add header to first row
    if data and len(data) > 0:
        data[0].append('data_updated_at')
        
        # Add timestamp to all data rows  
        for i in range(1, len(data)):
            data[i].append(timestamp)
    
    return data


# Google Sheets è¨­å®š
SPREADSHEET_NAME = 'Facebook Insights Metrics_Data Warehouse'
ANALYTICS_WORKSHEET_NAME = 'analytics_dashboard'


# ==================== è¼”åŠ©å‡½æ•¸ ====================

def update_with_timestamp(worksheet, range_name, values):
    """Wrapper to add timestamp column to all worksheet updates"""
    if not values or len(values) == 0:
        worksheet.update(range_name, values)
        return
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # Make a copy to avoid modifying original
    updated_values = [row[:] if isinstance(row, list) else list(row) for row in values]
    
    # Add header to first row if it exists
    if len(updated_values) > 0:
        updated_values[0].append('data_updated_at')
        
        # Add timestamp to all data rows
        for i in range(1, len(updated_values)):
            updated_values[i].append(timestamp)
    
    worksheet.update(range_name, updated_values)


def convert_to_gmt8(iso_time_str):
    """å°‡ ISO æ™‚é–“è½‰æ›ç‚º GMT+8 æ ¼å¼å­—ä¸²"""
    if not iso_time_str:
        return ''
    try:
        # è§£æ ISO æ ¼å¼ (2024-12-01T12:00:00+0000)
        dt = datetime.fromisoformat(iso_time_str.replace('+0000', '+00:00').replace('Z', '+00:00'))
        # è½‰æ›åˆ° GMT+8 å°ç£æ™‚é–“
        dt_gmt8 = dt + timedelta(hours=8)
        return dt_gmt8.strftime('%Y-%m-%d %H:%M:%S')
    except:
        return iso_time_str[:19] if len(iso_time_str) >= 19 else iso_time_str


def hour_to_12h_format(hour):
    """å°‡ 24 å°æ™‚åˆ¶è½‰æ›ç‚º 12 å°æ™‚åˆ¶"""
    if hour == 0:
        return '12:00 AM'
    elif hour < 12:
        return f'{hour}:00 AM'
    elif hour == 12:
        return '12:00 PM'
    else:
        return f'{hour - 12}:00 PM'


def get_day_name_chinese(day_code):
    """å°‡æ˜ŸæœŸä»£ç¢¼è½‰æ›ç‚ºä¸­æ–‡ (é€±ä¸€ç‚ºç¬¬ä¸€å¤©)"""
    day_map = {
        'Mon': 'é€±ä¸€', 'Tue': 'é€±äºŒ', 'Wed': 'é€±ä¸‰',
        'Thu': 'é€±å››', 'Fri': 'é€±äº”', 'Sat': 'é€±å…­', 'Sun': 'é€±æ—¥',
        0: 'é€±ä¸€', 1: 'é€±äºŒ', 2: 'é€±ä¸‰', 3: 'é€±å››', 4: 'é€±äº”', 5: 'é€±å…­', 6: 'é€±æ—¥'
    }
    return day_map.get(day_code, str(day_code))


def extract_hashtags(message):
    """å¾è¨Šæ¯ä¸­æå– hashtags"""
    if not message:
        return ''
    hashtags = re.findall(r'#[\w\u4e00-\u9fff]+', message)
    return ', '.join(hashtags) if hashtags else ''

# è¡Œå‹•é¡å‹ç¿»è­¯ (åŸ format_type)
FORMAT_TYPE_CHINESE = {
    'event': 'å®šæœŸæ´»å‹•',
    'press': 'è¨˜è€…æœƒ',
    'statement': 'è²æ˜ç¨¿',
    'opinion': 'æ–°èè§€é»',
    'op_ed': 'æŠ•æ›¸',
    'report': 'å ±å‘Šç™¼å¸ƒ',
    'booth': 'æ“ºæ”¤è³‡è¨Š',
    'edu': 'ç§‘æ™®/Podcast',
    'action': 'è¡Œå‹•è™Ÿå¬',
    '': 'å…¶ä»–è¡Œå‹• (ç„¡é—œéµå­—åŒ¹é…)',
    None: 'å…¶ä»–è¡Œå‹• (ç„¡é—œéµå­—åŒ¹é…)'
}

# è­°é¡Œé¡å‹ç¿»è­¯
ISSUE_TOPIC_CHINESE = {
    'nuclear': 'æ ¸èƒ½ç™¼é›»',
    'climate': 'æ°£å€™å•é¡Œ',
    'net_zero': 'æ·¨é›¶æ”¿ç­–',
    'industry': 'ç”¢æ¥­åˆ†æ',
    'renewable': 'èƒ½æºç™¼å±•',
    'other': 'å…¶ä»–è­°é¡Œ',
    '': 'å…¶ä»–è­°é¡Œ (ç„¡é—œéµå­—åŒ¹é…)',
    None: 'å…¶ä»–è­°é¡Œ (ç„¡é—œéµå­—åŒ¹é…)'
}


# æ™‚æ®µç¿»è­¯
TIME_SLOT_CHINESE = {
    'morning': 'æ—©ä¸Š (6-12é»)',
    'noon': 'ä¸­åˆ (12-15é»)',
    'afternoon': 'ä¸‹åˆ (15-18é»)',
    'evening': 'æ™šä¸Š (18-23é»)',
    'night': 'æ·±å¤œ (23-6é»)',
    '': 'æœªçŸ¥',
    None: 'æœªçŸ¥'
}

# è¡¨ç¾ç­‰ç´šç¿»è­¯ (å«å®šç¾©èªªæ˜)
PERFORMANCE_TIER_CHINESE = {
    'viral': 'ç†±é–€ (å‰5%)',
    'high': 'å„ªè³ª (å‰25%)',
    'average': 'ä¸€èˆ¬ (ä¸­é–“50%)',
    'low': 'å¾…æ”¹é€² (å¾Œ25%)',
    '': 'æœªè©•ç´š',
    None: 'æœªè©•ç´š'
}


def translate_format_type(code):
    """å°‡è¡Œå‹•ä»£ç¢¼ç¿»è­¯ç‚ºä¸­æ–‡"""
    return FORMAT_TYPE_CHINESE.get(code, code or 'æœªåˆ†é¡')


def translate_issue_topic(code):
    """å°‡è­°é¡Œä»£ç¢¼ç¿»è­¯ç‚ºä¸­æ–‡"""
    return ISSUE_TOPIC_CHINESE.get(code, code or 'æœªåˆ†é¡')


def translate_time_slot(code):
    """å°‡æ™‚æ®µä»£ç¢¼ç¿»è­¯ç‚ºä¸­æ–‡"""
    return TIME_SLOT_CHINESE.get(code, code or 'æœªçŸ¥')


def translate_performance_tier(code):
    """å°‡è¡¨ç¾ç­‰ç´šç¿»è­¯ç‚ºä¸­æ–‡"""
    return PERFORMANCE_TIER_CHINESE.get(code, code or 'æœªè©•ç´š')


# Credential file path (relative to this file)
CREDENTIALS_FILE = Path(__file__).parent.parent / 'fb-dashboard' / 'esg-reports-collection-9661012923ed.json'

def setup_google_sheets_client():
    """è¨­å®š Google Sheets å®¢æˆ¶ç«¯

    å„ªå…ˆé †åº:
    1. æœ¬åœ° JSON æ†‘è­‰æª”æ¡ˆ (CREDENTIALS_FILE)
    2. ç’°å¢ƒè®Šæ•¸ GCP_SA_CREDENTIALS (JSON å­—ä¸²)
    3. ç’°å¢ƒè®Šæ•¸ GCP_SA_CREDENTIALS_BASE64 (Base64 ç·¨ç¢¼)
    """
    try:
        scope = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]

        # Priority 1: Load from local JSON file
        if CREDENTIALS_FILE.exists():
            print(f"  ä½¿ç”¨æ†‘è­‰æª”æ¡ˆ: {CREDENTIALS_FILE.name}")
            credentials = service_account.Credentials.from_service_account_file(
                str(CREDENTIALS_FILE), scopes=scope)
            client = gspread.authorize(credentials)
            print("âœ“ Google Sheets å®¢æˆ¶ç«¯è¨­å®šæˆåŠŸ (æª”æ¡ˆæ†‘è­‰)")
            return client

        # Priority 2 & 3: Load from environment variables
        credentials_json = os.environ.get('GCP_SA_CREDENTIALS')
        credentials_base64 = os.environ.get('GCP_SA_CREDENTIALS_BASE64')

        if credentials_base64:
            credentials_json = base64.b64decode(credentials_base64).decode('utf-8')
        elif not credentials_json:
            print("âš ï¸  æ‰¾ä¸åˆ° Google Sheets æ†‘è­‰")
            print(f"   å˜—è©¦çš„æ†‘è­‰æª”æ¡ˆè·¯å¾‘: {CREDENTIALS_FILE}")
            print("   æˆ–è¨­å®šç’°å¢ƒè®Šæ•¸: GCP_SA_CREDENTIALS / GCP_SA_CREDENTIALS_BASE64")
            return None

        credentials_dict = json.loads(credentials_json)
        credentials = service_account.Credentials.from_service_account_info(
            credentials_dict, scopes=scope)

        client = gspread.authorize(credentials)
        print("âœ“ Google Sheets å®¢æˆ¶ç«¯è¨­å®šæˆåŠŸ (ç’°å¢ƒè®Šæ•¸)")
        return client

    except Exception as e:
        print(f"âœ— Google Sheets å®¢æˆ¶ç«¯è¨­å®šå¤±æ•—: {e}")
        return None


def export_best_posting_times(client, conn):
    """å°å‡ºæœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æ (å« General / æŒ‰è­°é¡Œ / æŒ‰è¡Œå‹•åˆ†çµ„)"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        # æª¢æŸ¥å·¥ä½œè¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å»ºç«‹
        try:
            worksheet = spreadsheet.worksheet('best_posting_times')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='best_posting_times', rows=200, cols=15)

        # æ¸…ç©ºç¾æœ‰æ•¸æ“š
        worksheet.clear()

        # æ™‚æ®µè½‰æ›å°ç…§è¡¨
        time_slot_map = {
            'morning': '6:00 AM - 12:00 PM',
            'noon': '12:00 PM - 3:00 PM',
            'afternoon': '3:00 PM - 6:00 PM',
            'evening': '6:00 PM - 11:00 PM',
            'night': '11:00 PM - 6:00 AM'
        }

        rows = []

        # === Section 1: General ===
        rows.append(['ğŸ“Š æ•´é«”æœ€ä½³ç™¼æ–‡æ™‚é–“', '', '', '', ''])
        headers = ['æ™‚æ®µ', 'æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)']
        rows.append(headers)

        data_general = analytics_reports.get_best_posting_times(conn, limit=20)
        for item in data_general:
            rows.append([
                time_slot_map.get(item['time_slot'], item['time_slot']),
                get_day_name_chinese(item['day_of_week']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        rows.append(['', '', '', '', ''])  # ç©ºè¡Œ

        # === Section 2: By Issue Topic ===
        rows.append(['ğŸ“Œ æŒ‰è­°é¡Œåˆ†çµ„', '', '', '', '', ''])
        headers_topic = ['è­°é¡Œ', 'æ™‚æ®µ', 'æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)']
        rows.append(headers_topic)

        data_topic = analytics_reports.get_best_posting_times_by_topic(conn, limit=50)
        for item in data_topic:
            rows.append([
                translate_issue_topic(item['issue_topic']),
                time_slot_map.get(item['time_slot'], item['time_slot']),
                get_day_name_chinese(item['day_of_week']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        rows.append(['', '', '', '', '', ''])  # ç©ºè¡Œ

        # === Section 3: By Format Type ===
        rows.append(['ğŸ¯ æŒ‰è¡Œå‹•åˆ†çµ„', '', '', '', '', ''])
        headers_format = ['è¡Œå‹•', 'æ™‚æ®µ', 'æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)']
        rows.append(headers_format)

        data_format = analytics_reports.get_best_posting_times_by_format(conn, limit=50)
        for item in data_format:
            rows.append([
                translate_format_type(item['format_type']),
                time_slot_map.get(item['time_slot'], item['time_slot']),
                get_day_name_chinese(item['day_of_week']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        # å¯«å…¥æ•¸æ“š
        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œåˆ—
        worksheet.format('A1:E1', {
            "backgroundColor": {"red": 0.9, "green": 0.5, "blue": 0.2},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºæœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆGeneral: {len(data_general)}, è­°é¡Œ: {len(data_topic)}, è¡Œå‹•: {len(data_format)}ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºæœ€ä½³ç™¼æ–‡æ™‚é–“å¤±æ•—: {e}")
        return False


def export_format_type_performance(client, conn):
    """å°å‡ºè²¼æ–‡å½¢å¼è¡¨ç¾åˆ†æ (ä¸»é¡Œ)"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('format_type_performance')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='format_type_performance', rows=100, cols=15)

        worksheet.clear()

        data = analytics_reports.get_format_type_performance(conn)

        headers = ['è¡Œå‹•', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)',
                   'å¹³å‡ç•™è¨€ç‡ (%)', 'ç†±é–€æ•¸ (å‰5%)', 'å„ªè³ªæ•¸ (å‰25%)']
        rows = [headers]

        for item in data:
            rows.append([
                translate_format_type(item['format_type']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_share_rate'], 2),
                round(item['avg_comment_rate'], 2),
                item['viral_count'],
                item['high_count']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:G1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè²¼æ–‡å½¢å¼è¡¨ç¾ï¼ˆ{len(data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè²¼æ–‡å½¢å¼è¡¨ç¾å¤±æ•—: {e}")
        return False


def export_issue_topic_performance(client, conn):
    """å°å‡ºè­°é¡Œè¡¨ç¾åˆ†æ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('issue_topic_performance')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='issue_topic_performance', rows=100, cols=15)

        worksheet.clear()

        data = analytics_reports.get_issue_topic_performance(conn)

        headers = ['è­°é¡Œ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)',
                   'å¹³å‡ç•™è¨€ç‡ (%)', 'ç†±é–€æ•¸ (å‰5%)', 'å„ªè³ªæ•¸ (å‰25%)']
        rows = [headers]

        for item in data:
            rows.append([
                translate_issue_topic(item['issue_topic']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_share_rate'], 2),
                round(item['avg_comment_rate'], 2),
                item['viral_count'],
                item['high_count']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:G1', {
            "backgroundColor": {"red": 0.4, "green": 0.7, "blue": 0.4},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè­°é¡Œè¡¨ç¾åˆ†æï¼ˆ{len(data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè­°é¡Œè¡¨ç¾å¤±æ•—: {e}")
        return False


def export_format_issue_cross(client, conn):
    """å°å‡ºè¡Œå‹• Ã— è­°é¡Œäº¤å‰åˆ†æ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('format_issue_cross')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='format_issue_cross', rows=200, cols=15)

        worksheet.clear()

        data = analytics_reports.get_format_issue_cross_performance(conn)

        headers = ['è¡Œå‹•', 'è­°é¡Œ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)', 'é«˜è¡¨ç¾è²¼æ–‡æ•¸']
        rows = [headers]

        for item in data:
            rows.append([
                translate_format_type(item['format_type']),
                translate_issue_topic(item['issue_topic']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_share_rate'], 2),
                item['high_performer_count']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:F1', {
            "backgroundColor": {"red": 0.6, "green": 0.4, "blue": 0.7},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè¡Œå‹•Ã—è­°é¡Œäº¤å‰åˆ†æï¼ˆ{len(data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè¡Œå‹•Ã—è­°é¡Œäº¤å‰åˆ†æå¤±æ•—: {e}")
        return False


# ä¿ç•™èˆŠå‡½æ•¸åç¨±ç›¸å®¹
def export_topic_performance(client, conn):
    """å°å‡ºä¸»é¡Œè¡¨ç¾åˆ†æ (å‘å¾Œç›¸å®¹)"""
    return export_format_type_performance(client, conn)


def export_top_posts(client, conn, days=30, limit=20):
    """å°å‡ºè¡¨ç¾æœ€ä½³è²¼æ–‡"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('top_posts')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='top_posts', rows=100, cols=15)

        worksheet.clear()

        data = analytics_reports.get_top_posts(conn, days=days, limit=limit)

        headers = ['è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ™‚é–“ (GMT+8)', 'è¡Œå‹•', 'è­°é¡Œ', 'æ™‚æ®µ',
                   'äº’å‹•ç‡ (%)', 'è¡¨ç¾ç­‰ç´š', 'ç™¾åˆ†ä½æ•¸', 'è§¸åŠ', 'ç¸½äº’å‹•æ•¸', 'é€£çµ']
        rows = [headers]

        for item in data:
            rows.append([
                item['post_id'][-15:],  # åªé¡¯ç¤ºå¾Œ 15 ç¢¼
                (item['message_preview'] or '')[:50],
                convert_to_gmt8(item['created_time'])[:10],  # åªé¡¯ç¤ºæ—¥æœŸ (GMT+8)
                translate_format_type(item['topic_primary']),  # è¡Œå‹•ä¸­æ–‡
                translate_issue_topic(item.get('issue_topic')),  # è­°é¡Œä¸­æ–‡
                translate_time_slot(item['time_slot']),  # æ™‚æ®µä¸­æ–‡
                round(item['engagement_rate'], 2),
                translate_performance_tier(item['performance_tier']),  # ç­‰ç´šä¸­æ–‡
                round(item['percentile_rank'], 1),
                item['reach'],
                item['total_engagement'],
                item.get('permalink_url', '')  # é€£çµ
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:L1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡º Top è²¼æ–‡ï¼ˆ{len(data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡º Top è²¼æ–‡å¤±æ•—: {e}")
        return False


def export_weekly_trends(client, conn, weeks=104):  # é è¨­å…©å¹´
    """å°å‡ºé€±åº¦è¶¨å‹¢"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('weekly_trends')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='weekly_trends', rows=100, cols=10)

        worksheet.clear()

        data = analytics_reports.get_weekly_trends(conn, weeks=weeks)

        # ä½¿ç”¨æ—¥æœŸç¯„åœæ ¼å¼è€Œéé€±æ¬¡è™Ÿ
        headers = ['é€±æ¬¡ (æ—¥æœŸç¯„åœ)', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'ç¸½è§¸åŠ', 'ç¸½äº’å‹•æ•¸']
        rows = [headers]

        for item in data:
            # é¡¯ç¤º yyyy-mm-dd ~ yyyy-mm-dd æ ¼å¼
            week_range = f"{item.get('week_start', '')} ~ {item.get('week_end', '')}"
            rows.append([
                week_range,
                item['post_count'],
                round(item['avg_er'], 2),
                item['total_reach'],
                item['total_engagement']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:E1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºé€±åº¦è¶¨å‹¢ï¼ˆ{len(data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºé€±åº¦è¶¨å‹¢å¤±æ•—: {e}")
        return False


def export_hourly_performance(client, conn):
    """å°å‡ºæ¯å°æ™‚è¡¨ç¾åˆ†æ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('hourly_performance')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='hourly_performance', rows=100, cols=10)

        worksheet.clear()

        data = analytics_reports.get_hourly_performance(conn)

        headers = ['æ™‚é–“', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)']
        rows = [headers]

        for item in data:
            rows.append([
                hour_to_12h_format(item['hour_of_day']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:D1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºæ¯å°æ™‚è¡¨ç¾ï¼ˆ{len(data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºæ¯å°æ™‚è¡¨ç¾å¤±æ•—: {e}")
        return False


def export_raw_posts(client, conn):
    """å°å‡º posts è¡¨åŸå§‹è³‡æ–™"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('raw_posts')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='raw_posts', rows=1000, cols=10)

        worksheet.clear()

        cursor = conn.cursor()
        cursor.execute("""
            SELECT p.post_id, p.page_id, p.created_time, p.message, p.permalink_url,
                   pc.format_type, pc.issue_topic, pc.media_type
            FROM posts p
            LEFT JOIN posts_classification pc ON p.post_id = pc.post_id
            ORDER BY p.created_time DESC
        """)
        rows_data = cursor.fetchall()

        headers = ['Post ID', 'Page ID', 'ç™¼å¸ƒæ™‚é–“ (GMT+8)', 'å…§å®¹', 'æ¨™ç±¤ (Hashtag)', 'è¡Œå‹•', 'è­°é¡Œ', 'åª’é«”é¡å‹', 'é€£çµ']
        rows = [headers]

        # åª’é«”é¡å‹ç¿»è­¯
        media_type_chinese = {
            'photo': 'åœ–ç‰‡',
            'photos': 'å¤šåœ–',
            'video': 'å½±ç‰‡',
            'link': 'é€£çµ',
            'text': 'ç´”æ–‡å­—',
            'album': 'ç›¸ç°¿',
            None: 'æœªåˆ†é¡',
            '': 'æœªåˆ†é¡'
        }

        for row in rows_data:
            message = row[3] or ''
            rows.append([
                row[0],  # post_id
                row[1],  # page_id
                convert_to_gmt8(row[2]),  # created_time (GMT+8)
                message[:500],  # message (é™åˆ¶é•·åº¦)
                extract_hashtags(message),  # hashtags
                translate_format_type(row[5]),  # format_type â†’ è¡Œå‹•ä¸­æ–‡
                translate_issue_topic(row[6]),  # issue_topic â†’ è­°é¡Œä¸­æ–‡
                media_type_chinese.get(row[7], row[7] or 'æœªåˆ†é¡'),  # media_type
                row[4] or ''  # permalink_url
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:I1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè²¼æ–‡åŸå§‹è³‡æ–™ï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè²¼æ–‡åŸå§‹è³‡æ–™å¤±æ•—: {e}")
        return False


def export_raw_post_insights(client, conn):
    """å°å‡ºè²¼æ–‡å®Œæ•´è³‡æ–™ (åˆä½µ posts + post_insights_snapshots + posts_classification)"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('raw_post_insights')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='raw_post_insights', rows=1000, cols=25)

        worksheet.clear()

        # åˆä½µæ¬„ä½ï¼šåŸºæœ¬è³‡è¨Š + åˆ†é¡ + äº’å‹•æ•¸æ“š
        headers = [
            'Post ID', 'ç™¼å¸ƒæ™‚é–“ (GMT+8)', 'å…§å®¹é è¦½', 'è¡Œå‹•é¡å‹', 'è­°é¡Œé¡å‹',
            'ç¸½è®šæ•¸', 'ç•™è¨€æ•¸', 'åˆ†äº«æ•¸', 'é»æ“Šæ•¸', 'è§¸åŠäººæ•¸',
            'å½±ç‰‡è§€çœ‹', 'è‡ªç„¶è§€çœ‹', 'ä»˜è²»è§€çœ‹',
            'ğŸ‘åæ‡‰', 'â¤ï¸åæ‡‰', 'ğŸ˜®åæ‡‰', 'ğŸ˜†åæ‡‰', 'ğŸ˜¢åæ‡‰', 'ğŸ˜ åæ‡‰',
            'è²¼æ–‡é€£çµ'
        ]

        cursor = conn.cursor()
        # åˆä½µ posts + post_insights_snapshots + posts_classification
        cursor.execute("""
            WITH latest_snapshots AS (
                SELECT post_id, MAX(fetch_date) as latest_date
                FROM post_insights_snapshots
                GROUP BY post_id
            )
            SELECT
                p.post_id, p.created_time, SUBSTR(p.message, 1, 100) as message_preview,
                pc.format_type, pc.issue_topic,
                i.likes_count, i.comments_count, i.shares_count,
                i.post_clicks, i.post_impressions_unique,
                i.post_video_views, i.post_video_views_organic, i.post_video_views_paid,
                i.post_reactions_like_total, i.post_reactions_love_total,
                i.post_reactions_wow_total, i.post_reactions_haha_total,
                i.post_reactions_sorry_total, i.post_reactions_anger_total,
                p.permalink_url
            FROM post_insights_snapshots i
            JOIN latest_snapshots ls ON i.post_id = ls.post_id AND i.fetch_date = ls.latest_date
            JOIN posts p ON i.post_id = p.post_id
            LEFT JOIN posts_classification pc ON p.post_id = pc.post_id
            ORDER BY p.created_time DESC
        """)
        rows_data = cursor.fetchall()

        # è¡Œå‹•/è­°é¡Œç¿»è­¯
        format_map = {
            'event': 'å®šæœŸæ´»å‹•', 'press': 'è¨˜è€…æœƒ', 'statement': 'è²æ˜ç¨¿',
            'opinion': 'æ–°èè§€é»', 'op_ed': 'æŠ•æ›¸', 'report': 'å ±å‘Šç™¼å¸ƒ',
            'booth': 'æ“ºæ”¤è³‡è¨Š', 'edu': 'ç§‘æ™®/Podcast', 'action': 'è¡Œå‹•è™Ÿå¬'
        }
        issue_map = {
            'nuclear': 'æ ¸èƒ½ç™¼é›»', 'climate': 'æ°£å€™å•é¡Œ', 'net_zero': 'æ·¨é›¶æ”¿ç­–',
            'industry': 'ç”¢æ¥­åˆ†æ', 'renewable': 'èƒ½æºç™¼å±•', 'other': 'å…¶ä»–è­°é¡Œ'
        }

        rows = [headers]
        for row in rows_data:
            rows.append([
                row[0],  # post_id
                convert_to_gmt8(row[1]),  # created_time (GMT+8)
                (row[2] or '')[:100],  # message_preview
                format_map.get(row[3], row[3] or ''),  # format_type
                issue_map.get(row[4], row[4] or ''),  # issue_topic
                row[5] or 0, row[6] or 0, row[7] or 0,  # likes, comments, shares
                row[8] or 0, row[9] or 0,  # clicks, reach
                row[10] or 0, row[11] or 0, row[12] or 0,  # video views
                row[13] or 0, row[14] or 0, row[15] or 0, row[16] or 0, row[17] or 0, row[18] or 0,  # reactions
                row[19] or ''  # permalink_url
            ])

        if rows:
            update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œ (21 columns including timestamp)
        worksheet.format('A1:U1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè²¼æ–‡å®Œæ•´è³‡æ–™ï¼ˆ{len(rows_data)} ç­†ï¼Œå«åˆ†é¡èˆ‡äº’å‹•æ•¸æ“šï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè²¼æ–‡å®Œæ•´è³‡æ–™å¤±æ•—: {e}")
        return False


def export_page_daily_metrics(client, conn):
    """å°å‡º page_daily_metrics è¡¨åŸå§‹è³‡æ–™ (å«æ¯æ—¥è²¼æ–‡æ•¸)"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('page_daily_metrics')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='page_daily_metrics', rows=500, cols=15)

        # å®Œæ•´é‡å¯«æ¨¡å¼ï¼Œç¢ºä¿è³‡æ–™ä¸€è‡´
        worksheet.clear()

        # æ–°æ¨™é¡Œï¼šæ—¥æœŸã€è²¼æ–‡æ•¸ã€è§¸åŠäººæ•¸...
        headers = ['æ—¥æœŸ', 'è²¼æ–‡æ•¸', 'è§¸åŠäººæ•¸', 'äº’å‹•æ•¸', 'å½±ç‰‡è§€çœ‹',
                   'è®š', 'æ„›å¿ƒ', 'å“‡', 'å“ˆå“ˆ', 'å—šå—š', 'æ€’', 'ç¸½åæ‡‰æ•¸']

        cursor = conn.cursor()
        # çµåˆ page_daily_metrics èˆ‡ posts è¡¨è¨ˆç®—æ¯æ—¥è²¼æ–‡æ•¸
        cursor.execute("""
            SELECT 
                pdm.date,
                COALESCE(post_counts.post_count, 0) as post_count,
                pdm.page_impressions_unique, 
                pdm.page_post_engagements, 
                pdm.page_video_views,
                pdm.reactions_like, 
                pdm.reactions_love, 
                pdm.reactions_wow,
                pdm.reactions_haha, 
                pdm.reactions_sorry, 
                pdm.reactions_anger, 
                pdm.reactions_total
            FROM page_daily_metrics pdm
            LEFT JOIN (
                SELECT DATE(created_time) as post_date, COUNT(*) as post_count
                FROM posts
                GROUP BY DATE(created_time)
            ) post_counts ON pdm.date = post_counts.post_date
            ORDER BY pdm.date DESC
        """)
        rows_data = cursor.fetchall()

        rows = [headers]
        for row in rows_data:
            rows.append(list(row))

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œ
        worksheet.format('A1:L1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºé é¢æ¯æ—¥æŒ‡æ¨™ï¼ˆ{len(rows_data)} ç­†ï¼Œå«è²¼æ–‡æ•¸ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºé é¢æ¯æ—¥æŒ‡æ¨™å¤±æ•—: {e}")
        return False


def export_raw_ads(client, conn):
    """å°å‡ºå»£å‘ŠåŸå§‹è³‡æ–™ï¼ˆads + ad_insights åˆä½µï¼‰"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('raw_ads')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='raw_ads', rows=500, cols=18)

        worksheet.clear()

        cursor = conn.cursor()

        # æª¢æŸ¥ ads è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ads'")
        if not cursor.fetchone():
            headers = ['å°šç„¡å»£å‘Šè³‡æ–™']
            worksheet.update([headers], 'A1')
            print("  âŠ˜ å»£å‘Šè³‡æ–™è¡¨å°šæœªå»ºç«‹")
            return True

        # åˆä½µ ads èˆ‡ ad_insightsï¼Œå–å¾—æœ€æ–°æ•¸æ“š
        cursor.execute("""
            WITH latest_insights AS (
                SELECT ad_id, MAX(date_stop) as latest_date
                FROM ad_insights
                GROUP BY ad_id
            )
            SELECT
                a.ad_id,
                a.name as ad_name,
                ac.name as campaign_name,
                a.status,
                a.post_id,
                DATE(a.created_time) as created_date,
                ai.date_start,
                ai.date_stop,
                COALESCE(ai.impressions, 0) as impressions,
                COALESCE(ai.reach, 0) as reach,
                COALESCE(ai.clicks, 0) as clicks,
                COALESCE(ai.spend, 0) as spend,
                COALESCE(ai.cpm, 0) as cpm,
                COALESCE(ai.cpc, 0) as cpc,
                COALESCE(ai.ctr, 0) as ctr
            FROM ads a
            LEFT JOIN ad_campaigns ac ON a.campaign_id = ac.campaign_id
            LEFT JOIN latest_insights li ON a.ad_id = li.ad_id
            LEFT JOIN ad_insights ai ON a.ad_id = ai.ad_id AND ai.date_stop = li.latest_date
            ORDER BY ai.spend DESC NULLS LAST, a.created_time DESC
        """)
        rows_data = cursor.fetchall()

        # ç‹€æ…‹ç¿»è­¯
        status_chinese = {
            'ACTIVE': 'é‹è¡Œä¸­',
            'PAUSED': 'å·²æš«åœ',
            'DELETED': 'å·²åˆªé™¤',
            'ARCHIVED': 'å·²å°å­˜',
            'PENDING_REVIEW': 'å¯©æ ¸ä¸­',
            'DISAPPROVED': 'æœªé€šé',
        }

        headers = [
            'å»£å‘Š ID', 'å»£å‘Šåç¨±', 'æ´»å‹•åç¨±', 'ç‹€æ…‹', 'æ¨å»£è²¼æ–‡ ID', 'å»ºç«‹æ—¥æœŸ',
            'çµ±è¨ˆèµ·å§‹', 'çµ±è¨ˆçµæŸ', 'æ›å…‰æ•¸', 'è§¸åŠäººæ•¸', 'é»æ“Šæ•¸',
            'èŠ±è²» (NT$)', 'CPM', 'CPC', 'CTR (%)'
        ]
        rows = [headers]

        for row in rows_data:
            rows.append([
                row[0][-15:] if row[0] else '',  # ad_id
                row[1] or '',  # ad_name
                row[2] or '',  # campaign_name
                status_chinese.get(row[3], row[3] or ''),  # status
                row[4][-15:] if row[4] else '',  # post_id
                row[5] or '',  # created_date
                row[6] or '',  # date_start
                row[7] or '',  # date_stop
                row[8],  # impressions
                row[9],  # reach
                row[10],  # clicks
                round(row[11], 2),  # spend
                round(row[12], 2),  # cpm
                round(row[13], 2),  # cpc
                round(row[14], 2) if row[14] else 0  # ctr
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:O1', {
            "backgroundColor": {"red": 0.8, "green": 0.4, "blue": 0.2},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºå»£å‘ŠåŸå§‹è³‡æ–™ï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºå»£å‘ŠåŸå§‹è³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_quadrant_analysis(client, conn):
    """å°å‡ºè±¡é™åˆ†æè³‡æ–™ï¼ˆç”¨æ–¼ Looker Studio è¦–è¦ºåŒ–ï¼‰"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('quadrant_analysis')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='quadrant_analysis', rows=600, cols=15)

        worksheet.clear()

        data = analytics_reports.get_quadrant_analysis(conn)

        headers = ['è²¼æ–‡ ID', 'ç™¼å¸ƒæ™‚é–“ (GMT+8)', 'è§¸åŠäººæ•¸', 'äº’å‹•ç‡ (%)',
                   'ä¸­ä½æ•¸è§¸åŠ', 'ä¸­ä½æ•¸äº’å‹•ç‡ (%)', 'è±¡é™', 'è­°é¡Œ', 'è¡Œå‹•', 'å…§å®¹é è¦½', 'é€£çµ']
        rows = [headers]

        for item in data:
            rows.append([
                item['post_id'][-18:],  # åªé¡¯ç¤ºå¾Œ 18 ç¢¼
                convert_to_gmt8(item['created_time'])[:10],
                item['reach'],
                round(item['engagement_rate'] * 100, 2),
                item['median_reach'],
                round(item['median_er'] * 100, 2),
                item['quadrant'],
                translate_issue_topic(item['topic_tag']),
                translate_format_type(item['format_type']),
                (item['content_short'] or '')[:40],
                item['permalink_url'] or ''
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œ
        worksheet.format('A1:K1', {
            "backgroundColor": {"red": 0.4, "green": 0.2, "blue": 0.6},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        # çµ±è¨ˆå„è±¡é™æ•¸é‡
        quadrant_counts = {}
        for item in data:
            q = item['quadrant']
            quadrant_counts[q] = quadrant_counts.get(q, 0) + 1

        print(f"  âœ“ å·²å°å‡ºè±¡é™åˆ†æï¼ˆ{len(data)} ç­†ï¼‰")
        for q, count in quadrant_counts.items():
            print(f"    - {q}: {count}")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè±¡é™åˆ†æå¤±æ•—: {e}")
        return False


def export_deep_dive_metrics(client, conn, limit=100):
    """å°å‡ºæ·±åº¦æŒ‡æ¨™åˆ†æ - åŒ…å«æ‰€æœ‰æ ¸å¿ƒ KPI"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('deep_dive_metrics')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='deep_dive_metrics', rows=200, cols=20)

        worksheet.clear()

        cursor = conn.cursor()
        # ä½¿ç”¨ MAX èšåˆï¼ˆèˆ‡ posts_performance KPI è¨ˆç®—ä¸€è‡´ï¼Œé¿å…æ•¸æ“šä¸ä¸€è‡´ï¼‰
        cursor.execute("""
            WITH max_snapshots AS (
                SELECT post_id,
                       MAX(post_impressions_unique) as post_impressions_unique,
                       MAX(likes_count) as likes_count,
                       MAX(comments_count) as comments_count,
                       MAX(shares_count) as shares_count,
                       MAX(post_clicks) as post_clicks
                FROM post_insights_snapshots
                GROUP BY post_id
            )
            SELECT
                p.post_id,
                substr(p.created_time, 1, 10) as post_date,
                SUBSTR(p.message, 1, 80) as message_preview,
                pc.format_type,
                pc.issue_topic,
                pp.engagement_rate,
                pp.share_rate,
                pp.comment_rate,
                pp.click_through_rate,
                pp.virality_score,
                pp.discussion_depth,
                pp.performance_tier,
                pp.percentile_rank,
                ms.post_impressions_unique as reach,
                ms.likes_count,
                ms.comments_count,
                ms.shares_count,
                ms.post_clicks,
                (ms.likes_count + ms.comments_count + ms.shares_count) as total_engagement,
                p.permalink_url
            FROM posts p
            LEFT JOIN posts_classification pc ON p.post_id = pc.post_id
            LEFT JOIN posts_performance pp ON p.post_id = pp.post_id
            LEFT JOIN max_snapshots ms ON p.post_id = ms.post_id
            ORDER BY pp.engagement_rate DESC
            LIMIT ?
        """, (limit,))

        rows_data = cursor.fetchall()

        headers = [
            'è²¼æ–‡ ID', 'ç™¼å¸ƒæ—¥æœŸ', 'å…§å®¹é è¦½', 'è¡Œå‹•', 'è­°é¡Œ',
            'äº’å‹•ç‡ (%)', 'åˆ†äº«ç‡ (%)', 'ç•™è¨€ç‡ (%)', 'é»æ“Šç‡ (%)',
            'ç—…æ¯’æ€§åˆ†æ•¸', 'è¨è«–æ·±åº¦',
            'è¡¨ç¾ç­‰ç´š', 'ç™¾åˆ†ä½æ•¸',
            'è§¸åŠ', 'è®šæ•¸', 'ç•™è¨€æ•¸', 'åˆ†äº«æ•¸', 'é»æ“Šæ•¸', 'ç¸½äº’å‹•æ•¸',
            'é€£çµ'
        ]
        rows = [headers]

        for row in rows_data:
            rows.append([
                row[0][-15:],  # post_id å¾Œ 15 ç¢¼
                row[1],  # post_date
                row[2] or '',  # message_preview
                translate_format_type(row[3]),  # format_type
                translate_issue_topic(row[4]),  # issue_topic
                round(row[5], 2) if row[5] is not None else '',  # engagement_rate
                round(row[6], 2) if row[6] is not None else '',  # share_rate
                round(row[7], 2) if row[7] is not None else '',  # comment_rate
                round(row[8], 2) if row[8] is not None else '',  # click_through_rate
                round(row[9], 2) if row[9] is not None else '',  # virality_score
                round(row[10], 2) if row[10] is not None else '',  # discussion_depth
                translate_performance_tier(row[11]),  # performance_tier
                round(row[12], 1) if row[12] is not None else '',  # percentile_rank
                row[13] or 0,  # reach
                row[14] or 0,  # likes
                row[15] or 0,  # comments
                row[16] or 0,  # shares
                row[17] or 0,  # clicks
                row[18] or 0,  # total_engagement
                row[19] or ''  # permalink
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œ - æ·±è—è‰²
        worksheet.format('A1:T1', {
            "backgroundColor": {"red": 0.2, "green": 0.4, "blue": 0.7},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        # æ¢ä»¶æ ¼å¼åŒ– - äº’å‹•ç‡ (Fæ¬„)
        # ç¶ è‰²: > 3%, é»ƒè‰²: 1-3%, ç´…è‰²: < 1%
        # è¨»ï¼šGoogle Sheets API çš„æ¢ä»¶æ ¼å¼è¼ƒè¤‡é›œï¼Œé€™è£¡åªåšåŸºæœ¬æ ¼å¼åŒ–

        print(f"  âœ“ å·²å°å‡ºæ·±åº¦æŒ‡æ¨™åˆ†æï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºæ·±åº¦æŒ‡æ¨™åˆ†æå¤±æ•—: {e}")
        return False


def export_ad_recommendations(client, conn, limit=50):
    """å°å‡ºæŠ•å»£æ¨è–¦æ¸…å–®ï¼ˆå«æ­·å²æœ€ä½³çµ„åˆå»ºè­°ï¼‰"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ad_recommendations')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ad_recommendations', rows=300, cols=15)

        worksheet.clear()

        # æ›´æ–°æŠ•å»£æ½›åŠ›åˆ†æ•¸
        ad_predictor.update_all_ad_potentials(conn)

        # === Section 1: æ­·å²æœ€ä½³çµ„åˆå»ºè­°ï¼ˆä¾›æœªç™¼å¸ƒå…§å®¹åƒè€ƒï¼‰ ===
        cursor = conn.cursor()
        cursor.execute("""
            SELECT 
                COALESCE(pc.issue_topic, 'æœªåˆ†é¡') as issue_topic,
                COALESCE(pc.format_type, 'æœªåˆ†é¡') as format_type,
                pc.time_slot,
                CASE pc.day_of_week
                    WHEN 0 THEN 'é€±ä¸€' WHEN 1 THEN 'é€±äºŒ' WHEN 2 THEN 'é€±ä¸‰'
                    WHEN 3 THEN 'é€±å››' WHEN 4 THEN 'é€±äº”' WHEN 5 THEN 'é€±å…­' WHEN 6 THEN 'é€±æ—¥'
                END as day_name,
                COUNT(*) as post_count,
                ROUND(AVG(pp.engagement_rate), 2) as avg_er,
                SUM(CASE WHEN pp.performance_tier IN ('viral', 'high') THEN 1 ELSE 0 END) as high_performers
            FROM posts_classification pc
            JOIN posts_performance pp ON pc.post_id = pp.post_id
            GROUP BY pc.issue_topic, pc.format_type, pc.time_slot, pc.day_of_week
            HAVING post_count >= 3
            ORDER BY avg_er DESC
            LIMIT 15
        """)
        best_combos = cursor.fetchall()

        rows = [
            ['ğŸ“Š æ­·å²æœ€ä½³çµ„åˆï¼ˆä¾›æ–°å…§å®¹æŠ•å»£åƒè€ƒï¼‰', '', '', '', '', '', ''],
            ['è­°é¡Œ', 'è¡Œå‹•', 'æ™‚æ®µ', 'æ˜ŸæœŸ', 'æ¨£æœ¬æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'é«˜è¡¨ç¾æ•¸'],
        ]

        for combo in best_combos:
            rows.append([
                translate_issue_topic(combo[0]),
                translate_format_type(combo[1]),
                translate_time_slot(combo[2]),
                combo[3] or 'æœªåˆ†é¡',
                combo[4],
                combo[5],
                combo[6]
            ])

        rows.append(['', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', ''])

        # === Section 2: å·²ç™¼å¸ƒè²¼æ–‡æ¨è–¦ ===
        rows.append(['ğŸ“Œ å·²ç™¼å¸ƒè²¼æ–‡æŠ•å»£æ¨è–¦', '', '', '', '', '', '', '', '', '', '', '', ''])
        headers = [
            'è²¼æ–‡ ID', 'ç™¼å¸ƒæ™‚é–“', 'æŠ•å»£å»ºè­°', 'æ½›åŠ›åˆ†æ•¸', 'è¡¨ç¾ç­‰ç´š',
            'è¡Œå‹•', 'è­°é¡Œ', 'äº’å‹•ç‡åˆ†æ•¸', 'åˆ†äº«ç‡åˆ†æ•¸', 'ç•™è¨€ç‡åˆ†æ•¸',
            'è­°é¡Œå› å­', 'æ™‚æ®µå› å­', 'è²¼æ–‡é€£çµ'
        ]
        rows.append(headers)

        # å–å¾—æ¨è–¦è²¼æ–‡
        recommended = ad_predictor.get_recommended_posts(conn, limit=limit, min_score=40)

        for item in recommended:
            breakdown = item.get('breakdown', {})
            rows.append([
                item['post_id'][-15:],
                convert_to_gmt8(item.get('created_time', ''))[:10],
                item['ad_recommendation'],
                item['ad_potential_score'],
                translate_performance_tier(item['performance_tier']),
                translate_format_type(item['format_type']),
                translate_issue_topic(item['issue_topic']),
                round(breakdown.get('engagement_rate_score', 0), 1),
                round(breakdown.get('share_rate_score', 0), 1),
                round(breakdown.get('comment_rate_score', 0), 1),
                breakdown.get('topic_factor', 1),
                breakdown.get('time_factor', 1),
                item.get('permalink_url', '')
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ­·å²å»ºè­°æ¨™é¡Œ
        worksheet.format('A1:G1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.4},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })
        worksheet.format('A2:G2', {
            "backgroundColor": {"red": 0.3, "green": 0.5, "blue": 0.4},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        # æ ¼å¼åŒ–å·²ç™¼å¸ƒæ¨è–¦æ¨™é¡Œ
        detail_header_row = len(best_combos) + 5
        worksheet.format(f'A{detail_header_row}:M{detail_header_row}', {
            "backgroundColor": {"red": 0.8, "green": 0.4, "blue": 0.2},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })
        worksheet.format(f'A{detail_header_row + 1}:M{detail_header_row + 1}', {
            "backgroundColor": {"red": 0.7, "green": 0.4, "blue": 0.3},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºæŠ•å»£æ¨è–¦æ¸…å–®ï¼ˆæ­·å²çµ„åˆ: {len(best_combos)}, è²¼æ–‡: {len(recommended)}ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºæŠ•å»£æ¨è–¦æ¸…å–®å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_trending_posts(client, conn, hours=96):
    """å°å‡ºè¿‘æœŸç†±é–€è²¼æ–‡ï¼ˆæ­£åœ¨èµ·é£›çš„è²¼æ–‡ï¼‰"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('trending_posts')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='trending_posts', rows=100, cols=12)

        worksheet.clear()

        # å–å¾—ç†±é–€è²¼æ–‡
        trending = analytics_trends.get_trending_posts(conn, hours=hours)

        headers = [
            'è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ™‚é–“', 'å·²ç™¼å¸ƒå°æ™‚æ•¸',
            'ç•¶å‰äº’å‹•æ•¸', 'è§¸åŠ', 'æ¯å°æ™‚äº’å‹•', 'äº’å‹•ç‡ (%)'
        ]
        rows = [headers]

        for item in trending:
            rows.append([
                item['post_id'][-15:],
                (item['message_preview'] or '')[:50],
                item['created_time'][:16] if item['created_time'] else '',
                item['hours_since_post'],
                item['current_engagement'],
                item['reach'] or 0,
                item['engagement_per_hour'],
                item['engagement_rate']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–
        worksheet.format('A1:H1', {
            "backgroundColor": {"red": 0.3, "green": 0.7, "blue": 0.3},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè¿‘æœŸç†±é–€è²¼æ–‡ï¼ˆ{len(trending)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè¿‘æœŸç†±é–€è²¼æ–‡å¤±æ•—: {e}")
        return False


def export_organic_vs_paid(client, conn):
    """å°å‡ºè‡ªç„¶ vs ä»˜è²»è²¼æ–‡æˆæ•ˆæ¯”è¼ƒ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('organic_vs_paid')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='organic_vs_paid', rows=200, cols=20)

        worksheet.clear()

        cursor = conn.cursor()
        
        # å–å¾—æœ€æ–°çš„ post_insights_snapshotsï¼ˆé¿å…é‡è¤‡ï¼‰
        cursor.execute("""
            WITH latest_snapshots AS (
                SELECT post_id, MAX(fetch_date) as latest_date
                FROM post_insights_snapshots
                GROUP BY post_id
            ),
            promoted_posts AS (
                SELECT DISTINCT post_id FROM ads WHERE post_id IS NOT NULL
            )
            SELECT 
                p.post_id,
                SUBSTR(p.message, 1, 60) as message_preview,
                DATE(p.created_time) as post_date,
                CASE WHEN pp.post_id IS NOT NULL THEN 'æœ‰å»£å‘Š' ELSE 'è‡ªç„¶è§¸åŠ' END as ad_status,
                COALESCE(pc.format_type, 'æœªåˆ†é¡') as format_type,
                COALESCE(pc.issue_topic, 'æœªåˆ†é¡') as issue_topic,
                perf.engagement_rate,
                perf.share_rate,
                perf.comment_rate,
                perf.click_through_rate,
                perf.performance_tier,
                i.post_impressions_unique as reach,
                i.likes_count + i.comments_count + i.shares_count as total_engagement,
                i.likes_count,
                i.comments_count,
                i.shares_count,
                i.post_clicks,
                p.permalink_url
            FROM posts p
            JOIN latest_snapshots ls ON p.post_id = ls.post_id
            JOIN post_insights_snapshots i ON p.post_id = i.post_id AND i.fetch_date = ls.latest_date
            LEFT JOIN promoted_posts pp ON p.post_id = pp.post_id
            LEFT JOIN posts_classification pc ON p.post_id = pc.post_id
            LEFT JOIN posts_performance perf ON p.post_id = perf.post_id
            ORDER BY i.post_impressions_unique DESC
            LIMIT 600
        """)
        rows_data = cursor.fetchall()

        # è¨ˆç®—æ‘˜è¦çµ±è¨ˆ
        cursor.execute("""
            WITH latest_snapshots AS (
                SELECT post_id, MAX(fetch_date) as latest_date
                FROM post_insights_snapshots
                GROUP BY post_id
            ),
            promoted_posts AS (
                SELECT DISTINCT post_id FROM ads WHERE post_id IS NOT NULL
            )
            SELECT 
                CASE WHEN pp.post_id IS NOT NULL THEN 'paid' ELSE 'organic' END as ad_status,
                COUNT(*) as post_count,
                ROUND(AVG(perf.engagement_rate), 2) as avg_er,
                ROUND(AVG(perf.share_rate), 2) as avg_sr,
                ROUND(AVG(perf.comment_rate), 2) as avg_cr,
                ROUND(AVG(perf.click_through_rate), 2) as avg_ctr,
                SUM(i.post_impressions_unique) as total_reach,
                SUM(i.likes_count + i.comments_count + i.shares_count) as total_engagement
            FROM posts p
            JOIN latest_snapshots ls ON p.post_id = ls.post_id
            JOIN post_insights_snapshots i ON p.post_id = i.post_id AND i.fetch_date = ls.latest_date
            LEFT JOIN promoted_posts pp ON p.post_id = pp.post_id
            LEFT JOIN posts_performance perf ON p.post_id = perf.post_id
            GROUP BY ad_status
        """)
        summary_data = cursor.fetchall()

        # æº–å‚™æ‘˜è¦å€å¡Š
        rows = [
            ['è‡ªç„¶ vs ä»˜è²»è²¼æ–‡æˆæ•ˆæ¯”è¼ƒ', '', '', '', '', '', '', ''],
            ['ï¼ˆè¡¨ç¾ç­‰ç´šä¾äº’å‹•ç‡ç™¾åˆ†ä½è¨ˆç®—ï¼šå‰5%=ç†±é–€, å‰25%=å„ªè³ª, ä¸­é–“50%=ä¸€èˆ¬, å¾Œ25%=å¾…æ”¹é€²ï¼‰', '', '', '', '', '', '', ''],
            ['', '', '', '', '', '', '', ''],
            ['é¡å‹', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)', 'å¹³å‡ç•™è¨€ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)', 'ç¸½è§¸åŠ', 'ç¸½äº’å‹•æ•¸'],
        ]
        
        for row in summary_data:
            status = 'æœ‰å»£å‘Š' if row[0] == 'paid' else 'è‡ªç„¶è§¸åŠ'
            rows.append([
                status, row[1], row[2] or 0, row[3] or 0, row[4] or 0, row[5] or 0, row[6] or 0, row[7] or 0
            ])
        
        rows.append(['', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', ''])
        
        # è©³ç´°æ•¸æ“šæ¨™é¡Œ
        detail_headers = [
            'è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ—¥æœŸ', 'å»£å‘Šç‹€æ…‹', 'è¡Œå‹•', 'è­°é¡Œ',
            'äº’å‹•ç‡ (%)', 'åˆ†äº«ç‡ (%)', 'ç•™è¨€ç‡ (%)', 'é»æ“Šç‡ (%)',
            'è¡¨ç¾ç­‰ç´š', 'è§¸åŠ', 'ç¸½äº’å‹•', 'è®šæ•¸', 'ç•™è¨€æ•¸', 'åˆ†äº«æ•¸', 'é»æ“Šæ•¸', 'é€£çµ'
        ]
        rows.append(detail_headers)

        for row in rows_data:
            rows.append([
                row[0][-15:],
                row[1] or '',
                row[2] or '',
                row[3],
                translate_format_type(row[4]),
                translate_issue_topic(row[5]),
                round(row[6], 2) if row[6] else 0,
                round(row[7], 2) if row[7] else 0,
                round(row[8], 2) if row[8] else 0,
                round(row[9], 2) if row[9] else 0,
                translate_performance_tier(row[10]),
                row[11] or 0,
                row[12] or 0,
                row[13] or 0,
                row[14] or 0,
                row[15] or 0,
                row[16] or 0,
                row[17] or ''
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ‘˜è¦æ¨™é¡Œ
        worksheet.format('A1:H1', {
            "backgroundColor": {"red": 0.6, "green": 0.2, "blue": 0.6},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 14}
        })
        worksheet.format('A2:H2', {
            "textFormat": {"italic": True, "foregroundColor": {"red": 0.4, "green": 0.4, "blue": 0.4}, "fontSize": 10}
        })
        worksheet.format('A4:H4', {
            "backgroundColor": {"red": 0.4, "green": 0.2, "blue": 0.5},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºè‡ªç„¶ vs ä»˜è²»æ¯”è¼ƒï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè‡ªç„¶ vs ä»˜è²»æ¯”è¼ƒå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_ad_campaigns(client, conn):
    """å°å‡ºå»£å‘Šæ´»å‹•æ¸…å–®"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ad_campaigns')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ad_campaigns', rows=100, cols=12)

        worksheet.clear()

        cursor = conn.cursor()
        
        # æª¢æŸ¥ ad_campaigns è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ad_campaigns'")
        if not cursor.fetchone():
            print("  âŠ˜ å»£å‘Šæ´»å‹•è¡¨å°šæœªå»ºç«‹")
            return True  # éè‡´å‘½éŒ¯èª¤

        cursor.execute("""
            SELECT 
                ac.campaign_id,
                ac.name,
                ac.objective,
                ac.status,
                COALESCE(ac.daily_budget, 0) as daily_budget,
                COALESCE(ac.lifetime_budget, 0) as lifetime_budget,
                DATE(ac.created_time) as created_date,
                COUNT(DISTINCT a.ad_id) as ad_count,
                COALESCE(SUM(ai.spend), 0) as total_spend,
                COALESCE(SUM(ai.impressions), 0) as total_impressions,
                COALESCE(SUM(ai.clicks), 0) as total_clicks,
                CASE WHEN SUM(ai.clicks) > 0 
                     THEN ROUND(SUM(ai.spend) / SUM(ai.clicks), 2) 
                     ELSE 0 END as avg_cpc
            FROM ad_campaigns ac
            LEFT JOIN ads a ON ac.campaign_id = a.campaign_id
            LEFT JOIN ad_insights ai ON a.ad_id = ai.ad_id
            GROUP BY ac.campaign_id
            ORDER BY total_spend DESC
        """)
        rows_data = cursor.fetchall()

        headers = [
            'æ´»å‹• ID', 'æ´»å‹•åç¨±', 'ç›®æ¨™', 'ç‹€æ…‹', 'æ¯æ—¥é ç®— (NT$)', 'ç¸½é ç®— (NT$)',
            'å»ºç«‹æ—¥æœŸ', 'å»£å‘Šæ•¸', 'ç¸½èŠ±è²» (NT$)', 'ç¸½æ›å…‰', 'ç¸½é»æ“Š', 'å¹³å‡ CPC (NT$)'
        ]
        rows = [headers]

        # ç›®æ¨™ç¿»è­¯
        objective_chinese = {
            'OUTCOME_AWARENESS': 'å“ç‰ŒçŸ¥ååº¦',
            'OUTCOME_ENGAGEMENT': 'äº’å‹•æ¨å»£',
            'OUTCOME_TRAFFIC': 'æµé‡å°å¼•',
            'OUTCOME_LEADS': 'åå–®æ”¶é›†',
            'OUTCOME_SALES': 'éŠ·å”®è½‰æ›',
            'LINK_CLICKS': 'é€£çµé»æ“Š',
            'POST_ENGAGEMENT': 'è²¼æ–‡äº’å‹•',
            'PAGE_LIKES': 'ç²‰å°ˆæŒ‰è®š',
            'VIDEO_VIEWS': 'å½±ç‰‡è§€çœ‹',
        }
        
        status_chinese = {
            'ACTIVE': 'é€²è¡Œä¸­',
            'PAUSED': 'æš«åœ',
            'DELETED': 'å·²åˆªé™¤',
            'ARCHIVED': 'å·²å°å­˜',
        }

        for row in rows_data:
            rows.append([
                row[0][-15:] if row[0] else '',
                row[1] or '',
                objective_chinese.get(row[2], row[2] or ''),
                status_chinese.get(row[3], row[3] or ''),
                round(row[4], 0) if row[4] else 0,
                round(row[5], 0) if row[5] else 0,
                row[6] or '',
                row[7] or 0,
                round(row[8], 0) if row[8] else 0,
                row[9] or 0,
                row[10] or 0,
                row[11] or 0
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:L1', {
            "backgroundColor": {"red": 0.2, "green": 0.4, "blue": 0.7},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºå»£å‘Šæ´»å‹•æ¸…å–®ï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºå»£å‘Šæ´»å‹•æ¸…å–®å¤±æ•—: {e}")
        return False


def export_ad_roi_analysis(client, conn):
    """å°å‡ºå»£å‘Š ROI åˆ†æï¼ˆé€ç­†å»£å‘Šï¼‰"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ad_roi_analysis')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ad_roi_analysis', rows=500, cols=18)

        worksheet.clear()

        cursor = conn.cursor()
        
        # æª¢æŸ¥å¿…è¦è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ads'")
        if not cursor.fetchone():
            print("  âŠ˜ å»£å‘Šè¡¨å°šæœªå»ºç«‹")
            return True

        # é€ç­†å»£å‘Š ROI åˆ†æ
        cursor.execute("""
            SELECT 
                a.ad_id,
                a.name as ad_name,
                ac.name as campaign_name,
                a.post_id,
                SUBSTR(p.message, 1, 50) as post_preview,
                COALESCE(pc.format_type, 'æœªåˆ†é¡') as format_type,
                COALESCE(pc.issue_topic, 'æœªåˆ†é¡') as issue_topic,
                a.status,
                COALESCE(ai.impressions, 0) as impressions,
                COALESCE(ai.reach, 0) as reach,
                COALESCE(ai.clicks, 0) as clicks,
                COALESCE(ai.spend, 0) as spend,
                COALESCE(ai.cpm, 0) as cpm,
                COALESCE(ai.cpc, 0) as cpc,
                COALESCE(ai.ctr, 0) as ctr,
                ai.date_start,
                ai.date_stop
            FROM ads a
            LEFT JOIN ad_campaigns ac ON a.campaign_id = ac.campaign_id
            LEFT JOIN posts p ON a.post_id = p.post_id
            LEFT JOIN posts_classification pc ON a.post_id = pc.post_id
            LEFT JOIN (
                SELECT ad_id,
                       SUM(impressions) as impressions,
                       SUM(reach) as reach,
                       SUM(clicks) as clicks,
                       SUM(spend) as spend,
                       CASE WHEN SUM(impressions) > 0
                            THEN ROUND((SUM(spend) / SUM(impressions)) * 1000, 2)
                            ELSE 0 END as cpm,
                       CASE WHEN SUM(clicks) > 0
                            THEN ROUND(SUM(spend) / SUM(clicks), 2)
                            ELSE 0 END as cpc,
                       CASE WHEN SUM(impressions) > 0
                            THEN ROUND((SUM(clicks) / CAST(SUM(impressions) AS FLOAT)) * 100, 2)
                            ELSE 0 END as ctr,
                       MIN(date_start) as date_start,
                       MAX(date_stop) as date_stop
                FROM ad_insights
                GROUP BY ad_id
            ) ai ON a.ad_id = ai.ad_id
            ORDER BY ai.spend DESC NULLS LAST, a.ad_id
        """)
        rows_data = cursor.fetchall()

        headers = [
            'å»£å‘Š ID', 'å»£å‘Šåç¨±', 'æ´»å‹•åç¨±', 'è²¼æ–‡ ID', 'è²¼æ–‡é è¦½',
            'è¡Œå‹•', 'è­°é¡Œ', 'ç‹€æ…‹',
            'æ›å…‰', 'è§¸åŠ', 'é»æ“Š', 'èŠ±è²» (NT$)',
            'CPM', 'CPC', 'CTR (%)',
            'é–‹å§‹æ—¥æœŸ', 'çµæŸæ—¥æœŸ'
        ]
        rows = [headers]

        status_chinese = {
            'ACTIVE': 'é€²è¡Œä¸­',
            'PAUSED': 'æš«åœ',
            'DELETED': 'å·²åˆªé™¤',
            'ARCHIVED': 'å·²å°å­˜',
        }

        for row in rows_data:
            rows.append([
                row[0][-15:] if row[0] else '',
                row[1] or '',
                row[2] or '',
                row[3][-15:] if row[3] else '',
                row[4] or '',
                translate_format_type(row[5]),
                translate_issue_topic(row[6]),
                status_chinese.get(row[7], row[7] or ''),
                row[8] or 0,
                row[9] or 0,
                row[10] or 0,
                round(row[11], 0) if row[11] else 0,
                round(row[12], 2) if row[12] else 0,
                round(row[13], 2) if row[13] else 0,
                round(row[14], 2) if row[14] else 0,
                row[15] or '',
                row[16] or ''
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:Q1', {
            "backgroundColor": {"red": 0.7, "green": 0.3, "blue": 0.3},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        # çµ±è¨ˆæœ‰ç„¡ insights è³‡æ–™
        with_insights = sum(1 for r in rows_data if r[8] and r[8] > 0)
        print(f"  âœ“ å·²å°å‡ºå»£å‘Š ROI åˆ†æï¼ˆ{len(rows_data)} ç­†ï¼Œ{with_insights} ç­†æœ‰ insightsï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºå»£å‘Š ROI åˆ†æå¤±æ•—: {e}")
        return False


def export_ad_recommendations_data(client, conn):
    """å°å‡ºæŠ•å»£æ¨è–¦æ¸…å–® (Flat Sheet for Looker Studio)"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ad_recommendations_data')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ad_recommendations_data', rows=200, cols=12)

        worksheet.clear()

        # å–å¾—æ¨è–¦è²¼æ–‡ (ä½¿ç”¨ ad_predictor)
        # Limit è¨­å¤§ä¸€é»ä»¥ç²å–æ›´å¤šè³‡æ–™ä¾› Looker Studio ç¯©é¸
        recommended = ad_predictor.get_recommended_posts(conn, limit=200, min_score=40)
        
        # æ¨™é¡Œ (Row 1)
        headers = [
            'è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ—¥æœŸ', 'æ½›åŠ›åˆ†æ•¸', 'å»ºè­°', 
            'äº’å‹•ç‡ (%)', 'åˆ†äº«ç‡ (%)', 'ç•™è¨€ç‡ (%)', 'é»æ“Šç‡ (%)',
            'è­°é¡Œå› å­', 'æ™‚æ®µå› å­', 
            'è­°é¡Œ', 'è¡Œå‹•', 'è¡¨ç¾ç­‰ç´š', 'æ¨è–¦åŸå› ', 'é€£çµ'
        ]
        
        rows = [headers]
        
        # è³‡æ–™å…§å®¹
        for item in recommended:
            breakdown = item.get('breakdown', {})
            reason = []
            if breakdown.get('engagement_rate_score', 0) > 30: reason.append('é«˜äº’å‹•')
            if breakdown.get('share_rate_score', 0) > 25: reason.append('é«˜åˆ†äº«')
            if breakdown.get('topic_factor', 1) > 1.1: reason.append('ç†±é–€è­°é¡Œ')
            if breakdown.get('time_factor', 1) > 1.1: reason.append('ç†±é–€æ™‚æ®µ')
            
            rows.append([
                item['post_id'][-15:],
                item.get('message', '')[:50].replace('\n', ' ') if item.get('message') else '',
                convert_to_gmt8(item.get('created_time', ''))[:10],
                item['ad_potential_score'],
                item['ad_recommendation'],
                round(breakdown.get('engagement_rate_score', 0), 2), # é€™è£¡åŸæœ¬æ˜¯åˆ†æ•¸ï¼Œæ”¹ç‚ºå¯¦éš› ER æœƒæ›´å¥½ï¼Œä½†å…ˆç¶­æŒä¸€è‡´
                round(breakdown.get('share_rate_score', 0), 2),
                round(breakdown.get('comment_rate_score', 0), 2),
                0, # CTR ç›®å‰åœ¨ predictor ä¸­å¯èƒ½æ²’æœ‰ç›´æ¥å‚³éï¼Œæš«ç½® 0
                breakdown.get('topic_factor', 1),
                breakdown.get('time_factor', 1),
                translate_issue_topic(item.get('issue_topic')),
                translate_format_type(item.get('format_type')),
                translate_performance_tier(item.get('performance_tier')),
                ','.join(reason),
                item.get('permalink_url', '')
            ])
            
        update_with_timestamp(worksheet, 'A1', rows)
        
        print(f"  âœ“ å·²å°å‡ºæŠ•å»£æ¨è–¦æ¸…å–®è³‡æ–™ç‰ˆ (Looker Ready, {len(rows)-1} ç­†)")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºæŠ•å»£æ¨è–¦æ¸…å–®è³‡æ–™ç‰ˆå¤±æ•—: {e}")
        return False


def export_organic_vs_paid_data(client, conn):
    """å°å‡ºè‡ªç„¶ vs ä»˜è²»æ¯”è¼ƒè³‡æ–™ç‰ˆ (Flat Sheet for Looker Studio)"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('organic_vs_paid_data')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='organic_vs_paid_data', rows=500, cols=18)

        worksheet.clear()

        cursor = conn.cursor()
        
        # å–å¾—è©³ç´°è³‡æ–™ (é‡è¤‡åˆ©ç”¨æ—¢æœ‰ SQL é‚è¼¯ï¼Œä¿®æ­£ latest_insights ç‚ºå­æŸ¥è©¢)
        cursor.execute("""
            SELECT 
                p.post_id,
                SUBSTR(p.message, 1, 50) as post_preview,
                DATE(p.created_time) as created_date,
                CASE WHEN a.post_id IS NOT NULL THEN 'paid' ELSE 'organic' END as ad_status,
                COALESCE(pc.format_type, 'æœªåˆ†é¡') as format_type,
                COALESCE(pc.issue_topic, 'æœªåˆ†é¡') as issue_topic,
                perf.engagement_rate,
                perf.share_rate,
                perf.comment_rate,
                perf.click_through_rate,
                perf.performance_tier,
                i.reach,
                i.total_interactions,
                i.reactions,
                i.comments,
                i.shares,
                i.post_clicks,
                p.permalink_url
            FROM posts p
            LEFT JOIN (SELECT DISTINCT post_id FROM ads) a ON p.post_id = a.post_id
            LEFT JOIN (
                SELECT 
                    post_id,
                    MAX(post_impressions_unique) as reach,
                    MAX(post_clicks) as post_clicks,
                    MAX(post_reactions_like_total + post_reactions_love_total + post_reactions_wow_total + post_reactions_haha_total + post_reactions_sorry_total + post_reactions_anger_total) as reactions,
                    MAX(comments_count) as comments,
                    MAX(shares_count) as shares,
                    (MAX(post_reactions_like_total + post_reactions_love_total + post_reactions_wow_total + post_reactions_haha_total + post_reactions_sorry_total + post_reactions_anger_total) + MAX(comments_count) + MAX(shares_count)) as total_interactions
                FROM post_insights_snapshots
                GROUP BY post_id
            ) i ON p.post_id = i.post_id
            LEFT JOIN posts_classification pc ON p.post_id = pc.post_id
            LEFT JOIN posts_performance perf ON p.post_id = perf.post_id
            ORDER BY created_date DESC
        """)
        rows_data = cursor.fetchall()
        
        # æ¨™é¡Œ (Row 1)
        headers = [
            'è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ—¥æœŸ', 'å»£å‘Šç‹€æ…‹', 'è¡Œå‹•', 'è­°é¡Œ',
            'äº’å‹•ç‡ (%)', 'åˆ†äº«ç‡ (%)', 'ç•™è¨€ç‡ (%)', 'é»æ“Šç‡ (%)',
            'è¡¨ç¾ç­‰ç´š', 'è§¸åŠ', 'ç¸½äº’å‹•', 'è®šæ•¸', 'ç•™è¨€æ•¸', 'åˆ†äº«æ•¸', 'é»æ“Šæ•¸', 'é€£çµ'
        ]
        
        rows = [headers]
        
        # è³‡æ–™å…§å®¹
        for row in rows_data:
            rows.append([
                row[0][-15:],
                row[1] or '',
                row[2] or '',
                row[3],
                translate_format_type(row[4]),
                translate_issue_topic(row[5]),
                round(row[6], 2) if row[6] else 0,
                round(row[7], 2) if row[7] else 0,
                round(row[8], 2) if row[8] else 0,
                round(row[9], 2) if row[9] else 0,
                row[10] or 'low',
                row[11] or 0,
                row[12] or 0,
                row[13] or 0,
                row[14] or 0,
                row[15] or 0,
                row[16] or 0,
                row[17] or ''
            ])
            
        update_with_timestamp(worksheet, 'A1', rows)
        
        print(f"  âœ“ å·²å°å‡ºè‡ªç„¶ vs ä»˜è²»æ¯”è¼ƒè³‡æ–™ç‰ˆ (Looker Ready, {len(rows)-1} ç­†)")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè‡ªç„¶ vs ä»˜è²»æ¯”è¼ƒè³‡æ–™ç‰ˆå¤±æ•—: {e}")
        return False
    """å°å‡ºè³‡æ–™å­—å…¸èˆ‡èªªæ˜ - é‡æ–°è¨­è¨ˆç‰ˆ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        # å®Œå…¨åˆªé™¤ä¸¦é‡å»ºå·¥ä½œè¡¨ä»¥æ¸…é™¤æ‰€æœ‰æ ¼å¼
        try:
            old_worksheet = spreadsheet.worksheet('documentation')
            spreadsheet.del_worksheet(old_worksheet)
        except gspread.exceptions.WorksheetNotFound:
            pass
        
        # å»ºç«‹å…¨æ–°çš„å·¥ä½œè¡¨
        worksheet = spreadsheet.add_worksheet(title='documentation', rows=150, cols=5)

        # å®šç¾©æ–‡ä»¶å…§å®¹
        docs = []
        
        # ===== ç¬¬ä¸€å€å¡Šï¼šæ¨™é¡Œ =====
        docs.append(['ğŸ“Š Facebook ç¤¾ç¾¤æ•¸æ“šåˆ†æ - è³‡æ–™å­—å…¸', '', '', '', ''])
        docs.append([f'æœ€å¾Œæ›´æ–°: {datetime.now().strftime("%Y-%m-%d %H:%M")}', '', '', '', ''])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬äºŒå€å¡Šï¼šå ±è¡¨ç¸½è¦½ =====
        docs.append(['ğŸ“ å ±è¡¨ç¸½è¦½', '', '', '', ''])
        docs.append(['å ±è¡¨åç¨±', 'ç”¨é€”èªªæ˜', 'è³‡æ–™ç¯„åœ', 'æ›´æ–°æ–¹å¼', 'å»ºè­°ç”¨æ³•'])
        docs.append(['raw_posts', 'è²¼æ–‡åŸå§‹è³‡æ–™', 'æ‰€æœ‰è²¼æ–‡', 'æ¯æ—¥è¦†è“‹', 'æŸ¥è©¢ç‰¹å®šè²¼æ–‡è©³æƒ…'])
        docs.append(['raw_post_insights', 'è²¼æ–‡æ´å¯Ÿæ•¸æ“š', 'ç™¼å¸ƒ 30 å¤©å…§', 'æ¯æ—¥ç´¯åŠ ', 'è¿½è¹¤è²¼æ–‡æˆé•·è¶¨å‹¢'])
        docs.append(['page_daily_metrics', 'é é¢æ¯æ—¥æŒ‡æ¨™', 'è¿‘ 7 å¤©', 'æ¯æ—¥ç´¯åŠ ', 'ç›£æ§æ•´é«”ç²‰å°ˆå¥åº·'])
        docs.append(['top_posts', 'è¡¨ç¾æœ€ä½³è²¼æ–‡', 'è¿‘ 1 å¹´ / å‰ 100 å', 'æ¯æ—¥è¦†è“‹', 'æ‰¾å‡ºæˆåŠŸæ¡ˆä¾‹'])
        docs.append(['weekly_trends', 'é€±åº¦è¶¨å‹¢', 'è¿‘ 2 å¹´ (104 é€±)', 'æ¯æ—¥è¦†è“‹', 'è§€å¯Ÿé•·æœŸè®ŠåŒ–'])
        docs.append(['best_posting_times', 'æœ€ä½³ç™¼æ–‡æ™‚é–“', 'ä¾æ™‚æ®µ/è­°é¡Œ/è¡Œå‹•åˆ†çµ„', 'æ¯æ—¥è¦†è“‹', 'è¦åŠƒç™¼æ–‡æ’ç¨‹'])
        docs.append(['format_type_performance', 'è¡Œå‹•è¡¨ç¾åˆ†æ', 'ä¾è²¼æ–‡è¡Œå‹•åˆ†é¡', 'æ¯æ—¥è¦†è“‹', 'è©•ä¼°ä¸åŒè¡Œå‹•æ•ˆæœ'])
        docs.append(['issue_topic_performance', 'è­°é¡Œè¡¨ç¾åˆ†æ', 'ä¾æ”¿ç­–è­°é¡Œåˆ†é¡', 'æ¯æ—¥è¦†è“‹', 'è©•ä¼°ä¸åŒè­°é¡Œç†±åº¦'])
        docs.append(['format_issue_cross', 'è¡Œå‹•Ã—è­°é¡Œäº¤å‰', 'â‰¥2 ç¯‡è²¼æ–‡çš„çµ„åˆ', 'æ¯æ—¥è¦†è“‹', 'æ‰¾å‡ºæœ€ä½³å…§å®¹çµ„åˆ'])
        docs.append(['hourly_performance', 'æ¯å°æ™‚è¡¨ç¾', 'å…¨éƒ¨æ™‚æ®µ', 'æ¯æ—¥è¦†è“‹', 'ç²¾æº–æ’ç¨‹æ±ºç­–'])
        docs.append(['deep_dive_metrics', 'æ·±åº¦æŒ‡æ¨™åˆ†æ', 'å‰ 200 ç¯‡', 'æ¯æ—¥è¦†è“‹', 'å®Œæ•´ KPI åˆ†æ'])
        docs.append(['quadrant_analysis', 'è±¡é™åˆ†æ', 'å…¨éƒ¨è²¼æ–‡', 'æ¯æ—¥è¦†è“‹', 'Looker Studio è¦–è¦ºåŒ–'])
        docs.append(['trending_posts', 'è¿‘æœŸç†±é–€è²¼æ–‡', '48 å°æ™‚å…§ç™¼å¸ƒ', 'æ¯æ—¥è¦†è“‹', 'è­˜åˆ¥æ­£åœ¨èµ·é£›çš„å…§å®¹'])
        docs.append(['ad_recommendations', 'æŠ•å»£æ¨è–¦æ¸…å–®', 'æ½›åŠ›åˆ†æ•¸ â‰¥ 40', 'æ¯æ—¥è¦†è“‹', 'é¸æ“‡æŠ•å»£ç´ æ'])
        docs.append(['organic_vs_paid', 'è‡ªç„¶ vs ä»˜è²»', 'å…¨éƒ¨è²¼æ–‡', 'æ¯æ—¥è¦†è“‹', 'è©•ä¼°ä»˜è²»æ•ˆç›Š'])
        docs.append(['ad_campaigns', 'å»£å‘Šæ´»å‹•æ¸…å–®', 'å…¨éƒ¨ campaigns', 'æ¯æ—¥è¦†è“‹', 'å»£å‘Šç¸¾æ•ˆç¸½è¦½'])
        docs.append(['ad_roi_analysis', 'å»£å‘Š ROI åˆ†æ', 'å…¨éƒ¨å»£å‘Š', 'æ¯æ—¥è¦†è“‹', 'å»£å‘Šç´°é …åˆ†æ'])
        docs.append(['yearly_posting_analysis', 'å¹´åº¦ç™¼æ–‡åˆ†æ', 'æŒ‰æœˆä»½åˆ†çµ„', 'æ¯æ—¥è¦†è“‹', 'å­£ç¯€æ€§è¦åŠƒ'])
        docs.append(['pipeline_logs', 'åŸ·è¡Œç´€éŒ„', 'è¿‘ 50 æ¬¡', 'æ¯æ¬¡åŸ·è¡Œå¾Œ', 'ç›£æ§ç³»çµ±ç‹€æ…‹'])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬ä¸‰å€å¡Šï¼šæ ¸å¿ƒæŒ‡æ¨™èªªæ˜ =====
        docs.append(['ğŸ“ˆ æ ¸å¿ƒæŒ‡æ¨™èªªæ˜', '', '', '', ''])
        docs.append(['æŒ‡æ¨™åç¨±', 'è¨ˆç®—å…¬å¼', 'æ„ç¾©', 'åƒè€ƒæ¨™æº–', 'ä¾†æºä¾æ“š'])
        docs.append(['äº’å‹•ç‡ (ER)', '(è®š+ç•™è¨€+åˆ†äº«) Ã· è§¸åŠ Ã— 100', 'å…§å®¹å¼•èµ·äº’å‹•çš„èƒ½åŠ›', '> 3% ç‚ºä½³', 'Hootsuite 2024: FB å¹³å‡ 0.5-1%, NGO ç´„ 2-4%'])
        docs.append(['åˆ†äº«ç‡ (SR)', 'åˆ†äº« Ã· è§¸åŠ Ã— 100', 'å…§å®¹å‚³æ’­æ½›åŠ›', '> 0.5% ç‚ºä½³', 'åˆ†äº«æ˜¯æœ€é«˜åƒ¹å€¼äº’å‹•'])
        docs.append(['ç•™è¨€ç‡ (CR)', 'ç•™è¨€ Ã· è§¸åŠ Ã— 100', 'å¼•ç™¼è¨è«–çš„èƒ½åŠ›', '> 0.3% ç‚ºä½³', 'Sprout Social: é€šå¸¸ç‚ºæŒ‰è®šçš„ 10%'])
        docs.append(['é»æ“Šç‡ (CTR)', 'é»æ“Š Ã· è§¸åŠ Ã— 100', 'å¸å¼•é»æ“Šçš„èƒ½åŠ›', '> 2% ç‚ºä½³', 'WordStream 2024: FB ä¸­ä½æ•¸ 1.6%, NGO ç´„ 2.1%'])
        docs.append(['ç—…æ¯’æ€§ (VS)', 'åˆ†äº« Ã· åæ‡‰', 'åˆ†äº«æ„é¡˜å¼·åº¦', '> 0.5 ç‚ºä½³', 'æ¯ 2 æ¬¡åæ‡‰æœ‰ 1 æ¬¡åˆ†äº«'])
        docs.append(['è¨è«–æ·±åº¦ (DD)', 'ç•™è¨€ Ã· æŒ‰è®š', 'è¨è«– vs å¿«é€Ÿåæ‡‰', '> 0.1 ç‚ºä½³', '1:10 æ¯”ä¾‹è¡¨ç¤ºå¼•ç™¼æ€è€ƒ'])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬å››å€å¡Šï¼šè¡¨ç¾ç­‰ç´š =====
        docs.append(['ğŸ† è¡¨ç¾ç­‰ç´šèªªæ˜', '', '', '', ''])
        docs.append(['ç­‰ç´šåç¨±', 'ä¸­æ–‡', 'æ¢ä»¶', 'ä½”æ¯”', 'èªªæ˜'])
        docs.append(['viral', 'ç†±é–€', 'äº’å‹•ç‡ â‰¥ P95 (å‰ 5%)', '~5%', 'çˆ†æ¬¾è²¼æ–‡ï¼Œå¯ä½œç‚ºæˆåŠŸæ¡ˆä¾‹'])
        docs.append(['high', 'å„ªè³ª', 'äº’å‹•ç‡ â‰¥ P75 (å‰ 25%)', '~20%', 'è¡¨ç¾è‰¯å¥½ï¼Œé©åˆæŠ•å»£'])
        docs.append(['average', 'ä¸€èˆ¬', 'äº’å‹•ç‡ â‰¥ P25 (ä¸­é–“)', '~50%', 'æ­£å¸¸è¡¨ç¾'])
        docs.append(['low', 'å¾…æ”¹é€²', 'äº’å‹•ç‡ < P25 (å¾Œ 25%)', '~25%', 'éœ€æª¢è¦–åŸå› '])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬äº”å€å¡Šï¼šyearly_posting_analysis æ¬„ä½èªªæ˜ =====
        docs.append(['ğŸ“Š yearly_posting_analysis æ¬„ä½èªªæ˜', '', '', '', ''])
        docs.append(['æ¬„ä½åç¨±', 'èªªæ˜', '', '', ''])
        docs.append(['æœˆä»½', 'ç™¼æ–‡æœˆä»½ (1-12æœˆ)', '', '', ''])
        docs.append(['æ™‚æ®µ', 'ç™¼æ–‡æ™‚æ®µ (æ—©æ™¨/ä¸­åˆ/ä¸‹åˆ/æ™šé–“/æ·±å¤œ)', '', '', ''])
        docs.append(['è­°é¡Œ', 'è²¼æ–‡è­°é¡Œåˆ†é¡', '', '', ''])
        docs.append(['è¡Œå‹•', 'è²¼æ–‡è¡Œå‹•åˆ†é¡ (æ´»å‹•/è²æ˜/å ±å‘Šç­‰)', '', '', ''])
        docs.append(['è²¼æ–‡æ•¸', 'è©²çµ„åˆçš„è²¼æ–‡ç¸½æ•¸', '', '', ''])
        docs.append(['å¹³å‡äº’å‹•ç‡', 'è©²çµ„åˆè²¼æ–‡çš„å¹³å‡äº’å‹•ç‡', '', '', ''])
        docs.append(['å¹³å‡é»æ“Šç‡', 'è©²çµ„åˆè²¼æ–‡çš„å¹³å‡é»æ“Šç‡', '', '', ''])
        docs.append(['å¹³å‡åˆ†äº«ç‡', 'è©²çµ„åˆè²¼æ–‡çš„å¹³å‡åˆ†äº«ç‡', '', '', ''])
        docs.append(['é«˜è¡¨ç¾æ•¸', 'è©²çµ„åˆä¸­ã€Œç†±é–€ viralã€æˆ–ã€Œå„ªè³ª highã€ç­‰ç´šçš„è²¼æ–‡æ•¸é‡', '', '', ''])
        docs.append(['ç¸½é»æ“Šæ•¸', 'è©²çµ„åˆæ‰€æœ‰è²¼æ–‡çš„é»æ“Šç¸½å’Œ', '', '', ''])
        docs.append(['ç¸½åˆ†äº«æ•¸', 'è©²çµ„åˆæ‰€æœ‰è²¼æ–‡çš„åˆ†äº«ç¸½å’Œ', '', '', ''])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬å…­å€å¡Šï¼šè±¡é™åˆ†æ =====
        docs.append(['ğŸ¯ è±¡é™åˆ†æèªªæ˜', '', '', '', ''])
        docs.append(['è±¡é™åç¨±', 'è§¸åŠ', 'äº’å‹•ç‡', 'ç‰¹å¾µ', 'å»ºè­°è¡Œå‹•'])
        docs.append(['ç‹ç‰Œè²¼æ–‡', 'é«˜ (â‰¥ä¸­ä½æ•¸)', 'é«˜ (â‰¥ä¸­ä½æ•¸)', 'æ“´æ•£åŠ›+å¸å¼•åŠ›ä¿±ä½³', 'æœ€ä½³æŠ•å»£ç´ æ'])
        docs.append(['æ½›åŠ›çå¯¶', 'ä½ (<ä¸­ä½æ•¸)', 'é«˜ (â‰¥ä¸­ä½æ•¸)', 'å…§å®¹å„ªè³ªä½†è§¸åŠä¸è¶³', 'æŠ•å»£æ¨å»£ï¼Œæå‡æ›å…‰'])
        docs.append(['å»£å‚³é™·é˜±', 'é«˜ (â‰¥ä¸­ä½æ•¸)', 'ä½ (<ä¸­ä½æ•¸)', 'è§¸åŠå¤§ä½†æ²’äººäº’å‹•', 'æª¢è¦–å…§å®¹ï¼Œæ”¹å–„å¸å¼•åŠ›'])
        docs.append(['å¸¸æ…‹å…§å®¹', 'ä½ (<ä¸­ä½æ•¸)', 'ä½ (<ä¸­ä½æ•¸)', 'ä¸€èˆ¬è¡¨ç¾', 'åƒè€ƒç”¨ï¼Œåˆ†ææ”¹å–„ç©ºé–“'])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬ä¸ƒå€å¡Šï¼šè¡Œå‹•åˆ†é¡ =====
        docs.append(['ğŸ·ï¸ è¡Œå‹•åˆ†é¡ (Format Type)', '', '', '', ''])
        docs.append(['ä»£ç¢¼', 'ä¸­æ–‡åç¨±', 'åˆ¤æ–·é—œéµå­—', '', ''])
        docs.append(['event', 'å®šæœŸæ´»å‹•', 'å½±å±•ã€è¬›åº§ã€è«–å£‡ã€å·¥ä½œåŠã€åˆ†äº«æœƒã€åº§è«‡ã€æ´»å‹•å ±å', '', ''])
        docs.append(['press', 'è¨˜è€…æœƒ', 'è¨˜è€…æœƒã€åª’é«”ã€æ¡è¨ªã€æ–°èç¨¿', '', ''])
        docs.append(['statement', 'è²æ˜ç¨¿', 'è²æ˜ã€ç™¼è¨€ã€ç«‹å ´ã€å‘¼ç±²ã€å¼·èª¿', '', ''])
        docs.append(['opinion', 'æ–°èè§€é»', 'è§€é»ã€è©•è«–ã€åˆ†æã€çœ‹æ³•ã€æ™‚äº‹', '', ''])
        docs.append(['op_ed', 'æŠ•æ›¸', 'æŠ•æ›¸ã€å°ˆæ¬„ã€åˆŠç™»ã€åª’é«”æŠ•æ›¸', '', ''])
        docs.append(['report', 'å ±å‘Šç™¼å¸ƒ', 'å ±å‘Šã€ç™¼å¸ƒã€ç ”ç©¶ã€èª¿æŸ¥ã€æ•¸æ“š', '', ''])
        docs.append(['booth', 'æ“ºæ”¤è³‡è¨Š', 'æ“ºæ”¤ã€å¸‚é›†ã€ç¾å ´ã€ä¾†æ‰¾æˆ‘å€‘', '', ''])
        docs.append(['edu', 'ç§‘æ™®/Podcast', 'æ‡¶äººåŒ…ã€Podcastã€ç§‘æ™®ã€Q&Aã€çŸ¥è­˜ã€è§£èªª', '', ''])
        docs.append(['action', 'è¡Œå‹•è™Ÿå¬', 'é€£ç½²ã€ææ¬¾ã€å¿—å·¥ã€è¡Œå‹•ã€åƒèˆ‡ã€æ”¯æŒæˆ‘å€‘', '', ''])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬å…«å€å¡Šï¼šè­°é¡Œåˆ†é¡ =====
        docs.append(['ğŸ·ï¸ è­°é¡Œåˆ†é¡ (Issue Topic)', '', '', '', ''])
        docs.append(['ä»£ç¢¼', 'ä¸­æ–‡åç¨±', 'åˆ¤æ–·é—œéµå­—', '', ''])
        docs.append(['nuclear', 'æ ¸èƒ½ç™¼é›»', 'æ ¸é›»ã€æ ¸èƒ½ã€æ ¸å››ã€æ ¸å»¢ã€æ ¸å®‰ã€è¼»å°„', '', ''])
        docs.append(['climate', 'æ°£å€™å•é¡Œ', 'æ°£å€™ã€æš–åŒ–ã€ç¢³æ’ã€COPã€æ¥µç«¯å¤©æ°£', '', ''])
        docs.append(['net_zero', 'æ·¨é›¶æ”¿ç­–', 'æ·¨é›¶ã€ç¢³ä¸­å’Œã€2050ã€æ¸›ç¢³', '', ''])
        docs.append(['industry', 'ç”¢æ¥­åˆ†æ', 'ç”¢æ¥­ã€ä¼æ¥­ã€ESGã€æ°¸çºŒã€ä¾›æ‡‰éˆ', '', ''])
        docs.append(['renewable', 'èƒ½æºç™¼å±•', 'å…‰é›»ã€é¢¨é›»ã€å†ç”Ÿèƒ½æºã€ç¶ é›»ã€å¤ªé™½èƒ½', '', ''])
        docs.append(['other', 'å…¶ä»–è­°é¡Œ', 'å‹å‹•ã€ç’°è©•ã€ç©ºæ±¡ã€æ°´è³‡æºã€ç”Ÿæ…‹', '', ''])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬ä¹å€å¡Šï¼šæŠ•å»£æ¨è–¦ =====
        docs.append(['ğŸ’° æŠ•å»£æ¨è–¦è©•åˆ†èªªæ˜', '', '', '', ''])
        docs.append(['åˆ†æ•¸é …ç›®', 'æ¬Šé‡', 'èªªæ˜', '', ''])
        docs.append(['äº’å‹•ç‡åˆ†æ•¸', '30%', 'äº’å‹•ç‡æ­£è¦åŒ–å¾Œ Ã— 100', '', ''])
        docs.append(['åˆ†äº«ç‡åˆ†æ•¸', '25%', 'åˆ†äº«ç‡æ­£è¦åŒ–å¾Œ Ã— 100 (ç—…æ¯’æ½›åŠ›)', '', ''])
        docs.append(['ç•™è¨€ç‡åˆ†æ•¸', '15%', 'ç•™è¨€ç‡æ­£è¦åŒ–å¾Œ Ã— 100 (è¨è«–æ·±åº¦)', '', ''])
        docs.append(['è­°é¡Œå› å­', '15%', 'è©²è­°é¡Œæ­·å² ER Ã· æ•´é«” ER (>1 = ç†±é–€è­°é¡Œ)', '', ''])
        docs.append(['æ™‚æ®µå› å­', '15%', 'è©²æ™‚æ®µæ­·å² ER Ã· æ•´é«” ER (>1 = ç†±é–€æ™‚æ®µ)', '', ''])
        docs.append(['', '', '', '', ''])
        docs.append(['æŠ•å»£å»ºè­°', 'æ¢ä»¶', '', '', ''])
        docs.append(['Yes (æ¨è–¦)', 'æ½›åŠ›åˆ†æ•¸ â‰¥ 70', '', '', ''])
        docs.append(['Maybe (è€ƒæ…®)', 'æ½›åŠ›åˆ†æ•¸ 50-69', '', '', ''])
        docs.append(['No (ä¸å»ºè­°)', 'æ½›åŠ›åˆ†æ•¸ < 50', '', '', ''])
        docs.append(['', '', '', '', ''])
        
        # ===== ç¬¬åå€å¡Šï¼šæ³¨æ„äº‹é … =====
        docs.append(['âš ï¸ æ³¨æ„äº‹é …', '', '', '', ''])
        docs.append(['1. Facebook API å°è¶…é 90 å¤©çš„ Insights æ•¸æ“šæœ‰å­˜å–é™åˆ¶', '', '', '', ''])
        docs.append(['2. æ‰€æœ‰æ™‚é–“å·²è½‰æ›ç‚º GMT+8 å°ç£æ™‚å€', '', '', '', ''])
        docs.append(['3. raw_post_insights åƒ…è¿½è¹¤ç™¼å¸ƒå¾Œ 30 å¤©å…§çš„è²¼æ–‡ (æ¯æ—¥å¿«ç…§)', '', '', '', ''])
        docs.append(['4. å»£å‘Šæ•¸æ“šä¸­ã€Œç„¡æ•¸æ“šã€çš„å»£å‘Šè¡¨ç¤ºå¾æœªæŠ•éé (è‰ç¨¿æˆ–æœªå•Ÿç”¨)', '', '', '', ''])
        docs.append(['5. æ­·å²å»£å‘Šæ•¸æ“šæœ€å¤šå¯å›æº¯ 37 å€‹æœˆ', '', '', '', ''])
        
        # å¯«å…¥è³‡æ–™
        update_with_timestamp(worksheet, 'A1', docs)
        
        # ===== æ ¼å¼åŒ– - ä½¿ç”¨ batch_format ä¸€æ¬¡å¥—ç”¨ =====
        # Level 1: ä¸»æ¨™é¡Œ (æ·±è—åº•ç™½å­— 14pt)
        # Level 2: å€å¡Šæ¨™é¡Œ (æ·ºè—åº•æ·±è—å­— 11pt ç²—é«”) - å…± 9 å€‹
        # Level 3: æ¬„ä½æ¨™é¡Œåˆ— (æ·ºç°åº• 10pt ç²—é«”) - å…± 9 å€‹
        
        formats = []
        
        # Level 1: ä¸»æ¨™é¡Œ
        formats.append({
            'range': 'A1:E1',
            'format': {
                "backgroundColor": {"red": 0.2, "green": 0.4, "blue": 0.65},
                "textFormat": {"bold": True, "fontSize": 14, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
            }
        })
        
        # æ›´æ–°æ™‚é–“
        formats.append({
            'range': 'A2:E2',
            'format': {
                "textFormat": {"italic": True, "fontSize": 9, "foregroundColor": {"red": 0.5, "green": 0.5, "blue": 0.5}}
            }
        })
        
        # Level 2: å€å¡Šæ¨™é¡Œ (emoji é–‹é ­çš„è¡Œ)
        section_rows = [4, 26, 35, 42, 56, 63, 75, 84, 97]
        for row in section_rows:
            formats.append({
                'range': f'A{row}:E{row}',
                'format': {
                    "backgroundColor": {"red": 0.85, "green": 0.92, "blue": 1.0},
                    "textFormat": {"bold": True, "fontSize": 11, "foregroundColor": {"red": 0.1, "green": 0.25, "blue": 0.5}}
                }
            })
        
        # Level 3: æ¬„ä½æ¨™é¡Œåˆ— (ç·Šæ¥åœ¨å€å¡Šæ¨™é¡Œå¾Œé¢)
        header_rows = [5, 27, 36, 43, 57, 64, 76, 85, 92]
        for row in header_rows:
            formats.append({
                'range': f'A{row}:E{row}',
                'format': {
                    "backgroundColor": {"red": 0.94, "green": 0.94, "blue": 0.94},
                    "textFormat": {"bold": True, "fontSize": 10}
                }
            })
        
        # ä¸€æ¬¡æ€§å¥—ç”¨æ‰€æœ‰æ ¼å¼
        worksheet.batch_format(formats)

        print(f"  âœ“ å·²å°å‡ºè³‡æ–™å­—å…¸èˆ‡èªªæ˜")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºèªªæ˜æ–‡ä»¶å¤±æ•—: {e}")
        return False

def export_yearly_posting_analysis(client, conn):
    """å°å‡ºå¹´åº¦æœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆæŒ‰æœˆä»½åˆ†çµ„ï¼Œå«è­°é¡Œ/è¡Œå‹•ç¯©é¸å™¨ï¼‰"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('yearly_posting_analysis')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='yearly_posting_analysis', rows=500, cols=12)

        worksheet.clear()

        cursor = conn.cursor()
        
        # æŒ‰æœˆä»½ + æ™‚æ®µ + è­°é¡Œ + è¡Œå‹•åˆ†çµ„çš„æœ€ä½³ç™¼æ–‡æ™‚é–“
        # ä½¿ç”¨ MAX å–å„æŒ‡æ¨™æœ€å¤§å€¼ï¼Œé¿å…ä¸å®Œæ•´ snapshot å°è‡´æ•¸æ“šç‚º 0
        cursor.execute("""
            SELECT 
                strftime('%m', substr(p.created_time, 1, 10)) as month,
                pc.time_slot,
                COALESCE(pc.issue_topic, 'æœªåˆ†é¡') as issue_topic,
                COALESCE(pc.format_type, 'æœªåˆ†é¡') as format_type,
                COUNT(*) as post_count,
                ROUND(AVG(pp.engagement_rate), 4) as avg_er,
                ROUND(AVG(pp.click_through_rate), 4) as avg_ctr,
                ROUND(AVG(pp.share_rate), 4) as avg_sr,
                SUM(CASE WHEN pp.performance_tier IN ('viral', 'high') THEN 1 ELSE 0 END) as high_performer_count,
                COALESCE(SUM(bs.max_clicks), 0) as sum_max_clicks,
                COALESCE(SUM(bs.max_shares), 0) as sum_max_shares
            FROM posts p
            JOIN posts_classification pc ON p.post_id = pc.post_id
            JOIN posts_performance pp ON p.post_id = pp.post_id
            LEFT JOIN (
                SELECT post_id, 
                       MAX(post_clicks) as max_clicks,
                       MAX(shares_count) as max_shares
                FROM post_insights_snapshots
                GROUP BY post_id
            ) bs ON p.post_id = bs.post_id
            GROUP BY month, pc.time_slot, pc.issue_topic, pc.format_type
            ORDER BY month, avg_er DESC
        """)
        rows_data = cursor.fetchall()

        # æœˆä»½å°ç…§
        month_names = {
            '01': '1æœˆ', '02': '2æœˆ', '03': '3æœˆ', '04': '4æœˆ',
            '05': '5æœˆ', '06': '6æœˆ', '07': '7æœˆ', '08': '8æœˆ',
            '09': '9æœˆ', '10': '10æœˆ', '11': '11æœˆ', '12': '12æœˆ'
        }
        
        time_slot_map = {
            'morning': 'æ—©æ™¨ (6-12)',
            'noon': 'ä¸­åˆ (12-15)',
            'afternoon': 'ä¸‹åˆ (15-18)',
            'evening': 'æ™šé–“ (18-23)',
            'night': 'æ·±å¤œ (23-6)',
            None: 'æœªåˆ†é¡'
        }

        headers = ['æœˆä»½', 'æ™‚æ®µ', 'è­°é¡Œ', 'è¡Œå‹•', 'è²¼æ–‡æ•¸',
                   'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)', 'é«˜è¡¨ç¾æ•¸',
                   'ç´¯ç©æœ€é«˜é»æ“Šæ•¸', 'ç´¯ç©æœ€é«˜åˆ†äº«æ•¸']
        rows = [headers]

        for row in rows_data:
            rows.append([
                month_names.get(row[0], row[0]),
                time_slot_map.get(row[1], row[1] or 'æœªåˆ†é¡'),
                translate_issue_topic(row[2]),
                translate_format_type(row[3]),
                row[4],
                round(row[5], 2) if row[5] else 0,
                round(row[6], 2) if row[6] else 0,
                round(row[7], 2) if row[7] else 0,
                row[8] or 0,
                row[9] or 0,
                row[10] or 0
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œ
        worksheet.format('A1:K1', {
            "backgroundColor": {"red": 0.3, "green": 0.5, "blue": 0.7},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡ºå¹´åº¦ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºå¹´åº¦ç™¼æ–‡æ™‚é–“åˆ†æå¤±æ•—: {e}")
        return False


def export_pipeline_logs(client, conn):
    """å°å‡º Pipeline åŸ·è¡Œç´€éŒ„"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('pipeline_logs')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='pipeline_logs', rows=100, cols=10)

        worksheet.clear()

        cursor = conn.cursor()
        
        # æª¢æŸ¥ pipeline_runs è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='pipeline_runs'")
        if not cursor.fetchone():
            # å»ºç«‹è¡¨æ ¼
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS pipeline_runs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    run_date TEXT NOT NULL,
                    run_time TEXT NOT NULL,
                    status TEXT NOT NULL,
                    posts_collected INTEGER,
                    posts_analyzed INTEGER,
                    sheets_exported INTEGER,
                    error_message TEXT,
                    duration_seconds REAL
                )
            """)
            conn.commit()
            
            # é¦–æ¬¡åŸ·è¡Œæ™‚åƒ…é¡¯ç¤ºæ¨™é¡Œ
            headers = ['åŸ·è¡Œ ID', 'æ—¥æœŸ', 'æ™‚é–“', 'ç‹€æ…‹', 'æ”¶é›†è²¼æ–‡æ•¸', 
                       'åˆ†æè²¼æ–‡æ•¸', 'åŒ¯å‡ºå ±è¡¨æ•¸', 'éŒ¯èª¤è¨Šæ¯', 'åŸ·è¡Œç§’æ•¸']
            worksheet.update([headers], 'A1')
            print("  âŠ˜ Pipeline ç´€éŒ„è¡¨å·²å»ºç«‹ï¼ˆå°šç„¡ç´€éŒ„ï¼‰")
            return True

        cursor.execute("""
            SELECT id, run_date, run_time, status, 
                   posts_collected, posts_analyzed, sheets_exported,
                   error_message, duration_seconds
            FROM pipeline_runs
            ORDER BY run_date DESC, run_time DESC
            LIMIT 50
        """)
        rows_data = cursor.fetchall()

        headers = ['åŸ·è¡Œ ID', 'æ—¥æœŸ', 'æ™‚é–“', 'ç‹€æ…‹', 'æ”¶é›†è²¼æ–‡æ•¸', 
                   'åˆ†æè²¼æ–‡æ•¸', 'åŒ¯å‡ºå ±è¡¨æ•¸', 'éŒ¯èª¤è¨Šæ¯', 'åŸ·è¡Œç§’æ•¸']
        rows = [headers]

        for row in rows_data:
            rows.append(list(row))

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–æ¨™é¡Œ
        worksheet.format('A1:I1', {
            "backgroundColor": {"red": 0.5, "green": 0.5, "blue": 0.5},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}}
        })

        print(f"  âœ“ å·²å°å‡º Pipeline åŸ·è¡Œç´€éŒ„ï¼ˆ{len(rows_data)} ç­†ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡º Pipeline åŸ·è¡Œç´€éŒ„å¤±æ•—: {e}")
        return False



def export_tab_documentation(client):
    """
    å°å‡ºå·¥ä½œè¡¨èªªæ˜æ–‡ä»¶
    è§£é‡‹æ¯å€‹ tab çš„ä½œç”¨èˆ‡ä½¿ç”¨æ–¹å¼
    """
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)
        
        try:
            worksheet = spreadsheet.worksheet('ğŸ“– Tab Documentation')
        except:
            worksheet = spreadsheet.add_worksheet(title='ğŸ“– Tab Documentation', rows=100, cols=5)
        
        # Clear existing content
        worksheet.clear()
        
        # Documentation data
        docs = [
            ['Tab Name', 'Category', 'Purpose', 'Update Frequency', 'Key Columns'],
            
            # Raw Data
            ['raw_posts', 'Raw Data', 'è²¼æ–‡åŸºæœ¬è³‡è¨Šï¼ˆIDã€å…§å®¹ã€ç™¼ä½ˆæ™‚é–“ã€é€£çµç­‰ï¼‰', 'Daily', 'post_id, message, created_time, permalink_url'],
            ['raw_post_insights', 'Raw Data', 'è²¼æ–‡æ´å¯Ÿæ•¸æ“šå¿«ç…§ï¼ˆæŒ‰æŠ“å–æ—¥æœŸå„²å­˜ï¼‰', 'Daily', 'post_id, fetch_date, likes/comments/shares, impressions'],
            ['raw_page_daily', 'Raw Data', 'ç²‰çµ²å°ˆé æ¯æ—¥æŒ‡æ¨™ï¼ˆç²‰çµ²æ•¸ã€è§¸åŠã€äº’å‹•ï¼‰', 'Daily', 'date, fan_count, page_impressions_unique, post_count'],
            
            # Analytics - Best Times
            ['best_posting_times_general', 'Analytics', 'æœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆæ•´é«”ï¼‰', 'Daily', 'time_slot, avg_engagement_rate, post_count'],
            ['best_posting_times_by_topic', 'Analytics', 'æœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆä¾è­°é¡Œåˆ†é¡ï¼‰', 'Daily', 'issue_topic, time_slot, avg_engagement_rate'],
            ['best_posting_times_by_action', 'Analytics', 'æœ€ä½³ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆä¾è¡Œå‹•é¡å‹ï¼‰', 'Daily', 'format_type, time_slot, avg_engagement_rate'],
            
            # Analytics - Performance
            ['format_type_performance', 'Analytics', 'è²¼æ–‡å½¢å¼è¡¨ç¾åˆ†æï¼ˆè¡Œå‹•é¡å‹ï¼šæ´»å‹•/é€£ç½²/æ‡¶äººåŒ…ç­‰ï¼‰', 'Daily', 'format_type, post_count, avg_engagement_rate'],
            ['issue_topic_performance', 'Analytics', 'è­°é¡Œè¡¨ç¾åˆ†æï¼ˆæ°£å€™/èƒ½æº/æ•™è‚²ç­‰ä¸»é¡Œï¼‰', 'Daily', 'issue_topic, post_count, avg_engagement_rate'],
            ['format_issue_cross', 'Analytics', 'è¡Œå‹•Ã—è­°é¡Œäº¤å‰åˆ†æï¼ˆå“ªç¨®å½¢å¼é…å“ªç¨®è­°é¡Œæœ€æœ‰æ•ˆï¼‰', 'Daily', 'format_type, issue_topic, post_count, avg_er'],
            
            # Analytics - Posts
            ['top_posts', 'Analytics', 'Top è²¼æ–‡æ’è¡Œï¼ˆä¾äº’å‹•ç‡æ’åºï¼‰', 'Daily', 'post_id, engagement_rate, reach, performance_tier'],
            ['quadrant_analysis', 'Analytics', 'è±¡é™åˆ†æï¼šç‹ç‰Œ/çå¯¶/å¸¸æ…‹/é™·é˜±å››é¡è²¼æ–‡', 'Daily', 'post_id, quadrant, engagement_rate, share_rate'],
            ['deep_dive_metrics', 'Analytics', 'æ·±åº¦æŒ‡æ¨™åˆ†æï¼ˆdiscussion_depth, virality_scoreç­‰ï¼‰', 'Daily', 'post_id, virality_score, discussion_depth'],
            
            # Analytics - Trends
            ['weekly_trends', 'Analytics', 'é€±åº¦è¶¨å‹¢ï¼ˆè§€å¯Ÿé•·æœŸè¡¨ç¾è®ŠåŒ–ï¼‰', 'Daily', 'week_start, post_count, avg_engagement_rate'],
            ['hourly_performance', 'Analytics', 'æ¯å°æ™‚è¡¨ç¾çµ±è¨ˆï¼ˆ0-23é»ï¼‰', 'Daily', 'hour, avg_engagement_rate, post_count'],
            
            # Ad Analytics
            ['ad_recommendations', 'Ad Analytics', 'æŠ•å»£æ¨è–¦æ¸…å–®ï¼ˆå“ªäº›è²¼æ–‡é©åˆæŠ•å»£ï¼‰', 'Daily', 'post_id, ad_potential_score, organic_er, predicted_paid_er'],
            ['trending_posts', 'Ad Analytics', 'è¿‘æœŸç†±é–€è²¼æ–‡ï¼ˆ96å°æ™‚å…§é«˜äº’å‹•ï¼‰', 'Daily', 'post_id, engagement_rate, created_time'],
            ['organic_vs_paid', 'Ad Analytics', 'è‡ªç„¶ vs ä»˜è²»æ¯”è¼ƒ', 'Daily', 'post_id, organic_reach, paid_reach, organic_er, paid_er'],
            ['ad_campaigns', 'Ad Analytics', 'å»£å‘Šæ´»å‹•æ¸…å–®', 'Daily', 'campaign_id, campaign_name, status, objective'],
            ['ad_roi_analysis', 'Ad Analytics', 'å»£å‘Š ROI åˆ†æï¼ˆæˆæœ¬æ•ˆç›Šï¼‰', 'Daily', 'ad_id, spend, impressions, clicks, cpc, ctr'],
            
            # Data Versions (Looker Studio Ready)
            ['ad_recommendations_data', 'Data Export', 'Looker Studio ç”¨ï¼šæŠ•å»£æ¨è–¦è³‡æ–™ç‰ˆ', 'Daily', '...'],
            ['organic_vs_paid_data', 'Data Export', 'Looker Studio ç”¨ï¼šè‡ªç„¶ä»˜è²»æ¯”è¼ƒè³‡æ–™ç‰ˆ', 'Daily', '...'],
            
            # Reports
            ['yearly_posting_analysis', 'Reports', 'å¹´åº¦ç™¼æ–‡æ™‚é–“åˆ†æï¼ˆæ­·å¹´ç™¼æ–‡æ¨¡å¼ï¼‰', 'Daily', 'year, month, hour, post_count'],
            ['pipeline_logs', 'Reports', 'Pipeline åŸ·è¡Œç´€éŒ„ï¼ˆç³»çµ±é‹è¡Œæ—¥èªŒï¼‰', 'Daily', 'run_date, status, posts_collected, duration'],
            
            ['', '', '', '', ''],
            ['ä½¿ç”¨èªªæ˜', '', '', '', ''],
            ['1. æ¯å€‹ tab å³å´æœ€å¾Œä¸€æ¬„æœƒé¡¯ç¤ºã€Œdata_updated_atã€æ™‚é–“æˆ³è¨˜', '', '', '', ''],
            ['2. è³‡æ–™æ¯æ—¥è‡ªå‹•æ›´æ–°ï¼ˆé€é Cloud Run + Cloud Schedulerï¼‰', '', '', '', ''],
            ['3. Raw data tabs ä¿ç•™å®Œæ•´æ­·å²è¨˜éŒ„ï¼Œanalytics tabs åŸºæ–¼æœ€æ–°å¿«ç…§è¨ˆç®—', '', '', '', ''],
            ['4. è‹¥æŸå€‹ tab è³‡æ–™ç‚ºç©ºï¼Œä»£è¡¨æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™ï¼ˆä¾‹å¦‚è¿‘æœŸç„¡æŠ•å»£ï¼‰', '', '', '', ''],
            ['', '', '', '', ''],
            ['è³‡æ–™ä¾†æº', '', '', '', ''],
            ['â€¢ Facebook Graph API v23.0', '', '', '', ''],
            ['â€¢ è²¼æ–‡è³‡æ–™ï¼š2024-01-01 è‡³ä»Š', '', '', '', ''],
            ['â€¢ Insights è³‡æ–™ï¼šéå» 90 å¤©ï¼ˆFacebook API é™åˆ¶ï¼‰', '', '', '', ''],
        ]
        
        # Write to sheet
        update_with_timestamp(worksheet, 'A1', docs)
        
        # Format header row
        worksheet.format('A1:E1', {
            'backgroundColor': {'red': 0.2, 'green': 0.4, 'blue': 0.7},
            'textFormat': {'bold': True, 'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}},
            'horizontalAlignment': 'CENTER'
        })
        
        # Auto-resize columns
        worksheet.columns_auto_resize(0, 4)
        
        print(f"  âœ“ å·²å°å‡º Tab Documentation ({len(docs)-1} tabs documented)")
        return True
        
    except Exception as e:
        print(f"  âœ— å°å‡º Tab Documentation å¤±æ•—: {e}")
        return False


# ==================== æ•´åˆå°å‡ºå‡½æ•¸ ====================

def export_content_analysis(client, conn):
    """æ•´åˆå°å‡º: è¡Œå‹•è¡¨ç¾ + è­°é¡Œè¡¨ç¾ + äº¤å‰åˆ†æ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ğŸ“Š content_analysis')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ğŸ“Š content_analysis', rows=300, cols=15)

        worksheet.clear()
        rows = []

        # === Section 1: è¡Œå‹•é¡å‹è¡¨ç¾ ===
        rows.append(['ğŸ“Œ è¡Œå‹•é¡å‹è¡¨ç¾åˆ†æ', '', '', '', '', '', ''])
        rows.append(['è¡Œå‹•é¡å‹', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)',
                     'å¹³å‡ç•™è¨€ç‡ (%)', 'ç†±é–€æ•¸ (å‰5%)', 'å„ªè³ªæ•¸ (å‰25%)'])

        format_data = analytics_reports.get_format_type_performance(conn)
        for item in format_data:
            rows.append([
                translate_format_type(item['format_type']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_share_rate'], 2),
                round(item['avg_comment_rate'], 2),
                item['viral_count'],
                item['high_count']
            ])

        rows.append(['', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', ''])

        # === Section 2: è­°é¡Œè¡¨ç¾ ===
        rows.append(['ğŸ“Œ è­°é¡Œè¡¨ç¾åˆ†æ', '', '', '', '', '', ''])
        rows.append(['è­°é¡Œ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)',
                     'å¹³å‡ç•™è¨€ç‡ (%)', 'ç†±é–€æ•¸ (å‰5%)', 'å„ªè³ªæ•¸ (å‰25%)'])

        issue_data = analytics_reports.get_issue_topic_performance(conn)
        for item in issue_data:
            rows.append([
                translate_issue_topic(item['issue_topic']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_share_rate'], 2),
                round(item['avg_comment_rate'], 2),
                item['viral_count'],
                item['high_count']
            ])

        rows.append(['', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', ''])

        # === Section 3: äº¤å‰åˆ†æ ===
        rows.append(['ğŸ“Œ è¡Œå‹• Ã— è­°é¡Œäº¤å‰åˆ†æ', '', '', '', '', ''])
        rows.append(['è¡Œå‹•', 'è­°é¡Œ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)', 'é«˜è¡¨ç¾è²¼æ–‡æ•¸'])

        cross_data = analytics_reports.get_format_issue_cross_performance(conn)
        for item in cross_data:
            rows.append([
                translate_format_type(item['format_type']),
                translate_issue_topic(item['issue_topic']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_share_rate'], 2),
                item['high_performer_count']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        # æ ¼å¼åŒ–å„å€å¡Šæ¨™é¡Œ
        worksheet.format('A1:G1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })

        print(f"  âœ“ å·²å°å‡ºå…§å®¹åˆ†æï¼ˆè¡Œå‹•: {len(format_data)}, è­°é¡Œ: {len(issue_data)}, äº¤å‰: {len(cross_data)}ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºå…§å®¹åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_posting_times(client, conn):
    """æ•´åˆå°å‡º: æœ€ä½³ç™¼æ–‡æ™‚é–“ + æ¯å°æ™‚è¡¨ç¾ + å¹´åº¦åˆ†æ"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('â° posting_times')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='â° posting_times', rows=400, cols=15)

        worksheet.clear()
        rows = []

        time_slot_map = {
            'morning': 'æ—©ä¸Š (6-12é»)',
            'noon': 'ä¸­åˆ (12-15é»)',
            'afternoon': 'ä¸‹åˆ (15-18é»)',
            'evening': 'æ™šä¸Š (18-23é»)',
            'night': 'æ·±å¤œ (23-6é»)'
        }

        # === Section 1: æ•´é«”æœ€ä½³ç™¼æ–‡æ™‚é–“ ===
        rows.append(['ğŸ“Š æ•´é«”æœ€ä½³ç™¼æ–‡æ™‚é–“', '', '', '', ''])
        rows.append(['æ™‚æ®µ', 'æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)'])

        data_general = analytics_reports.get_best_posting_times(conn, limit=20)
        for item in data_general:
            rows.append([
                time_slot_map.get(item['time_slot'], item['time_slot']),
                get_day_name_chinese(item['day_of_week']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        rows.append(['', '', '', '', ''])
        rows.append(['', '', '', '', ''])

        # === Section 2: æŒ‰è­°é¡Œåˆ†çµ„ ===
        rows.append(['ğŸ“Œ æŒ‰è­°é¡Œåˆ†çµ„', '', '', '', '', ''])
        rows.append(['è­°é¡Œ', 'æ™‚æ®µ', 'æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)'])

        data_topic = analytics_reports.get_best_posting_times_by_topic(conn, limit=50)
        for item in data_topic:
            rows.append([
                translate_issue_topic(item['issue_topic']),
                time_slot_map.get(item['time_slot'], item['time_slot']),
                get_day_name_chinese(item['day_of_week']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        rows.append(['', '', '', '', '', ''])
        rows.append(['', '', '', '', '', ''])

        # === Section 3: æŒ‰è¡Œå‹•åˆ†çµ„ ===
        rows.append(['ğŸ¯ æŒ‰è¡Œå‹•åˆ†çµ„', '', '', '', '', ''])
        rows.append(['è¡Œå‹•', 'æ™‚æ®µ', 'æ˜ŸæœŸ', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)'])

        data_format = analytics_reports.get_best_posting_times_by_format(conn, limit=50)
        for item in data_format:
            rows.append([
                translate_format_type(item['format_type']),
                time_slot_map.get(item['time_slot'], item['time_slot']),
                get_day_name_chinese(item['day_of_week']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        rows.append(['', '', '', '', '', ''])
        rows.append(['', '', '', '', '', ''])

        # === Section 4: æ¯å°æ™‚è¡¨ç¾ ===
        rows.append(['ğŸ• æ¯å°æ™‚è¡¨ç¾çµ±è¨ˆ', '', '', ''])
        rows.append(['æ™‚é–“', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)'])

        hourly_data = analytics_reports.get_hourly_performance(conn)
        for item in hourly_data:
            rows.append([
                hour_to_12h_format(item['hour_of_day']),
                item['post_count'],
                round(item['avg_er'], 2),
                round(item['avg_ctr'], 2)
            ])

        rows.append(['', '', '', ''])
        rows.append(['', '', '', ''])

        # === Section 5: å¹´åº¦ç™¼æ–‡åˆ†æ ===
        rows.append(['ğŸ“… å¹´åº¦ç™¼æ–‡æ™‚é–“åˆ†æ', '', '', '', '', '', '', '', '', ''])
        rows.append(['å¹´ä»½', 'æœˆä»½', 'è²¼æ–‡æ•¸', '00-06æ™‚', '06-09æ™‚', '09-12æ™‚', '12-15æ™‚', '15-18æ™‚', '18-21æ™‚', '21-24æ™‚'])

        cursor = conn.cursor()
        cursor.execute("""
            WITH hourly_posts AS (
                SELECT
                    strftime('%Y', created_time) as year,
                    strftime('%m', created_time) as month,
                    CAST(strftime('%H', created_time) AS INTEGER) as hour,
                    post_id
                FROM posts
            )
            SELECT
                year, month,
                COUNT(*) as total_posts,
                SUM(CASE WHEN hour BETWEEN 0 AND 5 THEN 1 ELSE 0 END) as h_00_06,
                SUM(CASE WHEN hour BETWEEN 6 AND 8 THEN 1 ELSE 0 END) as h_06_09,
                SUM(CASE WHEN hour BETWEEN 9 AND 11 THEN 1 ELSE 0 END) as h_09_12,
                SUM(CASE WHEN hour BETWEEN 12 AND 14 THEN 1 ELSE 0 END) as h_12_15,
                SUM(CASE WHEN hour BETWEEN 15 AND 17 THEN 1 ELSE 0 END) as h_15_18,
                SUM(CASE WHEN hour BETWEEN 18 AND 20 THEN 1 ELSE 0 END) as h_18_21,
                SUM(CASE WHEN hour BETWEEN 21 AND 23 THEN 1 ELSE 0 END) as h_21_24
            FROM hourly_posts
            GROUP BY year, month
            ORDER BY year DESC, month DESC
        """)
        yearly_data = cursor.fetchall()

        for row in yearly_data:
            rows.append([
                row[0], row[1], row[2],
                row[3] or 0, row[4] or 0, row[5] or 0, row[6] or 0,
                row[7] or 0, row[8] or 0, row[9] or 0
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:E1', {
            "backgroundColor": {"red": 0.9, "green": 0.5, "blue": 0.2},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })

        print(f"  âœ“ å·²å°å‡ºç™¼æ–‡æ™‚é–“åˆ†æï¼ˆæ•´é«”: {len(data_general)}, è­°é¡Œ: {len(data_topic)}, è¡Œå‹•: {len(data_format)}, æ¯å°æ™‚: {len(hourly_data)}, å¹´åº¦: {len(yearly_data)}ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºç™¼æ–‡æ™‚é–“åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_posts_performance(client, conn):
    """æ•´åˆå°å‡º: Top è²¼æ–‡ + è±¡é™åˆ†æ + æ·±åº¦æŒ‡æ¨™ + é€±è¶¨å‹¢"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ğŸ“ˆ posts_performance')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ğŸ“ˆ posts_performance', rows=500, cols=20)

        worksheet.clear()
        rows = []

        # === Section 1: Top è²¼æ–‡ ===
        rows.append(['ğŸ† Top 100 è²¼æ–‡æ’è¡Œ', '', '', '', '', '', '', '', '', '', '', ''])
        rows.append(['è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ—¥æœŸ', 'è¡Œå‹•', 'è­°é¡Œ', 'æ™‚æ®µ',
                     'äº’å‹•ç‡ (%)', 'è¡¨ç¾ç­‰ç´š', 'ç™¾åˆ†ä½æ•¸', 'è§¸åŠ', 'ç¸½äº’å‹•æ•¸', 'é€£çµ'])

        top_data = analytics_reports.get_top_posts(conn, days=365, limit=100)
        for item in top_data:
            rows.append([
                item['post_id'][-15:],
                (item['message_preview'] or '')[:50],
                convert_to_gmt8(item['created_time'])[:10],
                translate_format_type(item['topic_primary']),
                translate_issue_topic(item.get('issue_topic')),
                translate_time_slot(item['time_slot']),
                round(item['engagement_rate'], 2),
                translate_performance_tier(item['performance_tier']),
                round(item['percentile_rank'], 1),
                item['reach'],
                item['total_engagement'],
                item.get('permalink_url', '')
            ])

        rows.append(['', '', '', '', '', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', '', '', '', '', ''])

        # === Section 2: è±¡é™åˆ†æ ===
        rows.append(['ğŸ“Š è±¡é™åˆ†æ (Viral/High/Average/Low)', '', '', '', '', '', '', '', '', '', ''])
        rows.append(['è²¼æ–‡ ID', 'ç™¼å¸ƒæ—¥æœŸ', 'è§¸åŠäººæ•¸', 'äº’å‹•ç‡ (%)',
                     'ä¸­ä½æ•¸è§¸åŠ', 'ä¸­ä½æ•¸äº’å‹•ç‡ (%)', 'è±¡é™', 'è­°é¡Œ', 'è¡Œå‹•', 'å…§å®¹é è¦½', 'é€£çµ'])

        quadrant_data = analytics_reports.get_quadrant_analysis(conn)
        for item in quadrant_data:
            rows.append([
                item['post_id'][-18:],
                convert_to_gmt8(item['created_time'])[:10],
                item['reach'],
                round(item['engagement_rate'] * 100, 2),
                item['median_reach'],
                round(item['median_er'] * 100, 2),
                item['quadrant'],
                translate_issue_topic(item['topic_tag']),
                translate_format_type(item['format_type']),
                (item['content_short'] or '')[:40],
                item['permalink_url'] or ''
            ])

        rows.append(['', '', '', '', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', '', '', '', ''])

        # === Section 3: é€±è¶¨å‹¢ ===
        rows.append(['ğŸ“ˆ é€±åº¦è¶¨å‹¢ (è¿‘å…©å¹´)', '', '', '', ''])
        rows.append(['é€±æ¬¡ (æ—¥æœŸç¯„åœ)', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'ç¸½è§¸åŠ', 'ç¸½äº’å‹•æ•¸'])

        weekly_data = analytics_reports.get_weekly_trends(conn, weeks=104)
        for item in weekly_data:
            week_range = f"{item.get('week_start', '')} ~ {item.get('week_end', '')}"
            rows.append([
                week_range,
                item['post_count'],
                round(item['avg_er'], 2),
                item['total_reach'],
                item['total_engagement']
            ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:L1', {
            "backgroundColor": {"red": 0.2, "green": 0.6, "blue": 0.9},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })

        print(f"  âœ“ å·²å°å‡ºè²¼æ–‡è¡¨ç¾åˆ†æï¼ˆTop: {len(top_data)}, è±¡é™: {len(quadrant_data)}, é€±è¶¨å‹¢: {len(weekly_data)}ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºè²¼æ–‡è¡¨ç¾åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_ad_analytics(client, conn):
    """æ•´åˆå°å‡º: æŠ•å»£å»ºè­° + ç†±é–€è²¼æ–‡ + è‡ªç„¶vsä»˜è²» + å»£å‘Šæ´»å‹• + ROI"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('ğŸ’° ad_analytics')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='ğŸ’° ad_analytics', rows=800, cols=20)

        worksheet.clear()
        rows = []

        # === Section 1: è¿‘æœŸç†±é–€è²¼æ–‡ ===
        rows.append(['ğŸ”¥ è¿‘æœŸç†±é–€è²¼æ–‡ (72å°æ™‚å…§)', '', '', '', '', '', '', ''])
        rows.append(['è²¼æ–‡ ID', 'å…§å®¹é è¦½', 'ç™¼å¸ƒæ™‚é–“', 'å·²ç™¼å¸ƒå°æ™‚æ•¸',
                     'ç•¶å‰äº’å‹•æ•¸', 'è§¸åŠ', 'æ¯å°æ™‚äº’å‹•', 'äº’å‹•ç‡ (%)'])

        trending = analytics_trends.get_trending_posts(conn, hours=72)
        for item in trending:
            rows.append([
                item['post_id'][-15:],
                (item['message_preview'] or '')[:50],
                item['created_time'][:16] if item['created_time'] else '',
                item['hours_since_post'],
                item['current_engagement'],
                item['reach'] or 0,
                item['engagement_per_hour'],
                item['engagement_rate']
            ])

        rows.append(['', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', ''])

        # === Section 2: æŠ•å»£æ¨è–¦ ===
        ad_predictor.update_all_ad_potentials(conn)

        # æ­·å²æœ€ä½³çµ„åˆ
        cursor = conn.cursor()
        cursor.execute("""
            SELECT
                COALESCE(pc.issue_topic, 'æœªåˆ†é¡') as issue_topic,
                COALESCE(pc.format_type, 'æœªåˆ†é¡') as format_type,
                pc.time_slot,
                CASE pc.day_of_week
                    WHEN 0 THEN 'é€±ä¸€' WHEN 1 THEN 'é€±äºŒ' WHEN 2 THEN 'é€±ä¸‰'
                    WHEN 3 THEN 'é€±å››' WHEN 4 THEN 'é€±äº”' WHEN 5 THEN 'é€±å…­' WHEN 6 THEN 'é€±æ—¥'
                END as day_name,
                COUNT(*) as post_count,
                ROUND(AVG(pp.engagement_rate), 2) as avg_er,
                SUM(CASE WHEN pp.performance_tier IN ('viral', 'high') THEN 1 ELSE 0 END) as high_performers
            FROM posts_classification pc
            JOIN posts_performance pp ON pc.post_id = pp.post_id
            GROUP BY pc.issue_topic, pc.format_type, pc.time_slot, pc.day_of_week
            HAVING post_count >= 3
            ORDER BY avg_er DESC
            LIMIT 15
        """)
        best_combos = cursor.fetchall()

        rows.append(['ğŸ“Š æ­·å²æœ€ä½³çµ„åˆï¼ˆä¾›æ–°å…§å®¹æŠ•å»£åƒè€ƒï¼‰', '', '', '', '', '', ''])
        rows.append(['è­°é¡Œ', 'è¡Œå‹•', 'æ™‚æ®µ', 'æ˜ŸæœŸ', 'æ¨£æœ¬æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'é«˜è¡¨ç¾æ•¸'])

        for combo in best_combos:
            rows.append([
                translate_issue_topic(combo[0]),
                translate_format_type(combo[1]),
                translate_time_slot(combo[2]),
                combo[3] or 'æœªåˆ†é¡',
                combo[4],
                combo[5],
                combo[6]
            ])

        rows.append(['', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', ''])

        # å·²ç™¼å¸ƒè²¼æ–‡æ¨è–¦
        rows.append(['ğŸ“Œ å·²ç™¼å¸ƒè²¼æ–‡æŠ•å»£æ¨è–¦', '', '', '', '', '', '', '', '', '', '', '', ''])
        rows.append([
            'è²¼æ–‡ ID', 'ç™¼å¸ƒæ™‚é–“', 'æŠ•å»£å»ºè­°', 'æ½›åŠ›åˆ†æ•¸', 'è¡¨ç¾ç­‰ç´š',
            'è¡Œå‹•', 'è­°é¡Œ', 'äº’å‹•ç‡åˆ†æ•¸', 'åˆ†äº«ç‡åˆ†æ•¸', 'ç•™è¨€ç‡åˆ†æ•¸',
            'è­°é¡Œå› å­', 'æ™‚æ®µå› å­', 'è²¼æ–‡é€£çµ'
        ])

        recommended = ad_predictor.get_recommended_posts(conn, limit=50, min_score=40)
        for item in recommended:
            breakdown = item.get('breakdown', {})
            rows.append([
                item['post_id'][-15:],
                convert_to_gmt8(item.get('created_time', ''))[:10],
                item['ad_recommendation'],
                item['ad_potential_score'],
                translate_performance_tier(item['performance_tier']),
                translate_format_type(item['format_type']),
                translate_issue_topic(item['issue_topic']),
                round(breakdown.get('engagement_rate_score', 0), 1),
                round(breakdown.get('share_rate_score', 0), 1),
                round(breakdown.get('comment_rate_score', 0), 1),
                breakdown.get('topic_factor', 1),
                breakdown.get('time_factor', 1),
                item.get('permalink_url', '')
            ])

        rows.append(['', '', '', '', '', '', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', '', '', '', '', '', ''])

        # === Section 3: è‡ªç„¶ vs ä»˜è²» ===
        rows.append(['âš–ï¸ è‡ªç„¶ vs ä»˜è²»è²¼æ–‡æˆæ•ˆæ¯”è¼ƒ', '', '', '', '', '', '', ''])

        cursor.execute("""
            WITH latest_snapshots AS (
                SELECT post_id, MAX(fetch_date) as latest_date
                FROM post_insights_snapshots
                GROUP BY post_id
            ),
            promoted_posts AS (
                SELECT DISTINCT post_id FROM ads WHERE post_id IS NOT NULL
            )
            SELECT
                CASE WHEN pp.post_id IS NOT NULL THEN 'paid' ELSE 'organic' END as ad_status,
                COUNT(*) as post_count,
                ROUND(AVG(perf.engagement_rate), 2) as avg_er,
                ROUND(AVG(perf.share_rate), 2) as avg_sr,
                ROUND(AVG(perf.comment_rate), 2) as avg_cr,
                ROUND(AVG(perf.click_through_rate), 2) as avg_ctr,
                SUM(i.post_impressions_unique) as total_reach,
                SUM(i.likes_count + i.comments_count + i.shares_count) as total_engagement
            FROM posts p
            JOIN latest_snapshots ls ON p.post_id = ls.post_id
            JOIN post_insights_snapshots i ON p.post_id = i.post_id AND i.fetch_date = ls.latest_date
            LEFT JOIN promoted_posts pp ON p.post_id = pp.post_id
            LEFT JOIN posts_performance perf ON p.post_id = perf.post_id
            GROUP BY ad_status
        """)
        summary_data = cursor.fetchall()

        rows.append(['é¡å‹', 'è²¼æ–‡æ•¸', 'å¹³å‡äº’å‹•ç‡ (%)', 'å¹³å‡åˆ†äº«ç‡ (%)', 'å¹³å‡ç•™è¨€ç‡ (%)', 'å¹³å‡é»æ“Šç‡ (%)', 'ç¸½è§¸åŠ', 'ç¸½äº’å‹•æ•¸'])
        for row in summary_data:
            status = 'æœ‰å»£å‘Š' if row[0] == 'paid' else 'è‡ªç„¶è§¸åŠ'
            rows.append([
                status, row[1], row[2] or 0, row[3] or 0, row[4] or 0, row[5] or 0, row[6] or 0, row[7] or 0
            ])

        rows.append(['', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', ''])

        # === Section 4: å»£å‘Šæ´»å‹•æ¸…å–® ===
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ad_campaigns'")
        if cursor.fetchone():
            rows.append(['ğŸ“‹ å»£å‘Šæ´»å‹•æ¸…å–®', '', '', '', '', '', '', '', '', '', '', ''])
            rows.append([
                'æ´»å‹• ID', 'æ´»å‹•åç¨±', 'ç›®æ¨™', 'ç‹€æ…‹', 'æ¯æ—¥é ç®— (NT$)', 'ç¸½é ç®— (NT$)',
                'å»ºç«‹æ—¥æœŸ', 'å»£å‘Šæ•¸', 'ç¸½èŠ±è²» (NT$)', 'ç¸½æ›å…‰', 'ç¸½é»æ“Š', 'å¹³å‡ CPC (NT$)'
            ])

            objective_chinese = {
                'OUTCOME_AWARENESS': 'å“ç‰ŒçŸ¥ååº¦',
                'OUTCOME_ENGAGEMENT': 'äº’å‹•æ¨å»£',
                'OUTCOME_TRAFFIC': 'æµé‡å°å¼•',
                'OUTCOME_LEADS': 'åå–®æ”¶é›†',
                'OUTCOME_SALES': 'éŠ·å”®è½‰æ›',
                'LINK_CLICKS': 'é€£çµé»æ“Š',
                'POST_ENGAGEMENT': 'è²¼æ–‡äº’å‹•',
                'PAGE_LIKES': 'ç²‰å°ˆæŒ‰è®š',
            }

            cursor.execute("""
                SELECT
                    ac.campaign_id,
                    ac.name,
                    ac.objective,
                    ac.status,
                    COALESCE(ac.daily_budget, 0) as daily_budget,
                    COALESCE(ac.lifetime_budget, 0) as lifetime_budget,
                    DATE(ac.created_time) as created_date,
                    COUNT(DISTINCT a.ad_id) as ad_count,
                    COALESCE(SUM(ai.spend), 0) as total_spend,
                    COALESCE(SUM(ai.impressions), 0) as total_impressions,
                    COALESCE(SUM(ai.clicks), 0) as total_clicks,
                    CASE WHEN SUM(ai.clicks) > 0
                         THEN ROUND(SUM(ai.spend) / SUM(ai.clicks), 2)
                         ELSE 0 END as avg_cpc
                FROM ad_campaigns ac
                LEFT JOIN ads a ON ac.campaign_id = a.campaign_id
                LEFT JOIN ad_insights ai ON a.ad_id = ai.ad_id
                GROUP BY ac.campaign_id
                ORDER BY total_spend DESC
            """)
            campaigns = cursor.fetchall()

            for row in campaigns:
                rows.append([
                    row[0][-15:] if row[0] else '',
                    row[1] or '',
                    objective_chinese.get(row[2], row[2] or ''),
                    row[3] or '',
                    row[4], row[5],
                    row[6] or '',
                    row[7], row[8], row[9], row[10], row[11]
                ])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:H1', {
            "backgroundColor": {"red": 0.8, "green": 0.4, "blue": 0.2},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })

        print(f"  âœ“ å·²å°å‡ºæŠ•å»£åˆ†æï¼ˆç†±é–€: {len(trending)}, æ¨è–¦: {len(recommended)}, çµ„åˆ: {len(best_combos)}ï¼‰")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºæŠ•å»£åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def export_system_info(client, conn):
    """æ•´åˆå°å‡º: Pipeline ç´€éŒ„ + Tab èªªæ˜"""
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        try:
            worksheet = spreadsheet.worksheet('âš™ï¸ system_info')
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title='âš™ï¸ system_info', rows=150, cols=10)

        worksheet.clear()
        rows = []

        # === Section 1: Tab èªªæ˜ ===
        rows.append(['ğŸ“– åˆ†é èªªæ˜', '', '', '', ''])
        rows.append(['åˆ†é åç¨±', 'é¡åˆ¥', 'èªªæ˜', 'æ›´æ–°é »ç‡', 'ä¸»è¦æ¬„ä½'])

        docs = [
            ['ğŸ“¦ raw_posts', 'åŸå§‹è³‡æ–™', 'è²¼æ–‡åŸºæœ¬è³‡è¨Šï¼ˆIDã€å…§å®¹ã€ç™¼ä½ˆæ™‚é–“ã€é€£çµç­‰ï¼‰', 'æ¯æ—¥', 'post_id, message, created_time'],
            ['ğŸ“¦ raw_post_insights', 'åŸå§‹è³‡æ–™', 'è²¼æ–‡æ´å¯Ÿæ•¸æ“šï¼ˆæ¯è²¼æ–‡æœ€æ–°å¿«ç…§ï¼‰', 'æ¯æ—¥', 'post_id, likes, comments, shares, reach'],
            ['ğŸ“¦ page_daily_metrics', 'åŸå§‹è³‡æ–™', 'ç²‰çµ²å°ˆé æ¯æ—¥æŒ‡æ¨™ï¼ˆç²‰çµ²æ•¸ã€è§¸åŠã€äº’å‹•ï¼‰', 'æ¯æ—¥', 'date, fan_count, page_impressions_unique'],
            ['ğŸ“¦ raw_ads', 'åŸå§‹è³‡æ–™', 'å»£å‘ŠåŸå§‹è³‡æ–™ï¼ˆå»£å‘Š + æˆæ•ˆæ•¸æ“šåˆä½µï¼‰', 'æ¯æ—¥', 'ad_id, impressions, reach, spend, cpc'],
            ['ğŸ“Š content_analysis', 'å…§å®¹åˆ†æ', 'è¡Œå‹•é¡å‹è¡¨ç¾ + è­°é¡Œè¡¨ç¾ + äº¤å‰åˆ†æ', 'æ¯æ—¥', 'format_type, issue_topic, avg_er'],
            ['â° posting_times', 'æ™‚é–“åˆ†æ', 'æœ€ä½³ç™¼æ–‡æ™‚é–“ + æ¯å°æ™‚è¡¨ç¾ + å¹´åº¦åˆ†æ', 'æ¯æ—¥', 'time_slot, hour, avg_engagement_rate'],
            ['ğŸ“ˆ posts_performance', 'è²¼æ–‡è¡¨ç¾', 'Top è²¼æ–‡ + è±¡é™åˆ†æ + é€±è¶¨å‹¢', 'æ¯æ—¥', 'post_id, engagement_rate, performance_tier'],
            ['ğŸ’° ad_analytics', 'æŠ•å»£åˆ†æ', 'ç†±é–€è²¼æ–‡ + æŠ•å»£å»ºè­° + è‡ªç„¶vsä»˜è²» + å»£å‘Šæ´»å‹•', 'æ¯æ—¥', 'ad_potential_score, ad_recommendation'],
            ['âš™ï¸ system_info', 'ç³»çµ±è³‡è¨Š', 'åˆ†é èªªæ˜ + Pipeline åŸ·è¡Œç´€éŒ„', 'æ¯æ—¥', 'run_date, status, duration'],
        ]

        for doc in docs:
            rows.append(doc)

        rows.append(['', '', '', '', ''])
        rows.append(['', '', '', '', ''])

        # === Section 2: è¡Œå‹•é¡å‹åˆ†é¡æ¨™æº– ===
        rows.append(['ğŸ¯ è¡Œå‹•é¡å‹ (Format Type) åˆ†é¡æ¨™æº–', '', '', ''])
        rows.append(['ä»£ç¢¼', 'ä¸­æ–‡åç¨±', 'èªªæ˜', 'åŒ¹é…é—œéµå­—ï¼ˆå®Œæ•´æ¸…å–®ï¼‰'])

        format_types = [
            ['event', 'å®šæœŸæ´»å‹•', 'å½±å±•ã€æ¼”è¬›ã€åº§è«‡æœƒç­‰å®šæœŸèˆ‰è¾¦çš„æ´»å‹•', 'å½±å±•, è¬›åº§, è«–å£‡, å·¥ä½œåŠ, åˆ†äº«æœƒ, åº§è«‡, æ´»å‹•å ±å, æ­¡è¿åƒåŠ '],
            ['press', 'è¨˜è€…æœƒ', 'å¬é–‹è¨˜è€…æœƒç™¼å¸ƒè¨Šæ¯', 'è¨˜è€…æœƒ, åª’é«”, æ¡è¨ª, æ–°èç¨¿'],
            ['statement', 'è²æ˜ç¨¿', 'å…¬é–‹ç™¼è¨€æˆ–æ­£å¼è²æ˜', 'è²æ˜, ç™¼è¨€, ç«‹å ´, å‘¼ç±², å¼·èª¿, æˆ‘å€‘èªç‚º'],
            ['opinion', 'æ–°èè§€é»', 'é‡å°æ™‚äº‹æ–°èçš„è©•è«–è§€é»', 'è§€é», è©•è«–, åˆ†æ, çœ‹æ³•, æ™‚äº‹'],
            ['op_ed', 'æŠ•æ›¸', 'ç¶ ç›ŸæŠ•æ›¸è‡³åª’é«”çš„æ–‡ç« ', 'æŠ•æ›¸, å°ˆæ¬„, åˆŠç™», åª’é«”æŠ•æ›¸'],
            ['report', 'å ±å‘Šç™¼å¸ƒ', 'ç ”ç©¶å ±å‘Šæˆ–èª¿æŸ¥å ±å‘Šç™¼å¸ƒ', 'å ±å‘Š, ç™¼å¸ƒ, ç ”ç©¶, èª¿æŸ¥, æ•¸æ“š, å‡ºçˆ'],
            ['booth', 'æ“ºæ”¤è³‡è¨Š', 'æ“ºæ”¤æ´»å‹•æˆ–å¸‚é›†è³‡è¨Š', 'æ“ºæ”¤, å¸‚é›†, ç¾å ´, ä¾†æ‰¾æˆ‘å€‘'],
            ['edu', 'ç§‘æ™®/Podcast', 'ç§‘æ™®æ–‡ç« æˆ– Podcast ç¯€ç›®', 'æ‡¶äººåŒ…, Podcast, ç§‘æ™®, Q&A, çŸ¥è­˜, è§£èªª, ä½ çŸ¥é“å—, ä¸€æ¬¡çœ‹æ‡‚'],
            ['action', 'è¡Œå‹•è™Ÿå¬', 'é€£ç½²ã€è¡Œå‹•å‘¼ç±²ç­‰', 'é€£ç½², ææ¬¾, å¿—å·¥, è¡Œå‹•, åƒèˆ‡, æ”¯æŒæˆ‘å€‘, ä¸€èµ·'],
            ['(ç©ºç™½)', 'å…¶ä»–è¡Œå‹•', 'ç„¡æ³•æ­¸é¡çš„å…¶ä»–å…§å®¹', 'ï¼ˆç„¡é—œéµå­—åŒ¹é…æ™‚é è¨­ï¼‰'],
        ]
        for ft in format_types:
            rows.append(ft)

        rows.append(['', '', '', ''])
        rows.append(['', '', '', ''])

        # === Section 3: è­°é¡Œåˆ†é¡æ¨™æº– ===
        rows.append(['ğŸ“Œ è­°é¡Œé¡å‹ (Issue Topic) åˆ†é¡æ¨™æº–', '', '', ''])
        rows.append(['ä»£ç¢¼', 'ä¸­æ–‡åç¨±', 'èªªæ˜', 'åŒ¹é…é—œéµå­—ï¼ˆå®Œæ•´æ¸…å–®ï¼‰'])

        issue_topics = [
            ['nuclear', 'æ ¸èƒ½ç™¼é›»', 'æ ¸é›»å» ã€æ ¸å»¢æ–™ã€æ ¸èƒ½æ”¿ç­–ç›¸é—œ', 'æ ¸é›», æ ¸èƒ½, æ ¸å››, æ ¸å»¢, æ ¸å®‰, è¼»å°„'],
            ['climate', 'æ°£å€™å•é¡Œ', 'æ°£å€™è®Šé·ã€æ¥µç«¯æ°£å€™ç›¸é—œ', 'æ°£å€™, æš–åŒ–, ç¢³æ’, COP, æ¥µç«¯å¤©æ°£, æ°£å€™è®Šé·'],
            ['net_zero', 'æ·¨é›¶æ”¿ç­–', '2050æ·¨é›¶ã€æ¸›ç¢³æ”¿ç­–ç›¸é—œ', 'æ·¨é›¶, ç¢³ä¸­å’Œ, 2050, æ·¨é›¶è½‰å‹, æ¸›ç¢³'],
            ['industry', 'ç”¢æ¥­åˆ†æ', 'ç”¢æ¥­ç¢³æ’ã€ä¼æ¥­è²¬ä»»ç›¸é—œ', 'ç”¢æ¥­, ä¼æ¥­, ESG, æ°¸çºŒ, ä¾›æ‡‰éˆ, ç¢³ç›¤æŸ¥'],
            ['renewable', 'èƒ½æºç™¼å±•', 'å†ç”Ÿèƒ½æºã€èƒ½æºè½‰å‹ç›¸é—œ', 'å…‰é›», é¢¨é›», å†ç”Ÿèƒ½æº, ç¶ é›», å¤ªé™½èƒ½, é›¢å²¸é¢¨é›», å±‹é ‚, å…¬æ°‘é›»å» '],
            ['other', 'å…¶ä»–è­°é¡Œ', 'å…¶ä»–ç’°å¢ƒæˆ–å…¬æ°‘è­°é¡Œ', 'å‹å‹•, ç’°è©•, ç©ºæ±¡, æ°´è³‡æº, ç”Ÿæ…‹'],
        ]
        for it in issue_topics:
            rows.append(it)

        rows.append(['', '', '', ''])
        rows.append(['', '', '', ''])

        # === Section 4: ä½¿ç”¨èªªæ˜ ===
        rows.append(['ğŸ“‹ ä½¿ç”¨èªªæ˜', '', '', '', ''])
        rows.append(['1. æ¯å€‹åˆ†é å³å´æœ€å¾Œä¸€æ¬„é¡¯ç¤ºã€Œdata_updated_atã€æ›´æ–°æ™‚é–“', '', '', '', ''])
        rows.append(['2. è³‡æ–™æ¯æ—¥è‡ªå‹•æ›´æ–°ï¼ˆé€é Cloud Run + Cloud Schedulerï¼‰', '', '', '', ''])
        rows.append(['3. åŸå§‹è³‡æ–™åˆ†é ä¿ç•™å®Œæ•´æ­·å²è¨˜éŒ„ï¼Œåˆ†æåˆ†é åŸºæ–¼æœ€æ–°å¿«ç…§è¨ˆç®—', '', '', '', ''])
        rows.append(['4. è‹¥æŸå€‹åˆ†é è³‡æ–™ç‚ºç©ºï¼Œä»£è¡¨æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™', '', '', '', ''])

        rows.append(['', '', '', '', ''])
        rows.append(['', '', '', '', ''])

        # === Section 5: Pipeline åŸ·è¡Œç´€éŒ„ ===
        rows.append(['ğŸ”§ Pipeline åŸ·è¡Œç´€éŒ„', '', '', '', '', '', '', '', ''])

        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='pipeline_runs'")
        if cursor.fetchone():
            cursor.execute("""
                SELECT id, run_date, run_time, status,
                       posts_collected, posts_analyzed, sheets_exported,
                       error_message, duration_seconds
                FROM pipeline_runs
                ORDER BY run_date DESC, run_time DESC
                LIMIT 30
            """)
            logs = cursor.fetchall()

            rows.append(['åŸ·è¡Œ ID', 'æ—¥æœŸ', 'æ™‚é–“', 'ç‹€æ…‹', 'æ”¶é›†è²¼æ–‡æ•¸',
                         'åˆ†æè²¼æ–‡æ•¸', 'åŒ¯å‡ºå ±è¡¨æ•¸', 'éŒ¯èª¤è¨Šæ¯', 'åŸ·è¡Œç§’æ•¸'])
            for log in logs:
                rows.append(list(log))
        else:
            rows.append(['å°šç„¡åŸ·è¡Œç´€éŒ„', '', '', '', '', '', '', '', ''])

        rows.append(['', '', '', '', '', '', '', '', ''])
        rows.append(['', '', '', '', '', '', '', '', ''])

        # === Section 6: è³‡æ–™ä¾†æº ===
        rows.append(['ğŸ“¡ è³‡æ–™ä¾†æº', '', '', '', ''])
        rows.append(['â€¢ Facebook Graph API v23.0', '', '', '', ''])
        rows.append(['â€¢ è²¼æ–‡è³‡æ–™ï¼š2024-01-01 è‡³ä»Š', '', '', '', ''])
        rows.append(['â€¢ Insights è³‡æ–™ï¼šéå» 90 å¤©ï¼ˆFacebook API é™åˆ¶ï¼‰', '', '', '', ''])

        update_with_timestamp(worksheet, 'A1', rows)

        worksheet.format('A1:E1', {
            "backgroundColor": {"red": 0.5, "green": 0.5, "blue": 0.5},
            "textFormat": {"bold": True, "foregroundColor": {"red": 1, "green": 1, "blue": 1}, "fontSize": 12}
        })

        print(f"  âœ“ å·²å°å‡ºç³»çµ±è³‡è¨Š")
        return True

    except Exception as e:
        print(f"  âœ— å°å‡ºç³»çµ±è³‡è¨Šå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def cleanup_old_tabs(client):
    """åˆªé™¤èˆŠçš„ä¸éœ€è¦çš„åˆ†é ï¼Œåªä¿ç•™ 9 å€‹æ–°åˆ†é """
    try:
        spreadsheet = client.open(SPREADSHEET_NAME)

        # è¦ä¿ç•™çš„åˆ†é åç¨±
        keep_tabs = {
            'raw_posts',
            'raw_post_insights',
            'page_daily_metrics',
            'raw_ads',
            'ğŸ“Š content_analysis',
            'â° posting_times',
            'ğŸ“ˆ posts_performance',
            'ğŸ’° ad_analytics',
            'âš™ï¸ system_info',
        }

        # å–å¾—æ‰€æœ‰ç¾æœ‰åˆ†é 
        all_worksheets = spreadsheet.worksheets()

        deleted_count = 0
        for ws in all_worksheets:
            if ws.title not in keep_tabs:
                try:
                    spreadsheet.del_worksheet(ws)
                    print(f"  ğŸ—‘ï¸ å·²åˆªé™¤: {ws.title}")
                    deleted_count += 1
                except Exception as e:
                    print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {ws.title}: {e}")

        if deleted_count > 0:
            print(f"  âœ“ å·²æ¸…ç† {deleted_count} å€‹èˆŠåˆ†é ")
        else:
            print("  âœ“ ç„¡éœ€æ¸…ç†èˆŠåˆ†é ")

        return True

    except Exception as e:
        print(f"  âœ— æ¸…ç†èˆŠåˆ†é å¤±æ•—: {e}")
        return False


def main():
    """ä¸»ç¨‹å¼ - å°å‡ºæ‰€æœ‰åˆ†æå ±è¡¨ï¼ˆæ•´åˆç‰ˆï¼š9 å€‹åˆ†é ï¼‰"""
    print("\n" + "="*60)
    print("Facebook åˆ†æå ±è¡¨å°å‡ºè‡³ Google Sheets")
    print("="*60)
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # è¨­å®š Google Sheets å®¢æˆ¶ç«¯
    client = setup_google_sheets_client()
    if not client:
        print("\nâœ— ç„¡æ³•è¨­å®š Google Sheets å®¢æˆ¶ç«¯")
        return False

    # æ¸…ç†èˆŠåˆ†é 
    print("ğŸ§¹ æ¸…ç†èˆŠåˆ†é :")
    cleanup_old_tabs(client)

    # é€£æ¥è³‡æ–™åº«
    conn = analytics_reports.get_connection()

    print("\né–‹å§‹å°å‡ºåˆ†æå ±è¡¨...\n")

    # å°å‡ºå„é …å ±è¡¨ï¼ˆæ•´åˆç‰ˆï¼š9 å€‹åˆ†é ï¼‰
    success_count = 0
    total_count = 8  # raw_posts å·²åˆä½µåˆ° raw_post_insights

    # ğŸ“¦ åŸå§‹è³‡æ–™ (3 å€‹åˆ†é : raw_post_insights, page_daily_metrics, raw_ads)
    print("ğŸ“¦ åŸå§‹è³‡æ–™å°å‡º:")
    if export_raw_post_insights(client, conn):  # åŒ…å«è²¼æ–‡åŸºæœ¬è³‡è¨Š + åˆ†é¡ + äº’å‹•æ•¸æ“š
        success_count += 1
    if export_page_daily_metrics(client, conn):
        success_count += 1
    if export_raw_ads(client, conn):           # å»£å‘ŠåŸå§‹è³‡æ–™
        success_count += 1

    # ğŸ“Š æ•´åˆåˆ†æå ±è¡¨ (4 å€‹åˆ†é )
    print("\nğŸ“Š æ•´åˆåˆ†æå ±è¡¨å°å‡º:")
    if export_content_analysis(client, conn):  # è¡Œå‹•+è­°é¡Œ+äº¤å‰
        success_count += 1
    if export_posting_times(client, conn):     # æ™‚é–“åˆ†æ
        success_count += 1
    if export_posts_performance(client, conn): # è²¼æ–‡è¡¨ç¾
        success_count += 1
    if export_ad_analytics(client, conn):      # æŠ•å»£åˆ†æ
        success_count += 1

    # âš™ï¸ ç³»çµ±è³‡è¨Š (1 å€‹åˆ†é )
    print("\nâš™ï¸ ç³»çµ±è³‡è¨Šå°å‡º:")
    if export_system_info(client, conn):       # èªªæ˜+ç´€éŒ„
        success_count += 1

    conn.close()

    print(f"\n{'='*60}")
    print(f"å°å‡ºå®Œæˆ: {success_count}/{total_count} é …å ±è¡¨æˆåŠŸ")
    print(f"è©¦ç®—è¡¨: {SPREADSHEET_NAME}")
    print(f"{'='*60}\n")

    return success_count == total_count


if __name__ == '__main__':
    import sys
    success = main()
    sys.exit(0 if success else 1)
